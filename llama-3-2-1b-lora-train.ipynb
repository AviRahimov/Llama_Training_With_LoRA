{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B LoRA Fine-tuning - Local Setup\n",
    "\n",
    "This notebook trains a LoRA (Low-Rank Adaptation) model on Llama 3.2 1B using your local machine.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **GPU**: NVIDIA GPU with CUDA support (recommended for faster training)\n",
    "2. **RAM**: At least 8GB of system RAM\n",
    "3. **Storage**: ~10GB free space for models and data\n",
    "4. **Hugging Face Account**: For model access\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Hugging Face Token**: \n",
    "   - Get your token from https://huggingface.co/settings/tokens\n",
    "   - Set it as environment variable: `set HF_TOKEN=your_token_here` (Windows)\n",
    "   - Or replace the token directly in the authentication cell\n",
    "\n",
    "2. **Data File**: \n",
    "   - Ensure `combined_human_conversations.csv` is in your Documents folder\n",
    "   - Or update the file path in the data loading cell\n",
    "\n",
    "3. **Run cells in order** - the first cell will install all required packages\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "- Fine-tunes Llama 3.2 1B Instruct model using LoRA\n",
    "- Uses 4-bit quantization to reduce memory usage\n",
    "- Trains on conversational data in \"Human 1/Human 2\" format\n",
    "- Saves the trained adapter for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:29:18.652036Z",
     "iopub.status.busy": "2025-06-16T14:29:18.651736Z",
     "iopub.status.idle": "2025-06-16T14:30:46.718110Z",
     "shell.execute_reply": "2025-06-16T14:30:46.717409Z",
     "shell.execute_reply.started": "2025-06-16T14:29:18.652017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing required packages for local LoRA training...\n",
      "This may take a few minutes...\n",
      "‚úÖ transformers[sentencepiece] installed successfully\n",
      "‚úÖ transformers[sentencepiece] installed successfully\n",
      "‚úÖ accelerate installed successfully\n",
      "‚úÖ accelerate installed successfully\n",
      "‚úÖ peft installed successfully\n",
      "‚úÖ peft installed successfully\n",
      "‚úÖ bitsandbytes installed successfully\n",
      "‚úÖ bitsandbytes installed successfully\n",
      "‚úÖ datasets installed successfully\n",
      "‚úÖ datasets installed successfully\n",
      "‚úÖ trl installed successfully\n",
      "‚úÖ trl installed successfully\n",
      "‚úÖ torch installed successfully\n",
      "‚úÖ torch installed successfully\n",
      "‚úÖ pandas installed successfully\n",
      "‚úÖ pandas installed successfully\n",
      "‚úÖ huggingface_hub installed successfully\n",
      "‚úÖ huggingface_hub installed successfully\n",
      "‚úÖ python-dotenv installed successfully\n",
      "‚ö° Installing Flash Attention (may take longer)...\n",
      "‚úÖ python-dotenv installed successfully\n",
      "‚ö° Installing Flash Attention (may take longer)...\n",
      "‚ö†Ô∏è Flash Attention installation failed (optional - training will still work)\n",
      "\n",
      "üéâ Package installation complete!\n",
      "üìù Don't forget to create a .env file with your HF_TOKEN:\n",
      "   HF_TOKEN=your_hugging_face_token_here\n",
      "\n",
      "üí° Tips for GPU usage:\n",
      "   - Ensure CUDA drivers are up to date\n",
      "   - Close other GPU-intensive applications\n",
      "   - Monitor GPU memory usage during training\n",
      "‚ö†Ô∏è Flash Attention installation failed (optional - training will still work)\n",
      "\n",
      "üéâ Package installation complete!\n",
      "üìù Don't forget to create a .env file with your HF_TOKEN:\n",
      "   HF_TOKEN=your_hugging_face_token_here\n",
      "\n",
      "üí° Tips for GPU usage:\n",
      "   - Ensure CUDA drivers are up to date\n",
      "   - Close other GPU-intensive applications\n",
      "   - Monitor GPU memory usage during training\n"
     ]
    }
   ],
   "source": [
    "# Local Environment Setup for Llama 3.2 LoRA Training\n",
    "# Install required packages for local training\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "# Required packages for LoRA training\n",
    "packages = [\n",
    "    \"transformers[sentencepiece]\",\n",
    "    \"accelerate\", \n",
    "    \"peft\",\n",
    "    \"bitsandbytes\",\n",
    "    \"datasets\",\n",
    "    \"trl\",\n",
    "    \"torch\",\n",
    "    \"pandas\",\n",
    "    \"huggingface_hub\",\n",
    "    \"python-dotenv\",  # For loading .env files\n",
    "    \"flash-attn --no-build-isolation\"  # Flash Attention for faster training (optional)\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required packages for local LoRA training...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "for package in packages:\n",
    "    if \"flash-attn\" in package:\n",
    "        print(\"‚ö° Installing Flash Attention (may take longer)...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"flash-attn\", \"--no-build-isolation\"])\n",
    "            print(\"‚úÖ flash-attn installed successfully\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"‚ö†Ô∏è Flash Attention installation failed (optional - training will still work)\")\n",
    "    else:\n",
    "        install_package(package)\n",
    "\n",
    "print(\"\\nüéâ Package installation complete!\")\n",
    "print(\"üìù Don't forget to create a .env file with your HF_TOKEN:\")\n",
    "print(\"   HF_TOKEN=your_hugging_face_token_here\")\n",
    "print(\"\\nüí° Tips for GPU usage:\")\n",
    "print(\"   - Ensure CUDA drivers are up to date\")\n",
    "print(\"   - Close other GPU-intensive applications\")\n",
    "print(\"   - Monitor GPU memory usage during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:30:46.719172Z",
     "iopub.status.busy": "2025-06-16T14:30:46.718945Z",
     "iopub.status.idle": "2025-06-16T14:30:46.879072Z",
     "shell.execute_reply": "2025-06-16T14:30:46.878288Z",
     "shell.execute_reply.started": "2025-06-16T14:30:46.719146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE GPU & CUDA DETECTION\n",
      "==================================================\n",
      "1Ô∏è‚É£ PyTorch Installation:\n",
      "   PyTorch version: 2.5.1+cu121\n",
      "   PyTorch file location: C:\\Users\\rahim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py\n",
      "\n",
      "2Ô∏è‚É£ CUDA Availability:\n",
      "   CUDA available: True\n",
      "   CUDA version (PyTorch): 12.1\n",
      "   cuDNN version: 90100\n",
      "   cuDNN enabled: True\n",
      "\n",
      "3Ô∏è‚É£ GPU Information:\n",
      "   GPU count: 1\n",
      "   GPU 0: NVIDIA GeForce RTX 4060\n",
      "   Memory: 8.0 GB\n",
      "   Compute capability: 8.9\n",
      "\n",
      "4Ô∏è‚É£ Selected device: cuda\n",
      "   ‚úÖ GPU memory fraction set to 90%\n",
      "\n",
      "5Ô∏è‚É£ GPU Functionality Test:\n",
      "   ‚úÖ GPU tensor operations working correctly\n",
      "   GPU Memory - Allocated: 0.01 GB, Reserved: 0.02 GB\n",
      "\n",
      "==================================================\n",
      "üîê HUGGING FACE AUTHENTICATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged in to Hugging Face\n",
      "\n",
      "üéØ FINAL SETUP - Training will use: cuda\n",
      "==================================================\n",
      "üöÄ Ready for GPU-accelerated training!\n"
     ]
    }
   ],
   "source": [
    "# Local Hugging Face Authentication\n",
    "# Option 1: Set your token as an environment variable HF_TOKEN\n",
    "# Option 2: Replace 'YOUR_HF_TOKEN_HERE' with your actual token\n",
    "# Option 3: Use huggingface_hub.notebook_login() for interactive login\n",
    "\n",
    "# GPU Detection and Environment Setup\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîç COMPREHENSIVE GPU & CUDA DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic PyTorch and CUDA check\n",
    "print(\"1Ô∏è‚É£ PyTorch Installation:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   PyTorch file location: {torch.__file__}\")\n",
    "\n",
    "# 2. CUDA Availability Check\n",
    "print(\"\\n2Ô∏è‚É£ CUDA Availability:\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"   CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"   CUDA version (PyTorch): {torch.version.cuda}\")\n",
    "    print(f\"   cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"   cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "    \n",
    "    # 3. GPU Information\n",
    "    print(\"\\n3Ô∏è‚É£ GPU Information:\")\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"   GPU count: {gpu_count}\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        memory_gb = props.total_memory / (1024**3)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"   Memory: {memory_gb:.1f} GB\")\n",
    "        print(f\"   Compute capability: {props.major}.{props.minor}\")\n",
    "    \n",
    "    # 4. Set up GPU device\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\\n4Ô∏è‚É£ Selected device: {device}\")\n",
    "    \n",
    "    # 5. GPU Memory Management\n",
    "    torch.cuda.empty_cache()\n",
    "    if hasattr(torch.cuda, 'set_per_process_memory_fraction'):\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "        print(\"   ‚úÖ GPU memory fraction set to 90%\")\n",
    "    \n",
    "    # 6. Test GPU functionality\n",
    "    print(\"\\n5Ô∏è‚É£ GPU Functionality Test:\")\n",
    "    try:\n",
    "        # Simple tensor operation test\n",
    "        test_tensor = torch.randn(100, 100).cuda()\n",
    "        result = torch.matmul(test_tensor, test_tensor.T)\n",
    "        print(\"   ‚úÖ GPU tensor operations working correctly\")\n",
    "        \n",
    "        # Memory info\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"   GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå GPU test failed: {e}\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"   Falling back to CPU\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå CUDA NOT AVAILABLE!\")\n",
    "    print(\"Possible reasons:\")\n",
    "    print(\"   1. PyTorch CPU version installed instead of CUDA version\")\n",
    "    print(\"   2. NVIDIA drivers not installed or outdated\")\n",
    "    print(\"   3. CUDA toolkit not installed\")\n",
    "    print(\"   4. GPU not CUDA-compatible\")\n",
    "    print(\"\\nüí° Solutions:\")\n",
    "    print(\"   1. Run the PyTorch CUDA installation cell below\")\n",
    "    print(\"   2. Update NVIDIA drivers\")\n",
    "    print(\"   3. Restart kernel after installation\")\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"\\nüìç Using device: {device}\")\n",
    "\n",
    "# Hugging Face Authentication from .env file\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üîê HUGGING FACE AUTHENTICATION\")\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå HF login failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No HF_TOKEN found in .env file!\")\n",
    "    print(\"Please create a .env file in your project directory with:\")\n",
    "    print(\"HF_TOKEN=your_token_here\")\n",
    "\n",
    "# Global device variable for use in other cells\n",
    "DEVICE = device\n",
    "print(f\"\\nüéØ FINAL SETUP - Training will use: {DEVICE}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if str(device) == \"cpu\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Training on CPU will be VERY slow!\")\n",
    "    print(\"   Consider fixing CUDA setup for much faster training\")\n",
    "else:\n",
    "    print(\"üöÄ Ready for GPU-accelerated training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ RTX 4060 OPTIMIZATION SETTINGS\n",
      "==================================================\n",
      "üîß Optimizing for RTX 4060:\n",
      "   Total VRAM: 8.0 GB\n",
      "   Available for training: 6.8 GB\n",
      "   Compute Capability: 8.9 (Ada Lovelace)\n",
      "‚öôÔ∏è Setting RTX 4060 optimal training parameters...\n",
      "‚úÖ Configuration set:\n",
      "   Batch size: 2\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 16\n",
      "   Max sequence length: 512\n",
      "   Learning rate: 0.0002\n",
      "   LoRA rank: 16, alpha: 32\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üöÄ RTX 4060 Optimized Configuration\n",
    "print(\"üéØ RTX 4060 OPTIMIZATION SETTINGS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# RTX 4060 Specific Settings\n",
    "RTX_4060_MEMORY = 8.0  # GB\n",
    "AVAILABLE_MEMORY = RTX_4060_MEMORY * 0.85  # Use 85% to leave room for system\n",
    "\n",
    "print(f\"üîß Optimizing for RTX 4060:\")\n",
    "print(f\"   Total VRAM: {RTX_4060_MEMORY} GB\")\n",
    "print(f\"   Available for training: {AVAILABLE_MEMORY:.1f} GB\")\n",
    "print(f\"   Compute Capability: 8.9 (Ada Lovelace)\")\n",
    "\n",
    "# RTX 4060 Optimal Settings\n",
    "print(\"‚öôÔ∏è Setting RTX 4060 optimal training parameters...\")\n",
    "\n",
    "# Optimal settings for RTX 4060 + Llama 3.2-1B\n",
    "batch_size = 2\n",
    "gradient_accumulation = 8\n",
    "max_length = 512\n",
    "learning_rate = 2e-4\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "\n",
    "print(f\"‚úÖ Configuration set:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Gradient accumulation: {gradient_accumulation}\")\n",
    "print(f\"   Effective batch size: {batch_size * gradient_accumulation}\")\n",
    "print(f\"   Max sequence length: {max_length}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   LoRA rank: {lora_r}, alpha: {lora_alpha}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:35:57.935169Z",
     "iopub.status.busy": "2025-06-16T14:35:57.934525Z",
     "iopub.status.idle": "2025-06-16T14:35:57.957021Z",
     "shell.execute_reply": "2025-06-16T14:35:57.956376Z",
     "shell.execute_reply.started": "2025-06-16T14:35:57.935141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found file: c:\\Users\\rahim\\Documents\\combined_human_conversations.csv\n",
      "‚úÖ Successfully loaded CSV file\n",
      "\n",
      "Dataset info:\n",
      "Shape: (3860, 3)\n",
      "Columns: ['conversation_id', 'Question', 'Answer']\n",
      "\n",
      "First few rows:\n",
      "  conversation_id                                           Question  \\\n",
      "0       room_1193  hi ~ how are you?? ~ tell me some things ~ I n...   \n",
      "1       room_1193      i live whith my familly ~ where are you from?   \n",
      "2       room_9025                                             Hlelo!   \n",
      "3       room_7750                       HI ~ hello? ~ is anyone here   \n",
      "4       room_7750             well, im human and i would like my .50   \n",
      "\n",
      "                                              Answer  \n",
      "0                                          i am fine  \n",
      "1                         i also live with my family  \n",
      "2  What is the name of John Coltrane's first albu...  \n",
      "3                                                 hi  \n",
      "4                                        okey i also  \n",
      "\n",
      "Number of unique conversation_ids: 685\n",
      "Total rows: 3860\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Local file path - update this to match your CSV file location\n",
    "file_path = r'c:\\Users\\rahim\\Documents\\combined_human_conversations.csv'\n",
    "\n",
    "# Check if file exists\n",
    "import os\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"‚ùå File not found: {file_path}\")\n",
    "    print(\"Please ensure the CSV file is in the correct location or update the file_path variable\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found file: {file_path}\")\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"‚úÖ Successfully loaded CSV file\")\n",
    "    \n",
    "    # Display some info to verify\n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nNumber of unique conversation_ids: {df['conversation_id'].nunique()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading CSV file: {e}\")\n",
    "    print(\"Please check the file format and path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:35:59.003749Z",
     "iopub.status.busy": "2025-06-16T14:35:59.003190Z",
     "iopub.status.idle": "2025-06-16T14:35:59.196815Z",
     "shell.execute_reply": "2025-06-16T14:35:59.196254Z",
     "shell.execute_reply.started": "2025-06-16T14:35:59.003727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 685 conversations from the new dataset.\n",
      "Example: Human 1: hi\n",
      "Human 1: how are you??\n",
      "Human 1: tell me some things\n",
      "Human 1: I need to know some thing  abouth you\n",
      "Human 1: Plase give me some information abouth AI\n",
      "Human 1: How was the day?\n",
      "Human 2: i am...\n"
     ]
    }
   ],
   "source": [
    "processed_new_conversations_by_id = {} \n",
    "for index, row in df.iterrows():\n",
    "    pair_id = str(row['conversation_id']) \n",
    "    questions_str = str(row['Question']) if pd.notna(row['Question']) else \"\"\n",
    "    answers_str = str(row['Answer']) if pd.notna(row['Answer']) else \"\"\n",
    "    current_turn_text = \"\"\n",
    "    human1_utterances = [utt.strip() for utt in questions_str.split('~') if utt.strip()]\n",
    "    for utt in human1_utterances:\n",
    "        current_turn_text += f\"Human 1: {utt}\\n\"\n",
    "    human2_utterances = [utt.strip() for utt in answers_str.split('~') if utt.strip()]\n",
    "    for utt in human2_utterances:\n",
    "        current_turn_text += f\"Human 2: {utt}\\n\"\n",
    "    if pair_id not in processed_new_conversations_by_id:\n",
    "        processed_new_conversations_by_id[pair_id] = \"\"\n",
    "    processed_new_conversations_by_id[pair_id] += current_turn_text\n",
    "\n",
    "new_formatted_conversations_list = []\n",
    "for conv_id, conversation_body_text in processed_new_conversations_by_id.items():\n",
    "    if conversation_body_text.strip():\n",
    "        full_conversation_string = conversation_body_text.strip()\n",
    "        new_formatted_conversations_list.append({\n",
    "            \"text\": full_conversation_string,\n",
    "            \"conversation_id\": conv_id\n",
    "        })\n",
    "combined_dataset = Dataset.from_list(new_formatted_conversations_list)\n",
    "print(f\"Processed {len(combined_dataset)} conversations from the new dataset.\")\n",
    "if combined_dataset: print(f\"Example: {combined_dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:36:01.593711Z",
     "iopub.status.busy": "2025-06-16T14:36:01.593476Z",
     "iopub.status.idle": "2025-06-16T14:36:01.611207Z",
     "shell.execute_reply": "2025-06-16T14:36:01.610649Z",
     "shell.execute_reply.started": "2025-06-16T14:36:01.593695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Dataset structure for SFTTrainer:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'conversation_id'],\n",
      "        num_rows: 616\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['text', 'conversation_id'],\n",
      "        num_rows: 69\n",
      "    })\n",
      "})\n",
      "\n",
      "Total Training examples: 616\n",
      "Total Evaluation examples: 69\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "train_test_split = combined_dataset.train_test_split(test_size=0.1, seed=42) # Use combined_dataset\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'eval': train_test_split['test']\n",
    "})\n",
    "print(\"\\nCombined Dataset structure for SFTTrainer:\")\n",
    "print(dataset_dict)\n",
    "print(f\"\\nTotal Training examples: {len(dataset_dict['train'])}\")\n",
    "print(f\"Total Evaluation examples: {len(dataset_dict['eval'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:38:51.195604Z",
     "iopub.status.busy": "2025-06-16T14:38:51.194902Z",
     "iopub.status.idle": "2025-06-16T14:39:02.102774Z",
     "shell.execute_reply": "2025-06-16T14:39:02.101975Z",
     "shell.execute_reply.started": "2025-06-16T14:38:51.195580Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading tokenizer for meta-llama/Llama-3.2-1B-Instruct...\n",
      "‚úÖ Set pad_token to eos_token\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "‚úÖ Set pad_token to eos_token\n",
      "‚úÖ Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # Using the Instruct version!\n",
    "\n",
    "print(f\"ü§ñ Loading tokenizer for {model_id}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True,\n",
    "    use_fast=True,  # Use fast tokenizer for better performance\n",
    ")\n",
    "\n",
    "# Set pad token if not set. For Llama 3, it's common to use eos_token as pad_token.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Set pad_token to eos_token\")\n",
    "\n",
    "# Llama 3 Instruct models have a chat template. While we are providing fully formatted\n",
    "# strings for SFT, the tokenizer itself is aware of roles. For generation, we will\n",
    "# manually construct the prompt in our \"Human 1/Human 2\" format.\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:39:02.104881Z",
     "iopub.status.busy": "2025-06-16T14:39:02.104067Z",
     "iopub.status.idle": "2025-06-16T14:39:02.109596Z",
     "shell.execute_reply": "2025-06-16T14:39:02.108900Z",
     "shell.execute_reply.started": "2025-06-16T14:39:02.104826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuring 4-bit quantization for GPU optimization...\n",
      "‚úÖ Using bfloat16 for optimal performance\n",
      "‚úÖ Quantization config ready\n",
      "   Quantization type: nf4\n",
      "   Compute dtype: torch.bfloat16\n",
      "   Double quantization: True\n"
     ]
    }
   ],
   "source": [
    "# Optimized 4-bit Quantization Configuration for GPU\n",
    "print(\"‚öôÔ∏è Configuring 4-bit quantization for GPU optimization...\")\n",
    "\n",
    "# Check if bfloat16 is supported on this GPU\n",
    "compute_dtype = torch.bfloat16\n",
    "if not torch.cuda.is_bf16_supported():\n",
    "    print(\"‚ö†Ô∏è bfloat16 not supported on this GPU, falling back to float16\")\n",
    "    compute_dtype = torch.float16\n",
    "else:\n",
    "    print(\"‚úÖ Using bfloat16 for optimal performance\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Normalized float 4-bit\n",
    "    bnb_4bit_compute_dtype=compute_dtype,  # Compute dtype based on GPU support\n",
    "    bnb_4bit_use_double_quant=True,       # Double quantization for memory efficiency\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Quantization config ready\")\n",
    "print(f\"   Quantization type: nf4\")\n",
    "print(f\"   Compute dtype: {compute_dtype}\")\n",
    "print(f\"   Double quantization: True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:39:02.110414Z",
     "iopub.status.busy": "2025-06-16T14:39:02.110227Z",
     "iopub.status.idle": "2025-06-16T14:39:28.781676Z",
     "shell.execute_reply": "2025-06-16T14:39:28.781143Z",
     "shell.execute_reply.started": "2025-06-16T14:39:02.110399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Llama 3.2-1B with RTX 4060 Optimizations...\n",
      "Original tokenizer chat_template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Modified tokenizer chat_template: None\n",
      "üì• Loading model with RTX 4060 optimizations...\n",
      "‚ö†Ô∏è Flash Attention 2 not available - using standard attention\n",
      "‚úÖ Model loaded successfully!\n",
      "‚úÖ Tokenizer attached to model\n",
      "‚úÖ Pad token ID configured\n",
      "‚úÖ Gradient checkpointing enabled for memory efficiency\n",
      "\n",
      "üìä RTX 4060 Model Status:\n",
      "   Model dtype: torch.bfloat16\n",
      "   Attention implementation: default\n",
      "   Gradient checkpointing: enabled\n",
      "\n",
      "üíæ RTX 4060 Memory Status:\n",
      "   Total VRAM: 8.0 GB\n",
      "   Allocated: 2.45 GB (30.7%)\n",
      "   Reserved: 3.41 GB (42.6%)\n",
      "   Available: 4.6 GB\n",
      "   ‚úÖ Good VRAM usage - ready for training\n",
      "\n",
      "üéØ Model ready for LoRA fine-tuning on RTX 4060!\n",
      "‚úÖ Model loaded successfully!\n",
      "‚úÖ Tokenizer attached to model\n",
      "‚úÖ Pad token ID configured\n",
      "‚úÖ Gradient checkpointing enabled for memory efficiency\n",
      "\n",
      "üìä RTX 4060 Model Status:\n",
      "   Model dtype: torch.bfloat16\n",
      "   Attention implementation: default\n",
      "   Gradient checkpointing: enabled\n",
      "\n",
      "üíæ RTX 4060 Memory Status:\n",
      "   Total VRAM: 8.0 GB\n",
      "   Allocated: 2.45 GB (30.7%)\n",
      "   Reserved: 3.41 GB (42.6%)\n",
      "   Available: 4.6 GB\n",
      "   ‚úÖ Good VRAM usage - ready for training\n",
      "\n",
      "üéØ Model ready for LoRA fine-tuning on RTX 4060!\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Loading Llama 3.2-1B with RTX 4060 Optimizations...\")\n",
    "\n",
    "# Disable chat template for custom training format\n",
    "print(f\"Original tokenizer chat_template: {tokenizer.chat_template}\")\n",
    "tokenizer.chat_template = None\n",
    "print(f\"Modified tokenizer chat_template: {tokenizer.chat_template}\")\n",
    "\n",
    "# RTX 4060 Optimized model loading\n",
    "print(\"üì• Loading model with RTX 4060 optimizations...\")\n",
    "\n",
    "# Check if Flash Attention is available\n",
    "try:\n",
    "    flash_attn_available = True\n",
    "    import flash_attn\n",
    "    print(\"‚úÖ Flash Attention 2 available - will use for faster training\")\n",
    "except ImportError:\n",
    "    flash_attn_available = False\n",
    "    print(\"‚ö†Ô∏è Flash Attention 2 not available - using standard attention\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically map to your RTX 4060\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=compute_dtype,  # Use bfloat16 for RTX 4060\n",
    "    low_cpu_mem_usage=True,     # Minimize CPU memory during loading\n",
    "    attn_implementation=\"flash_attention_2\" if flash_attn_available else \"sdpa\",  # Use Flash Attention or SDPA\n",
    "    use_cache=False,  # Disable cache for training\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# --- CRITICAL: Attach tokenizer to the BASE model ---\n",
    "model.tokenizer = tokenizer\n",
    "print(\"‚úÖ Tokenizer attached to model\")\n",
    "\n",
    "# Pad token configuration\n",
    "if model.config.pad_token_id is None or model.config.pad_token_id != tokenizer.pad_token_id:\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(\"‚úÖ Pad token ID configured\")\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency on RTX 4060\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled for memory efficiency\")\n",
    "\n",
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# Display RTX 4060 specific info\n",
    "print(f\"\\nüìä RTX 4060 Model Status:\")\n",
    "print(f\"   Model dtype: {model.dtype}\")\n",
    "print(f\"   Attention implementation: {getattr(model.config, 'attn_implementation', 'default')}\")\n",
    "print(f\"   Gradient checkpointing: {getattr(model.config, 'use_gradient_checkpointing', 'enabled')}\")\n",
    "\n",
    "# Memory usage on RTX 4060\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()  # Clear any cached memory\n",
    "    allocated = torch.cuda.memory_allocated()/1024**3\n",
    "    reserved = torch.cuda.memory_reserved()/1024**3\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory/1024**3\n",
    "    \n",
    "    print(f\"\\nüíæ RTX 4060 Memory Status:\")\n",
    "    print(f\"   Total VRAM: {total_vram:.1f} GB\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB ({allocated/total_vram*100:.1f}%)\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB ({reserved/total_vram*100:.1f}%)\")\n",
    "    print(f\"   Available: {total_vram-reserved:.1f} GB\")\n",
    "    \n",
    "    if reserved > 6.5:  # Warning if using too much VRAM\n",
    "        print(\"   ‚ö†Ô∏è High VRAM usage - consider reducing batch size if training fails\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Good VRAM usage - ready for training\")\n",
    "\n",
    "print(\"\\nüéØ Model ready for LoRA fine-tuning on RTX 4060!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T14:39:28.783431Z",
     "iopub.status.busy": "2025-06-16T14:39:28.782977Z",
     "iopub.status.idle": "2025-06-16T14:39:29.365610Z",
     "shell.execute_reply": "2025-06-16T14:39:29.364863Z",
     "shell.execute_reply.started": "2025-06-16T14:39:28.783411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparing model for RTX 4060 LoRA training...\n",
      "‚úÖ Model prepared for 4-bit training\n",
      "\n",
      "‚öôÔ∏è Configuring LoRA for RTX 4060...\n",
      "üìä LoRA Configuration:\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "   Dropout: 0.05\n",
      "\n",
      "üîß Creating PEFT model...\n",
      "‚úÖ PEFT model created successfully!\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "\n",
      "üìä Parameter Summary:\n",
      "   Trainable parameters: 11,272,192\n",
      "   Total parameters: 760,547,328\n",
      "   Trainable ratio: 1.48%\n",
      "   LoRA memory overhead: ~0.04 GB\n",
      "   Current GPU memory: 2.99 GB\n",
      "   ‚úÖ Excellent memory usage for RTX 4060!\n",
      "\n",
      "üéØ RTX 4060 LoRA setup complete!\n",
      "‚úÖ PEFT model created successfully!\n",
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "\n",
      "üìä Parameter Summary:\n",
      "   Trainable parameters: 11,272,192\n",
      "   Total parameters: 760,547,328\n",
      "   Trainable ratio: 1.48%\n",
      "   LoRA memory overhead: ~0.04 GB\n",
      "   Current GPU memory: 2.99 GB\n",
      "   ‚úÖ Excellent memory usage for RTX 4060!\n",
      "\n",
      "üéØ RTX 4060 LoRA setup complete!\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "print(\"üîß Preparing model for RTX 4060 LoRA training...\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úÖ Model prepared for 4-bit training\")\n",
    "\n",
    "# RTX 4060 Optimized LoRA Configuration\n",
    "print(\"\\n‚öôÔ∏è Configuring LoRA for RTX 4060...\")\n",
    "\n",
    "# Use the optimal config from earlier\n",
    "lora_r = globals().get('lora_r', 16)  # From RTX 4060 config\n",
    "lora_alpha = globals().get('lora_alpha', 32)\n",
    "\n",
    "# Target modules for Llama 3.2\n",
    "target_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projections\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP layers\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,                           # Rank - 16 is optimal for RTX 4060\n",
    "    lora_alpha=lora_alpha,              # Alpha - typically 2x the rank\n",
    "    target_modules=target_modules,       # Target the main linear layers\n",
    "    lora_dropout=0.05,                  # Small dropout for regularization\n",
    "    bias=\"none\",                        # Don't adapt bias terms\n",
    "    task_type=\"CAUSAL_LM\",             # Causal language modeling\n",
    "    inference_mode=False,               # Training mode\n",
    ")\n",
    "\n",
    "print(f\"üìä LoRA Configuration:\")\n",
    "print(f\"   Rank (r): {lora_r}\")\n",
    "print(f\"   Alpha: {lora_alpha}\")\n",
    "print(f\"   Target modules: {target_modules}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "\n",
    "# Create PEFT model\n",
    "print(f\"\\nüîß Creating PEFT model...\")\n",
    "try:\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    print(\"‚úÖ PEFT model created successfully!\")\n",
    "    \n",
    "    # Show trainable parameters\n",
    "    peft_model.print_trainable_parameters()\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "    \n",
    "    print(f\"\\nüìä Parameter Summary:\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Memory estimation for RTX 4060\n",
    "    lora_memory = trainable_params * 4 / (1024**3)  # 4 bytes per float32 param\n",
    "    print(f\"   LoRA memory overhead: ~{lora_memory:.2f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated()/1024**3\n",
    "        print(f\"   Current GPU memory: {allocated:.2f} GB\")\n",
    "        \n",
    "        if allocated < 6.0:\n",
    "            print(\"   ‚úÖ Excellent memory usage for RTX 4060!\")\n",
    "        elif allocated < 7.0:\n",
    "            print(\"   ‚úÖ Good memory usage for RTX 4060\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è High memory usage - monitor during training\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating PEFT model: {e}\")\n",
    "    print(\"Check the target modules and model architecture\")\n",
    "    raise e\n",
    "\n",
    "print(f\"\\nüéØ RTX 4060 LoRA setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:03:02.352687Z",
     "iopub.status.busy": "2025-06-16T16:03:02.352443Z",
     "iopub.status.idle": "2025-06-16T16:03:02.380380Z",
     "shell.execute_reply": "2025-06-16T16:03:02.379638Z",
     "shell.execute_reply.started": "2025-06-16T16:03:02.352671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ RTX 4060 Optimized Training Configuration\n",
      "==================================================\n",
      "üìä Training Configuration Summary:\n",
      "   Batch size: 2\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 16\n",
      "   Learning rate: 0.0002\n",
      "   Max sequence length: 512\n",
      "   Epochs: 3\n",
      "   Precision: bfloat16\n",
      "   Gradient checkpointing: True\n",
      "   Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n",
      "\n",
      "üíæ RTX 4060 Memory Estimation:\n",
      "   Batch memory: ~0 MB\n",
      "   With gradient accumulation: ~0 MB\n",
      "   Model + LoRA: ~2-3 GB\n",
      "   Training overhead: ~1-2 GB\n",
      "   Total estimated: ~4-6 GB (safe for 8GB RTX 4060)\n",
      "\n",
      "‚è±Ô∏è Training Time Estimation:\n",
      "   Training samples: 616\n",
      "   Steps per epoch: 38\n",
      "   Total training steps: 114\n",
      "   Estimated time: ~6 minutes on RTX 4060\n",
      "\n",
      "‚úÖ RTX 4060 training configuration ready!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig\n",
    "import os\n",
    "\n",
    "print(\"üöÄ RTX 4060 Optimized Training Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use optimal settings from RTX 4060 config\n",
    "batch_size = globals().get('batch_size', 2)\n",
    "gradient_accumulation = globals().get('gradient_accumulation', 8)\n",
    "learning_rate = globals().get('learning_rate', 2e-4)\n",
    "max_length = globals().get('max_length', 512)\n",
    "\n",
    "output_dir = \"./llama3_rtx4060_lora_training\"\n",
    "\n",
    "# RTX 4060 Optimized Training Configuration\n",
    "sft_config = SFTConfig(\n",
    "    # Output and logging\n",
    "    output_dir=output_dir,\n",
    "    run_name=\"llama3-1b-lora-rtx4060\",\n",
    "    \n",
    "    # RTX 4060 Optimized batch settings\n",
    "    per_device_train_batch_size=batch_size,          # Small batch for 8GB VRAM\n",
    "    per_device_eval_batch_size=1,                    # Even smaller for eval\n",
    "    gradient_accumulation_steps=gradient_accumulation, # Effective batch = 2*8 = 16\n",
    "    \n",
    "    # Model and sequence settings\n",
    "    max_seq_length=max_length,                       # Reasonable context length\n",
    "    \n",
    "    # Learning settings optimized for RTX 4060\n",
    "    learning_rate=learning_rate,                     # Good for LoRA fine-tuning\n",
    "    num_train_epochs=3,                              # Moderate epochs for RTX 4060\n",
    "    \n",
    "    # Memory and precision optimizations\n",
    "    fp16=False,                                      # Use bfloat16 instead\n",
    "    bf16=True,                                       # RTX 4060 supports bfloat16\n",
    "    gradient_checkpointing=True,                     # Save memory on RTX 4060\n",
    "    dataloader_pin_memory=False,                     # Reduce memory pressure\n",
    "    \n",
    "    # Optimizer optimized for RTX 4060\n",
    "    optim=\"paged_adamw_8bit\",                       # Memory efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",                      # Smooth learning rate decay\n",
    "    warmup_ratio=0.1,                               # 10% warmup\n",
    "    \n",
    "    # Gradient and regularization\n",
    "    max_grad_norm=0.3,                              # Prevent gradient explosion\n",
    "    weight_decay=0.01,                              # Light regularization\n",
    "    \n",
    "    # Logging and evaluation for RTX 4060\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,                               # More frequent logging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,                                  # Regular evaluation\n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=100,                                 # Save checkpoints regularly\n",
    "    \n",
    "    # Efficiency settings\n",
    "    remove_unused_columns=False,                    # Keep all columns\n",
    "    report_to=\"none\",                               # No external reporting\n",
    "    seed=42,\n",
    "    \n",
    "    # SFT specific settings\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,                                  # Don't pack sequences for RTX 4060\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We handle special tokens manually\n",
    "        \"append_concat_token\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display configuration summary\n",
    "print(f\"üìä Training Configuration Summary:\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Gradient accumulation: {gradient_accumulation}\")\n",
    "print(f\"   Effective batch size: {batch_size * gradient_accumulation}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Max sequence length: {max_length}\")\n",
    "print(f\"   Epochs: {sft_config.num_train_epochs}\")\n",
    "print(f\"   Precision: {'bfloat16' if sft_config.bf16 else 'float32'}\")\n",
    "print(f\"   Gradient checkpointing: {sft_config.gradient_checkpointing}\")\n",
    "print(f\"   Optimizer: {sft_config.optim}\")\n",
    "\n",
    "# Memory estimation\n",
    "print(f\"\\nüíæ RTX 4060 Memory Estimation:\")\n",
    "estimated_batch_memory = batch_size * max_length * 4 / (1024**2)  # MB per batch\n",
    "print(f\"   Batch memory: ~{estimated_batch_memory:.0f} MB\")\n",
    "print(f\"   With gradient accumulation: ~{estimated_batch_memory * gradient_accumulation:.0f} MB\")\n",
    "print(f\"   Model + LoRA: ~2-3 GB\")\n",
    "print(f\"   Training overhead: ~1-2 GB\")\n",
    "print(f\"   Total estimated: ~4-6 GB (safe for 8GB RTX 4060)\")\n",
    "\n",
    "# Training time estimation\n",
    "if 'dataset_dict' in globals():\n",
    "    train_samples = len(dataset_dict['train'])\n",
    "    steps_per_epoch = train_samples // (batch_size * gradient_accumulation)\n",
    "    total_steps = steps_per_epoch * sft_config.num_train_epochs\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Training Time Estimation:\")\n",
    "    print(f\"   Training samples: {train_samples}\")\n",
    "    print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"   Total training steps: {total_steps}\")\n",
    "    print(f\"   Estimated time: ~{total_steps * 3 / 60:.0f} minutes on RTX 4060\")\n",
    "\n",
    "print(f\"\\n‚úÖ RTX 4060 training configuration ready!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:03:03.163399Z",
     "iopub.status.busy": "2025-06-16T16:03:03.162587Z",
     "iopub.status.idle": "2025-06-16T16:03:04.598152Z",
     "shell.execute_reply": "2025-06-16T16:03:04.597585Z",
     "shell.execute_reply.started": "2025-06-16T16:03:03.163373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up standard Trainer (avoiding SFTTrainer issues)...\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 616/616 [00:00<00:00, 10319.24 examples/s]\n",
      "Tokenizing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 616/616 [00:00<00:00, 10319.24 examples/s]\n",
      "Tokenizing eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69/69 [00:00<00:00, 6321.97 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 616, Eval: 69\n",
      "Creating standard Trainer...\n",
      "‚úÖ Standard Trainer ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12\n",
    "# Simple Standard Trainer Setup (Avoiding SFTTrainer Issues)\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"Setting up standard Trainer (avoiding SFTTrainer issues)...\")\n",
    "\n",
    "# Simple single-item tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize each text individually\n",
    "    texts = examples[sft_config.dataset_text_field]\n",
    "    \n",
    "    # Tokenize with proper settings\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=sft_config.max_seq_length,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "# Apply tokenization\n",
    "tokenized_train = dataset_dict[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "tokenized_eval = dataset_dict[\"eval\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"eval\"].column_names,\n",
    "    desc=\"Tokenizing eval\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train: {len(tokenized_train)}, Eval: {len(tokenized_eval)}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "# Convert SFTConfig to TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=sft_config.output_dir,\n",
    "    per_device_train_batch_size=sft_config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=sft_config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=sft_config.gradient_accumulation_steps,\n",
    "    learning_rate=sft_config.learning_rate,\n",
    "    num_train_epochs=sft_config.num_train_epochs,\n",
    "    logging_strategy=sft_config.logging_strategy,\n",
    "    logging_steps=sft_config.logging_steps,\n",
    "    eval_strategy=sft_config.eval_strategy,\n",
    "    eval_steps=sft_config.eval_steps,\n",
    "    save_strategy=sft_config.save_strategy,\n",
    "    save_steps=sft_config.save_steps,\n",
    "    bf16=sft_config.bf16,\n",
    "    fp16=sft_config.fp16,\n",
    "    optim=sft_config.optim,\n",
    "    lr_scheduler_type=sft_config.lr_scheduler_type,\n",
    "    warmup_ratio=sft_config.warmup_ratio,\n",
    "    max_grad_norm=sft_config.max_grad_norm,\n",
    "    weight_decay=getattr(sft_config, 'weight_decay', 0.01),\n",
    "    dataloader_drop_last=False,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    seed=sft_config.seed,\n",
    ")\n",
    "\n",
    "print(\"Creating standard Trainer...\")\n",
    "# Create standard Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Standard Trainer ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:03:05.259411Z",
     "iopub.status.busy": "2025-06-16T16:03:05.259150Z",
     "iopub.status.idle": "2025-06-16T16:38:47.088000Z",
     "shell.execute_reply": "2025-06-16T16:38:47.087404Z",
     "shell.execute_reply.started": "2025-06-16T16:03:05.259392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RTX 4060 Optimized Training...\n",
      "==================================================\n",
      "üìä Pre-training Memory Status:\n",
      "   Used: 2.48 GB / 8.0 GB (31.0%)\n",
      "   Available: 5.5 GB\n",
      "üöÄ Starting training...\n",
      "\n",
      "‚ùå ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Safe Training with RTX 4060 Monitoring\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Starting RTX 4060 Optimized Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pre-training memory check\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    initial_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"üìä Pre-training Memory Status:\")\n",
    "    print(f\"   Used: {initial_memory:.2f} GB / {total_memory:.1f} GB ({initial_memory/total_memory*100:.1f}%)\")\n",
    "    print(f\"   Available: {total_memory - initial_memory:.1f} GB\")\n",
    "\n",
    "# Start Training\n",
    "print(\"üöÄ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Training with comprehensive error handling\n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è Training time: {training_duration/60:.1f} minutes\")\n",
    "    \n",
    "    # Post-training memory check\n",
    "    if torch.cuda.is_available():\n",
    "        final_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "        print(f\"üìä Post-training Memory:\")\n",
    "        print(f\"   Used: {final_memory:.2f} GB / {total_memory:.1f} GB ({final_memory/total_memory*100:.1f}%)\")\n",
    "        print(f\"   Memory increase: {final_memory - initial_memory:.2f} GB\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(f\"\\n‚ùå CUDA Out of Memory Error!\")\n",
    "        print(f\"üí° RTX 4060 Memory Solutions:\")\n",
    "        print(f\"   1. Reduce batch size: Try per_device_train_batch_size=1\")\n",
    "        print(f\"   2. Reduce sequence length: Try max_seq_length=256\")\n",
    "        print(f\"   3. Increase gradient accumulation to maintain effective batch size\")\n",
    "        print(f\"   4. Enable more aggressive gradient checkpointing\")\n",
    "        \n",
    "        # Clear memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        print(f\"\\nüîß Current memory after cleanup:\")\n",
    "        current_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "        print(f\"   Used: {current_memory:.2f} GB\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå Runtime Error: {e}\")\n",
    "        \n",
    "except ValueError as e:\n",
    "    if \"tensor\" in str(e).lower() and \"dimension\" in str(e).lower():\n",
    "        print(f\"\\n‚ùå Data Format Error!\")\n",
    "        print(f\"üí° Data Solutions:\")\n",
    "        print(f\"   1. Check dataset text field format\")\n",
    "        print(f\"   2. Verify tokenizer compatibility\") \n",
    "        print(f\"   3. Ensure no nested lists in dataset\")\n",
    "        print(f\"\\nOriginal error: {e}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ùå ValueError: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Unexpected Error: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Memory cleanup on any error\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:40:55.752558Z",
     "iopub.status.busy": "2025-06-16T16:40:55.751869Z",
     "iopub.status.idle": "2025-06-16T16:40:55.996314Z",
     "shell.execute_reply": "2025-06-16T16:40:55.995710Z",
     "shell.execute_reply.started": "2025-06-16T16:40:55.752533Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAHWCAYAAACSWtPeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOydd5gb1fX+31GXtneve8Xd2JhmO2AIzRRjCC1AAiaFJEASAiQBEgjlm/hHh0BCSTElOBCIMYTQbMChV2MwNrh73Xa9valLM78/Zu6dO6ORVtrVrrT2+TzPPllJo5k7I5nMu+8575EURVFAEARBEARBEARBZIQt1wsgCIIgCIIgCIIYjJCYIgiCIAiCIAiC6AUkpgiCIAiCIAiCIHoBiSmCIAiCIAiCIIheQGKKIAiCIAiCIAiiF5CYIgiCIAiCIAiC6AUkpgiCIAiCIAiCIHoBiSmCIAiCIAiCIIheQGKKIAiCIAiCIAiiF5CYIgiCyCMWL16M0aNH9+q9N910EyRJyu6C8owdO3ZAkiQ8+uijuV5KUkaPHo3Fixfn5NiD4foQBEHsT5CYIgiCSANJktL6Wb16da6XSgBYvXp1ys/pqaeeyvUS+8SyZctw77335noZBhYvXozCwsJcL4MgCGJAceR6AQRBEIOBJ554wvD48ccfx8qVKxOenzx5cp+O85e//AWyLPfqvb/97W9x7bXX9un4+xs/+9nPcNhhhyU8P2fOnBysJnssW7YMX375Ja688krD86NGjUIwGITT6czNwgiCIA4wSEwRBEGkwXe+8x3D4w8++AArV65MeN5MIBCAz+dL+zh9uQl2OBxwOOg/6yJHHXUUzj777FwvY8CQJAkejyfXyyAIgjhgoDI/giCILHHMMcdg2rRp+PTTT3H00UfD5/Ph+uuvBwA8//zzOPXUUzF06FC43W6MGzcOt956K+LxuGEf5p4p1gNz55134pFHHsG4cePgdrtx2GGH4eOPPza816pnSpIkXHHFFVixYgWmTZsGt9uNqVOn4pVXXklY/+rVq3HooYfC4/Fg3LhxePjhh9Puw3r77bdxzjnnYOTIkXC73RgxYgR+8YtfIBgMJpxfYWEh9uzZgzPOOAOFhYWoqqrCNddck3At2tvbsXjxYpSUlKC0tBQXX3wx2tvbe1xLJkybNg3HHntswvOyLGPYsGEGIXbnnXdi7ty5qKiogNfrxezZs/Hss8/2eIxk1/DRRx+FJEnYsWMHfy6d78kxxxyD//73v6irq+Nli+w7k6xn6o033sBRRx2FgoIClJaWYtGiRfjqq68s17llyxYsXrwYpaWlKCkpwSWXXIJAINDjeabLM888g9mzZ8Pr9aKyshLf+c53sGfPHsM2DQ0NuOSSSzB8+HC43W7U1tZi0aJFhmv1ySef4KSTTkJlZSW8Xi/GjBmD733ve1lbJ0EQRDrQnzAJgiCySEtLC04++WR8+9vfxne+8x3U1NQAUG+cCwsLcdVVV6GwsBBvvPEGbrzxRnR2duKOO+7ocb/Lli1DV1cXfvSjH0GSJNx+++341re+hW3btvXoZr3zzjtYvnw5LrvsMhQVFeGPf/wjzjrrLOzcuRMVFRUAgM8++wwLFixAbW0tbr75ZsTjcdxyyy2oqqpK67yfeeYZBAIB/OQnP0FFRQU++ugj3H///di9ezeeeeYZw7bxeBwnnXQSjjjiCNx5551YtWoV7rrrLowbNw4/+clPAACKomDRokV455138OMf/xiTJ0/Gc889h4svvjit9TC6urrQ3Nyc8HxFRQUkScJ5552Hm266CQ0NDRgyZIjhmu3duxff/va3+XP33XcfTj/9dFx44YWIRCJ46qmncM455+DFF1/EqaeemtG6kpHO9+Q3v/kNOjo6sHv3btxzzz0AkLJXadWqVTj55JMxduxY3HTTTQgGg7j//vsxb948rFmzJiHw5Nxzz8WYMWOwZMkSrFmzBn/9619RXV2N2267LSvnd8kll+Cwww7DkiVLsG/fPtx3331499138dlnn6G0tBQAcNZZZ2H9+vX46U9/itGjR6OxsRErV67Ezp07+eMTTzwRVVVVuPbaa1FaWoodO3Zg+fLlfV4jQRBERigEQRBExlx++eWK+T+h8+fPVwAoDz30UML2gUAg4bkf/ehHis/nU0KhEH/u4osvVkaNGsUfb9++XQGgVFRUKK2trfz5559/XgGg/Oc//+HP/e53v0tYEwDF5XIpW7Zs4c99/vnnCgDl/vvv588tXLhQ8fl8yp49e/hzmzdvVhwOR8I+rbA6vyVLliiSJCl1dXWG8wOg3HLLLYZtZ82apcyePZs/XrFihQJAuf322/lzsVhMOeqooxQAytKlS1Ou580331QAJP2pr69XFEVRNm7cmHAtFEVRLrvsMqWwsNBwXuZzjEQiyrRp05RvfvObhudHjRqlXHzxxfyx1eeiKIqydOlSBYCyffv2pMdQFOvvyamnnmr4njDY90W8PjNnzlSqq6uVlpYW/tznn3+u2Gw25aKLLkpY5/e+9z3DPs8880yloqIi4VhmLr74YqWgoCDp65FIRKmurlamTZumBINB/vyLL76oAFBuvPFGRVEUpa2tTQGg3HHHHUn39dxzzykAlI8//rjHdREEQfQnVOZHEASRRdxuNy655JKE571eL/+duSVHHXUUAoEAvv766x73e95556GsrIw/PuqoowAA27Zt6/G9xx9/PMaNG8cfz5gxA8XFxfy98Xgcq1atwhlnnIGhQ4fy7caPH4+TTz65x/0DxvPz+/1obm7G3LlzoSgKPvvss4Ttf/zjHxseH3XUUYZzeemll+BwOLhTBQB2ux0//elP01oP48Ybb8TKlSsTfsrLywEABx10EGbOnImnn36avycej+PZZ5/FwoULDecl/t7W1oaOjg4cddRRWLNmTUZrSkVfvydm6uvrsXbtWixevJifM6B+B0444QS89NJLCe+x+mxaWlrQ2dmZ8fFFPvnkEzQ2NuKyyy4z9HWdeuqpmDRpEv773/8CUK+By+XC6tWr0dbWZrkv5mC9+OKLiEajfVoXQRBEXyAxRRAEkUWGDRsGl8uV8Pz69etx5plnoqSkBMXFxaiqquLhFR0dHT3ud+TIkYbHTFglu9lM9V72fvbexsZGBINBjB8/PmE7q+es2LlzJ79hZ31Q8+fPB5B4fh6PJ6F8UFwPANTV1aG2tjahfG3ixIlprYcxffp0HH/88Qk/4md03nnn4d133+V9O6tXr0ZjYyPOO+88w75efPFFHHnkkfB4PCgvL0dVVRUefPDBtD6/dOnr98RMXV0dAOvrNnnyZDQ3N8Pv9xue78t3rbdrmTRpEn/d7Xbjtttuw8svv4yamhocffTRuP3229HQ0MC3nz9/Ps466yzcfPPNqKysxKJFi7B06VKEw+E+rZEgCCJTSEwRBEFkEdFZYLS3t2P+/Pn4/PPPccstt+A///kPVq5cyXtQ0olCt9vtls8ritKv702HeDyOE044Af/973/x61//GitWrMDKlSt5CIL5/JKtJ1ecd955UBSF93b961//QklJCRYsWMC3efvtt3H66afD4/Hgz3/+M1566SWsXLkSF1xwQY/XMVmAh1XgRl+/J9mgv78v6XDllVdi06ZNWLJkCTweD2644QZMnjyZu5ySJOHZZ5/F+++/jyuuuAJ79uzB9773PcyePRvd3d0Dtk6CIAgKoCAIguhnVq9ejZaWFixfvhxHH300f3779u05XJVOdXU1PB4PtmzZkvCa1XNm1q1bh02bNuGxxx7DRRddxJ9fuXJlr9c0atQovP766+ju7ja4Uxs3buz1PpMxZswYHH744Xj66adxxRVXYPny5TjjjDPgdrv5Nv/+97/h8Xjw6quvGp5funRpj/tnzk57ezsvTwN0p4aRyfcknYRFQL2OgPV1+/rrr1FZWYmCgoK09tVXxLV885vfNLy2ceNG/jpj3LhxuPrqq3H11Vdj8+bNmDlzJu666y784x//4NsceeSROPLII/H73/8ey5Ytw4UXXoinnnoKP/jBD/r/hAiCIEDOFEEQRL/D/tIv/mU/Eongz3/+c66WZMBut+P444/HihUrsHfvXv78li1b8PLLL6f1fsB4foqi4L777uv1mk455RTEYjE8+OCD/Ll4PI7777+/1/tMxXnnnYcPPvgAf//739Hc3JxQ4me32yFJksFN2rFjB1asWNHjvlm/2ltvvcWf8/v9eOyxxxKOAaT3PSkoKEir7K+2thYzZ87EY489ZoiV//LLL/Haa6/hlFNO6XEf2eLQQw9FdXU1HnroIUM53ssvv4yvvvqKJyIGAgGEQiHDe8eNG4eioiL+vra2tgSnbObMmQBApX4EQQwo5EwRBEH0M3PnzkVZWRkuvvhi/OxnP4MkSXjiiScGtGyqJ2666Sa89tprmDdvHn7yk58gHo/jgQcewLRp07B27dqU7500aRLGjRuHa665Bnv27EFxcTH+/e9/96nHZuHChZg3bx6uvfZa7NixA1OmTMHy5csz7ht6++23E27MATWAYcaMGfzxueeei2uuuQbXXHMNysvLcfzxxxu2P/XUU3H33XdjwYIFuOCCC9DY2Ig//elPGD9+PL744ouUazjxxBMxcuRIfP/738cvf/lL2O12/P3vf0dVVRV27tzJt8vkezJ79mw8/fTTuOqqq3DYYYehsLAQCxcutDz+HXfcgZNPPhlz5szB97//fR6NXlJSgptuuinl2jMlGo3i//7v/xKeLy8vx2WXXYbbbrsNl1xyCebPn4/zzz+fR6OPHj0av/jFLwAAmzZtwnHHHYdzzz0XU6ZMgcPhwHPPPYd9+/bxqPrHHnsMf/7zn3HmmWdi3Lhx6Orqwl/+8hcUFxcPqEAkCIKgaHSCIIhekCwaferUqZbbv/vuu8qRRx6peL1eZejQocqvfvUr5dVXX1UAKG+++SbfLlk0ulVMNADld7/7HX+cLBr98ssvT3ivOb5bURTl9ddfV2bNmqW4XC5l3Lhxyl//+lfl6quvVjweT5KroLNhwwbl+OOPVwoLC5XKykrlhz/8IY9gF2O6k8VnW629paVF+e53v6sUFxcrJSUlyne/+13ls88+y0o0unjdGPPmzVMAKD/4wQ8s9/m3v/1NmTBhguJ2u5VJkyYpS5cutVy31bX99NNPlSOOOEJxuVzKyJEjlbvvvtsyGj3d70l3d7dywQUXKKWlpQoA/p2xikZXFEVZtWqVMm/ePMXr9SrFxcXKwoULlQ0bNhi2YefS1NRkeN5qnVaw2Hurn3HjxvHtnn76aWXWrFmK2+1WysvLlQsvvFDZvXs3f725uVm5/PLLlUmTJikFBQVKSUmJcsQRRyj/+te/+DZr1qxRzj//fGXkyJGK2+1WqqurldNOO0355JNPUq6RIAgi20iKkkd/GiUIgiDyijPOOAPr16/H5s2bc70UgiAIgsg7qGeKIAiCAAAEg0HD482bN+Oll17CMccck5sFEQRBEESeQ84UQRAEAUANK1i8eDHGjh2Luro6PPjggwiHw/jss88wYcKEXC+PIAiCIPIOCqAgCIIgAAALFizAP//5TzQ0NMDtdmPOnDn4wx/+QEKKIAiCIJJAzhRBEARBEARBEEQvoJ4pgiAIgiAIgiCIXpBTMfXggw9ixowZKC4uRnFxMebMmdPjgMhnnnkGkyZNgsfjwfTp0/HSSy8N0GoJgiAIgiAIgiB0clrm95///Ad2ux0TJkyAoih47LHHcMcdd+Czzz7D1KlTE7Z/7733cPTRR2PJkiU47bTTsGzZMtx2221Ys2YNpk2bltYxZVnG3r17UVRUBEmSsn1KBEEQBEEQBEEMEhRFQVdXF4YOHQqbLXOfKe96psrLy3HHHXfg+9//fsJr5513Hvx+P1588UX+3JFHHomZM2fioYceSmv/u3fvxogRI7K2XoIgCIIgCIIgBje7du3C8OHDM35f3qT5xeNxPPPMM/D7/ZgzZ47lNu+//z6uuuoqw3MnnXQSVqxYkXS/4XAY4XCYP2bacdeuXSguLu77wgmCIAiCIAiCGJR0dnZixIgRKCoq6tX7cy6m1q1bhzlz5iAUCqGwsBDPPfccpkyZYrltQ0MDampqDM/V1NSgoaEh6f6XLFmCm2++OeF51qdFEARBEARBEMSBTW/bf3Ke5jdx4kSsXbsWH374IX7yk5/g4osvxoYNG7K2/+uuuw4dHR38Z9euXVnbN0EQBEEQBEEQBy45d6ZcLhfGjx8PAJg9ezY+/vhj3HfffXj44YcTth0yZAj27dtneG7fvn0YMmRI0v273W643e7sLpogCIIgCIIgiAOenDtTZmRZNvQ4icyZMwevv/664bmVK1cm7bEiCIIgCIIgCILoL3LqTF133XU4+eSTMXLkSHR1dWHZsmVYvXo1Xn31VQDARRddhGHDhmHJkiUAgJ///OeYP38+7rrrLpx66ql46qmn8Mknn+CRRx7J5WkQBEEQBEEQGaAoCmKxGOLxeK6XQhwAOJ1O2O32ftl3TsVUY2MjLrroItTX16OkpAQzZszAq6++ihNOOAEAsHPnTkPe+9y5c7Fs2TL89re/xfXXX48JEyZgxYoVac+YIgiCIAiCIHJLJBJBfX09AoFArpdCHCBIkoThw4ejsLAw+/vOtzlT/U1nZydKSkrQ0dFBaX4EQRAEQRADiCzL2Lx5M+x2O6qqquByuXqdokYQ6aAoCpqamhAIBDBhwoQEh6qv2iDnARQEQRAEQRDEgUEkEoEsyxgxYgR8Pl+ul0McIFRVVWHHjh2IRqNZL/fLuwAKgiAIgiAIYv9GbOMgiP6mP91P+iYTBEEQBEEQBEH0AhJTBEEQBEEQBEEQvYDEFEEQBEEQBEHkgNGjR+Pee+9Ne/vVq1dDkiS0t7f325qIzCAxRRAEQRAEQRApkCQp5c9NN93Uq/1+/PHHuPTSS9Pefu7cuXykUH9Coi19KM2PIAiCIAiCIFJQX1/Pf3/66adx4403YuPGjfw5cX6RoiiIx+NwOHq+za6qqspoHS6XC0OGDMnoPUT/Qs7UIGHD3k58928f4ovd7bleCkEQBEEQRNZQFAWBSCwnP+mOWx0yZAj/KSkpgSRJ/PHXX3+NoqIivPzyy5g9ezbcbjfeeecdbN26FYsWLUJNTQ0KCwtx2GGHYdWqVYb9msv8JEnCX//6V5x55pnw+XyYMGECXnjhBf662TF69NFHUVpaildffRWTJ09GYWEhFixYYBB/sVgMP/vZz1BaWoqKigr8+te/xsUXX4wzzjij159ZW1sbLrroIpSVlcHn8+Hkk0/G5s2b+et1dXVYuHAhysrKUFBQgKlTp+Kll17i773wwgtRVVUFr9eLCRMmYOnSpb1eS64hZ2qQsHzNbry9uRkH1RRhxvDSXC+HIAiCIAgiKwSjcUy58dWcHHvDLSfB58rO7fC1116LO++8E2PHjkVZWRl27dqFU045Bb///e/hdrvx+OOPY+HChdi4cSNGjhyZdD8333wzbr/9dtxxxx24//77ceGFF6Kurg7l5eWW2wcCAdx555144oknYLPZ8J3vfAfXXHMNnnzySQDAbbfdhieffBJLly7F5MmTcd9992HFihU49thje32uixcvxubNm/HCCy+guLgYv/71r3HKKadgw4YNcDqduPzyyxGJRPDWW2+hoKAAGzZs4O7dDTfcgA0bNuDll19GZWUltmzZgmAw2Ou15BoSU4OEFn8EABCJyTleCUEQBEEQBGHmlltuwQknnMAfl5eX4+CDD+aPb731Vjz33HN44YUXcMUVVyTdz+LFi3H++ecDAP7whz/gj3/8Iz766CMsWLDAcvtoNIqHHnoI48aNAwBcccUVuOWWW/jr999/P6677jqceeaZAIAHHniAu0S9gYmod999F3PnzgUAPPnkkxgxYgRWrFiBc845Bzt37sRZZ52F6dOnAwDGjh3L379z507MmjULhx56KADVnRvMkJgaJDAxFZNJTBEEQRAEsf/gddqx4ZaTcnbsbMHEAaO7uxs33XQT/vvf/6K+vh6xWAzBYBA7d+5MuZ8ZM2bw3wsKClBcXIzGxsak2/t8Pi6kAKC2tpZv39HRgX379uHwww/nr9vtdsyePRtyL+8pv/rqKzgcDhxxxBH8uYqKCkycOBFfffUVAOBnP/sZfvKTn+C1117D8ccfj7POOouf109+8hOcddZZWLNmDU488UScccYZXJQNRqhnapDQxp2p9Gp7CYIgCIIgBgOSJMHncuTkR5KkrJ1HQUGB4fE111yD5557Dn/4wx/w9ttvY+3atZg+fToikUjK/TidzoTrk0r4WG2fbi9Yf/GDH/wA27Ztw3e/+12sW7cOhx56KO6//34AwMknn4y6ujr84he/wN69e3Hcccfhmmuuyel6+wKJqUFCKzlTBEEQBEEQg4Z3330Xixcvxplnnonp06djyJAh2LFjx4CuoaSkBDU1Nfj444/5c/F4HGvWrOn1PidPnoxYLIYPP/yQP9fS0oKNGzdiypQp/LkRI0bgxz/+MZYvX46rr74af/nLX/hrVVVVuPjii/GPf/wD9957Lx555JFeryfXUJnfIKHFHwYAROMkpgiCIAiCIPKdCRMmYPny5Vi4cCEkScINN9zQ69K6vvDTn/4US5Yswfjx4zFp0iTcf//9aGtrS8uVW7duHYqKivhjSZJw8MEHY9GiRfjhD3+Ihx9+GEVFRbj22msxbNgwLFq0CABw5ZVX4uSTT8ZBBx2EtrY2vPnmm5g8eTIA4MYbb8Ts2bMxdepUhMNhvPjii/y1wQiJqUFAIBJDKKr+44vGqcyPIAiCIAgi37n77rvxve99D3PnzkVlZSV+/etfo7Ozc8DX8etf/xoNDQ246KKLYLfbcemll+Kkk06C3d5zv9jRRx9teGy32xGLxbB06VL8/Oc/x2mnnYZIJIKjjz4aL730Ei85jMfjuPzyy7F7924UFxdjwYIFuOeeewCos7Kuu+467NixA16vF0cddRSeeuqp7J/4ACEpuS6qHGA6OztRUlKCjo4OFBcX53o5Bupa/NjVGsQ3JlQant/dFsA3bnsTAHDMxCo8esnhVm8nCIIgCILIa0KhELZv344xY8bA4/HkejkHJLIsY/LkyTj33HNx66235no5A0Kq711ftQE5U3nEj574FF83dOG1XxyNg2p0S5X1SwFAjJwpgiAIgiAIIk3q6urw2muvYf78+QiHw3jggQewfft2XHDBBble2n4BBVDkCcFIHBv3dQEAvtjdYXitRRBTEeqZIgiCIAiCINLEZrPh0UcfxWGHHYZ58+Zh3bp1WLVq1aDuU8onyJnKE7Y2dYMVXG5u7DK81mZwpkhMEQRBEARBEOkxYsQIvPvuu7lexn4LOVN5giigtuzrNrwmlvlRAAVBEARBEARB5AckpvKEzYKA2txoFFMtBjFFzhRBEARBEARB5AMkpvIEUUDtagsgGInzx20kpgiCIAiCIAgi7yAxlSds3qeX+SmK2kPFEJ2pmExlfgRBEARBEASRD5CYygNC0Th2tgYAAGMqCwAAWwSnytAzFSNniiAIgiAIgiDyARJTecDWpm7IClDqc+LIsRUAjIEUBjFFzhRBEARBEARB5AUkpvIA5kJNqC7EQTWFAIyBFK3UM0UQBEEQBLHfs2PHDkiShLVr1/b7sR599FGUlpb2+3H2d0hM5QGbtH6pCTVFmFBdBEAXWNG4jI5glG8bo2h0giAIgiCIAWfx4sWQJCnhZ8GCBbleWo+MHj0a9957r+G58847D5s2ber3Yx9zzDG48sor+/04uYKG9uYB25r8AIDxVYWYoDlTO1r8iMRktAcjhm0j5EwRBEEQBEHkhAULFmDp0qWG59xud45W0ze8Xi+8Xm+ulzHoIWcqD+gMqc5TRaELlYXqP0hZAbpCUbT5o4ZtYySmCIIgCILYn1AUIOLPzY+SWcWP2+3GkCFDDD9lZWUAgAsuuADnnXeeYftoNIrKyko8/vjjAIBXXnkF3/jGN1BaWoqKigqcdtpp2Lp1a9LjWZXirVixApIk8cdbt27FokWLUFNTg8LCQhx22GFYtWoVf/2YY45BXV0dfvGLX3A3Ldm+H3zwQYwbNw4ulwsTJ07EE088YXhdkiT89a9/xZlnngmfz4cJEybghRdeSO/iJeHf//43pk6dCrfbjdGjR+Ouu+4yvP7nP/8ZEyZMgMfjQU1NDc4++2z+2rPPPovp06fD6/WioqICxx9/PPx+f5/WkynkTOUBAW2mlNdph90mwe2wIRyTEYzG0eIPAwAqC11o7o5AVoC4rMBuk1LtkiAIgiAIYnAQDQB/GJqbY1+/F3AVZGVXF154Ic455xx0d3ejsFCtNHr11VcRCARw5plnAgD8fj+uuuoqzJgxA93d3bjxxhtx5plnYu3atbDZeudxdHd345RTTsHvf/97uN1uPP7441i4cCE2btyIkSNHYvny5Tj44INx6aWX4oc//GHS/Tz33HP4+c9/jnvvvRfHH388XnzxRVxyySUYPnw4jj32WL7dzTffjNtvvx133HEH7r//flx44YWoq6tDeXl5xmv/9NNPce655+Kmm27Ceeedh/feew+XXXYZKioqsHjxYnzyySf42c9+hieeeAJz585Fa2sr3n77bQBAfX09zj//fNx+++0488wz0dXVhbfffhtKhgK5r5CYygPYgF6vyw4A8LnsqpiKxLkzVV3kQXO3WvIXjcuw2+y5WSxBEARBEMQByosvvsiFEuP666/H9ddfj5NOOgkFBQV47rnn8N3vfhcAsGzZMpx++ukoKlJ74s866yzDe//+97+jqqoKGzZswLRp03q1poMPPhgHH3wwf3zrrbfiueeewwsvvIArrrgC5eXlsNvtKCoqwpAhQ5Lu584778TixYtx2WWXAQCuuuoqfPDBB7jzzjsNYmrx4sU4//zzAQB/+MMf8Mc//hEfffRRr3rH7r77bhx33HG44YYbAAAHHXQQNmzYgDvuuAOLFy/Gzp07UVBQgNNOOw1FRUUYNWoUZs2aBUAVU7FYDN/61rcwatQoAMD06dMzXkNfITGVB4SiujPF/rcNUQQicXQJJYCMaFyGR9v27pWbEInJuPbkSQO8aoIgCIIgiCzg9KkOUa6OnQHHHnssHnzwQcNzzJFxOBw499xz8eSTT+K73/0u/H4/nn/+eTz11FN8282bN+PGG2/Ehx9+iObmZsiy2r6xc+fOXoup7u5u3HTTTfjvf//LBUYwGMTOnTsz2s9XX32FSy+91PDcvHnzcN999xmemzFjBv+9oKAAxcXFaGxs7NXav/rqKyxatCjhmPfeey/i8ThOOOEEjBo1CmPHjsWCBQuwYMECXmJ48MEH47jjjsP06dNx0kkn4cQTT8TZZ5/Nyy4HChJTeUDA5Eyx/w1E4vBrrxV7nXx7lujXFYrij69vBgBcevRYlBfogosgCIIgCGJQIElZK7XrbwoKCjB+/Pikr1944YWYP38+GhsbsXLlSni9XoNjs3DhQowaNQp/+ctfMHToUMiyjGnTpiESiVjuz2azJZStRaPGfvprrrkGK1euxJ133onx48fD6/Xi7LPPTrrPvuJ0Og2PJUniojDbFBUVYc2aNVi9ejVee+013Hjjjbjpppvw8ccfo7S0FCtXrsR7772H1157Dffffz9+85vf4MMPP8SYMWP6ZT1WUABFHhA0OVM+l6pxQ9E4gpEYAKDY4wBrk2Kzphq7wnwf4iwqgiAIgiAIYuCZO3cuRowYgaeffhpPPvkkzjnnHC4+WlpasHHjRvz2t7/Fcccdh8mTJ6OtrS3l/qqqqtDV1WUIVTDPoHr33XexePFinHnmmZg+fTqGDBmCHTt2GLZxuVyIx+MpjzV58mS8++67CfueMmVKD2fde5Id86CDDoLdrt4XOxwOHH/88bj99tvxxRdfYMeOHXjjjTcAqEJu3rx5uPnmm/HZZ5/B5XLhueee67f1WkHOVB7Ay/xSOFNepwNOuxpMEZXVv1A0CWKqPUBiiiAIgiAIoj8Jh8NoaGgwPOdwOFBZWckfX3DBBXjooYewadMmvPnmm/z5srIyVFRU4JFHHkFtbS127tyJa6+9NuXxjjjiCPh8Plx//fX42c9+hg8//BCPPvqoYZsJEyZg+fLlWLhwISRJwg033JDgFI0ePRpvvfUWvv3tb8PtdhvWy/jlL3+Jc889F7NmzcLxxx+P//znP1i+fLkhGbC3NDU1JYjA2tpaXH311TjssMNw66234rzzzsP777+PBx54AH/+858BqD1q27Ztw9FHH42ysjK89NJLkGUZEydOxIcffojXX38dJ554Iqqrq/Hhhx+iqakJkydP7vN6M4GcqRwTjcuIamV7PqeqbZlDFYjEEAirzlSB2w6nXf24orFEZ6o9YLR8CYIgCIIgiOzyyiuvoLa21vDzjW98w7DNhRdeiA0bNmDYsGGYN28ef95ms+Gpp57Cp59+imnTpuEXv/gF7rjjjpTHKy8vxz/+8Q+89NJLmD59Ov75z3/ipptuMmxz9913o6ysDHPnzsXChQtx0kkn4ZBDDjFsc8stt2DHjh0YN24cqqqqLI91xhln4L777sOdd96JqVOn4uGHH8bSpUtxzDHHpH+BkrBs2TLMmjXL8POXv/wFhxxyCP71r3/hqaeewrRp03DjjTfilltuweLFiwEApaWlWL58Ob75zW9i8uTJeOihh/DPf/4TU6dORXFxMd566y2ccsopOOigg/Db3/4Wd911F04++eQ+rzcTJGWg8wNzTGdnJ0pKStDR0YHi4uJcLwedoShm3PQaAGDj/y2A22HHT/7xKV7+sgG3LJqKdbs78Mynu/GrBRPxyFvb0B6IYtVVR2N8dRH+9s523PriBgDAHWfPwDmHjsjlqRAEQRAEQaQkFAph+/btGDNmDDweT66XQxwgpPre9VUbkDOVY0JaGZ9NAlya88TK/IKROA+nKHA5uDMVian6t7ErxPdDzhRBEARBEARBDCwkpnIME0s+l4NPpPYJPVMBLYDC57LDqSVQxLQ6WLFnqo16pgiCIAiCIAhiQCExlWNYkh+bGwXoaX7BqB5A4XM54HRoPVNxKzFFzhRBEARBEARBDCQkpnIMj0V36R+FRwygYM6U2w6H5kyxwApK8yMIgiAIgiCI3EFiKsewnimW5AfoZX7BiKyXATqFND+LOVNU5kcQBEEQxGDhAMs/I3JMf37fSEzlGCaWPC6xzE8TU9EYAmEtgMKtB1DE4gqicdkwqJcCKAiCIAiCyHfYANtAIJDjlRAHEpGIes/MBgFnExram2N4mZ9T17X6nCk9gMLrssNpZ2V+Mpq7w4b9kJgiCIIgCCLfsdvtKC0tRWNjIwDA5/PxAC6C6A9kWUZTUxN8Ph8cjuxLHxJTOUYXU7pS9hrS/PRodAcv81N4v5RNAmSFyvwIgiAIghgcDBkyBAC4oCKI/sZms2HkyJH9ItxJTOWYoJDWx2Blfp3BKGKyWuPpc9v5HKqYLKOxUxVToysKsK3Zj3BMRjAS50KMIAiCIAgiH5EkCbW1taiurkY0SpU1RP/jcrlgs/VPdxOJqRxjFY3u1cIomrt1t8nntMOhlflFYjL8YVVMjakswM7WAGKygrZABF6Xd6CWThAEQRAE0Wvsdnu/9LAQxEBCARQ5hjlTYjQ6c6Za/apgcjlscNhtegCFrJf5VRe7UepzAaBSP4IgCIIgCIIYSEhM5RjmTIllfqxUT6vw4+JKDKBo7AoBAKoK3Sjzqck4FEJBEARBEARBEAMHiakcw5wpY5mf0fIu0ISW0yKAoqrYgzLNmSIxRRAEQRAEQRADB4mpHGOV5uczhUiwxw6bPrSXDeytKnSjRHOmqMyPIAiCIAiCIAYOElM5Rk/zE8WUMReEveZyqGV+MWFgb1WRSyjzIzFFEARBEARBEAMFiakcY+VMuR3Gj4WJK+ZMReIKH+brczl4mV8blfkRBEEQBEEQxIBBYirH8J4pwZmy2STLsj+e5heXDcN8s5nmd/drG7Hg3rfQFSJhRhAEQRAEQRCpIDGVYwIszc9p3ScFAD43C6DQ50xxR8tlz2qa34q1e/F1QxfW7eno874IgiAIgiAIYn+GxFSOCUV0USQiPi4wOVPd4RgUITY9m84UKx+MxZU+74sgCIIgCIIg9mdITOUY5jB5UjhTTFg5NGeqI6g7UF6n7kx1ZMGZ8ofV9cRkuc/7IgiCIAiCIIj9GRJTOSZgkeYHGAMpzHOmmJjyOG2w2aSsOVNxWeHiLhIjZ4ogCIIgCIIgUkFiKseELNL8AGOZn5eX+anOVKcWDsFS/rgzFYxClnsvgpiQAsiZIgiCIAiCIIieIDGVQxRFMQRJiIizpsw9U8yZYgKMOVOyogut3hAIx/jv0TiJKYIgCIIgCIJIBYmpHBKJy4hrTlKqAAqW5ufQxFRnkM2YYsN8bVxw9WXWlD+iO1NRCqAgCIIgCIIgiJSQmMohoYju/iSU+VnMmXIllPnp22Sjb4ol+QHkTBEEQRAEQRBET5CYyiGsxM9hk3gJH8PnSgygcNjUbVgsuuhelRWwWVN9EVNCzxQ5UwRBEARBEASREhJTOYQ5QWZXCkgSQOEwCy69r6pMc6b6MrjXTz1TBEEQBEEQBJE2JKZySLLwCQDwOcUACi0a3SYZtvFalvn1IYBiEPZMKYqCbkEEEgRBEARBEMRAQWIqh4RSiCmvS/9ofG5jmh9/XnC0Sr19L/MbjM7UL5/9AofcuhK7WgO5XgpBEARBEARxgEFiKocwJ8i6zE93plj/lMNudKbEvio2a6pvARRiz9TgEFPrdncgEpOxpbE710shCIIgCIIgDjBITOWQYCRVmZ+Y5qcKK5fJmRIFVzbK/Pximl8fhv8OJFFtuPBgcdIIgiAIgiCI/QcSUzmE90xZOFOi66Q7U8kT/7KS5hcWeqZig0OcsNTBwdLjRRAEQRAEQew/5FRMLVmyBIcddhiKiopQXV2NM844Axs3bkz5nkcffRSSJBl+PB7PAK04uwRTlPl5+GwpG++VcqYo8+POlD8KWVawpbGLDwROF9GZig0SZ4qVI8bkwSH+CIIgCIIgiP2HnIqp//3vf7j88svxwQcfYOXKlYhGozjxxBPh9/tTvq+4uBj19fX8p66uboBWnF1Sp/mpz7HwCcAigMIiGr0jGMWTH9bh+LvfwrcfeR8NHaG01yM6U5FBUjYXIWeKIAiCIAiCyBGOnjfpP1555RXD40cffRTV1dX49NNPcfTRRyd9nyRJGDJkSH8vr99JVeZXWeRW/7fQzZ9LNdhXDKD4aEcbAODjHW045Y9v47nL5mJURUGP6wlEB18ARYx6pgiCIAiCIIgckVc9Ux0dHQCA8vLylNt1d3dj1KhRGDFiBBYtWoT169cn3TYcDqOzs9Pwky+kCqAYV1WI+749E/ecO5M/Z07zM8yZ8qrOVCASx/q96nUs8jjQ6o9g2Uc701pPwBCNPjBOT6SPvVmsZ2qwiD+CIAiCIAhi/yFvxJQsy7jyyisxb948TJs2Lel2EydOxN///nc8//zz+Mc//gFZljF37lzs3r3bcvslS5agpKSE/4wYMaK/TiFjUokpAFg0cximDy/hj81pfqIzVeRxgM303daklkn+6OixAIA3v25Maz2GNL8BECfbm/2Yectr+MNLX/V6H2ydVOZHEARBEARBDDR5I6Yuv/xyfPnll3jqqadSbjdnzhxcdNFFmDlzJubPn4/ly5ejqqoKDz/8sOX21113HTo6OvjPrl27+mP5veLbh4/EXy86FN+aNTyt7VPNmbLZJB5CAQBuhw0XHDEKdpuETfu60xpqK86ZGggxtX5vBwKRON7Z3NzrfbCgDCrzIwiCIAiCIAaavBBTV1xxBV588UW8+eabGD48PWHBcDqdmDVrFrZs2WL5utvtRnFxseEnXxhfXYjjp9Rg4pCitLY390x5ncaWt1KtbwpQywTLC1yYPbIMAPDmxp7dKb9Q5hcbAKeHCaDexrkrisITCwdL+iBBEARBEASx/5BTMaUoCq644go899xzeOONNzBmzJiM9xGPx7Fu3TrU1tb2wwrzC6cteZkfoCf6AapQA4BjJ1UDAN5Io9RPdKYGIs0vGlMFUG8HDYulfeRMEQRBEARBEANNTsXU5Zdfjn/84x9YtmwZioqK0NDQgIaGBgSDQb7NRRddhOuuu44/vuWWW/Daa69h27ZtWLNmDb7zne+grq4OP/jBD3JxCgOK05G8zA/QE/0AYIImpr6pian3t7bwHq1kDLQzxQRbMBpHKJp6bVaIs6VITBEEQRAEQRADTU7F1IMPPoiOjg4cc8wxqK2t5T9PP/0032bnzp2or6/nj9va2vDDH/4QkydPximnnILOzk689957mDJlSi5OYUBxmJwpc3CF2DM1oUYVUwfVFGJoiQfhmIy1u9qT7ltRlAHvmRIT+DqCmbtTojM1EOKPIAiCIAiCIERyOmdKUXq+AV69erXh8T333IN77rmnn1aU3zgTAihMPVNe3ZkaX632YUmShIOGFGFvRwg7WvyYM67Cct+RuGzoO4oOQA+SKIbaAhHUFHsyer8oxijNjyAIgiAIghho8iKAgkgPSZLg0PLPXQ4b7DajuCorUJ0ph03CqAoff360NrB3R4s/6b4DYWOZXbSP85/SQezLavP3zZmiMj+CIAiCIAhioCExNchg8ejmfilAT/MbU1lgSP5jwqquOXk8esDUsyT2I/UXogDqTaKf+P6BWC9BEARBEARBiJCYGmQwkeRzJoqpQ0aWwe2w4fgpNYbnR1em40zFDI8HomxOFEO9SfQzlCVSmR9BEARBEAQxwOS0Z4rIHCamzOETADC5thhf3HQi3A7ja6zMr64lAEVRIEmqu9UdjqHAZYckSfCbkv4GomzO3DOVKWLPVIzK/AiCIAiCIIgBhpypQYaTl/lZ62CzkAKAYaVe2G0SgtE4GrvCAID1ezsw8+bXcOuLXwGwcqYGoGcq1tcyP3KmCIIgCIIgiNxBYmqQweLRrZypZLgcNgwr9QIAdjSrpX4fb29FTFawepM6zJc5UwXafgcialzsc+pdmR/NmSIIgiAIgiByB4mpQYbLoX5kBRmIKUAIoWhRQyj2tAf540hMRiCiOlMlWrx6ZCDK/GK6YOurMxUbgCh3giAIgiAIghAhMTXIYNHoycr8kmGOR9/bHgIAxGUFdS1++LVo9GJNTA2EM2VM8+uFMxUnZ4ogCIIgCILIHSSmBhmpAihSYXamdmvOFABsbuzmzhSLVx+QnilDml8vAihkmjNFEARBEARB5A5K8xtkOFPMmUrFGC0efXszc6Z0MbWlsRuKpktYmV9MVgzJf/1BX50pw5wpCqAgCIIgCIIgBhhypgYZvXemWDy6H6FoHE1aqh+giinuTHld/Pn+TsgT998ejEJRMjueMc2PnCmCIAiCIAhiYCExNchwMGfKmZmpOKJcjUf3R+JYs7PN8Nrmxm74WQCFVuYHqGl54Vg8Y5GTLqIAissKOkOxFFsnYuyZImeKIAiCIAiCGFhITA0ymDOVaZmf22HHhOpCAMCrXzYAALxOdR/bmrrRHTKm+QFqXPm8//cGvv/YJ31etxVmNynTRL+oLKb5kTNFEARBEARBDCwkpgYZbi0a3efOTEwBwMHDSwEAr6xXxdTsUWVwOWwIx2S8s6UZAFBZqJf5bdrXhebuCD7Y1tLHVVtjdpMynTVFzhRBEARBEASRS0hMDTLOnj0Ch48pxzETqzN+7/ThJQCAfZ1qv9SIch/GasEUzd0RVBW5sWBqLQ+56Ayq4iYQiSMci2dj+QbMzlSmiX4x6pkiCIIgCIIgcgiJqUHGgmlD8K8fzcGwUm/G72XOFGNYqQfjtNI/APi/M6ahxOeEw6Z+LbqEHqaOoO4abW/244E3NsMfzqzHyUwkpgogFhiYeZkfpfkRBEEQBEEQuYPE1AHExCFFcNn1j3xYmRezR5YBABYePBQnTR0CQI9f7wzpAqpDKMH74+ubcedrm/Dsp7v7tB7mJlUUuAEAbf5My/yoZ4ogCIIgCILIHTRn6gDC5bBh8tBifL6rHQAwtMSLU6cPxcQhRTh8TDnfjoVciM5Uu+BMsXK8rxs6+7Qe1udUXeRGc3fYcIz03q8LKOZyEQRBEARBEMRAQc7UAcbBWt8UoDpTLocN88ZXcgEF6GKqUxA34lDdYETtn9q0r7tPa2FiqKrIrR0jw54pQ5oflfkRBEEQBEEQAwuJqQOMGVrflE0Caoo9ltuwWVYGZ0oQOiHNBdq8r6tPM6iYmCrTZlv5w5mFXIhpftQzRRAEQRAEQQw0JKYOMA4fXQ67TcJBNUUGN0qE9VUZeqYElyqkOVOdoRgau8K9Xgsr8yvyqGIqlGFiYEQQUJG43G/DhQmCIAiCIAjCCuqZOsAYWeHD85fPQ2WhO+k21s6UUOYX1UXPpn1dSR2unmDOVJFH/RqGo5n1PcVMcehxWeFrJwiCIAiCIIj+hpypA5Bpw0owpCS5ALLsmQoKZX6CmNrch74pJqYKmZjK0Jky90lR3xRBEARBEAQxkJCYIhJwWJT5JXOmNjd29eoYiqLoZX5uVUyJIi0dzIN6IzS4lyAIgiAIghhASEwRCbgsyvzEnimxHK+3iX5Rod+J90xlXOanpHxMEARBEARBEP0JiSkiAYdN/VoEIrpTxJypWFw2OEC9TfQTXSXWM5WpM2Ue1GvuoSIIgiAIgiCI/oTEFJGA05H4tWA9UyHTcNzeJvoZxVTv0vyiJieKyvwIgiAIgiCIgYTEFJGA05aYiMecKdE9Gl3hAwBsbcy81I8JIUkCCtx2bd99S/OjMj+CIAiCIAhiICExRSRgNX+qKxRDLC4jqJX+eZw2DC9TxVR9RyjjYzBnymm3weNUxVQ40wCKhDS/1GJMlhXD8GErFEVBQy/OhyAIgiAIgjjwIDFFJJBsVlNnKMadKa/TzuPVGzr7IKZsEtxaWaG5hLAnzM6UuezPzDXPfI5D/28Vtjf7k27zpze34Mglr+PV9Q0ZrYUgCIIgCII48CAxRSTgsnCmAKA9EOGleF6nHbWamKrvCGZ8DC6mHLozFYnJkDOYFWUu6zNHpZvZUN+JmKzgi93tSbdZv7cTALBud0fa6yAIgiAIgiAOTEhMEQkkc6bag1E+Y8ojOlO9KIuLxFQhJJb5AUA4A3fKHDjRkzPF9t2UIjCDRcC3+FOXAxIEQRAEQRAEiSkiAXPPFBuq2xGI8jI/j9OOIcV9L/Nz2W3wCOmBmcSjJ86ZSi3EWE9WU3fPYqrVn3lCIUEQBEEQBHFgQWKKSMAsppgD1R6McGfK6+qbM6UHUEhw2G1waAmCmcSjmwMn0nWmmruSu07cmeruX2cqFI2jIxDtecM8Z2dLAPEMSjP7eqz3t7YMyLEIgiAIgiDSgcQUkYDTVObHxZTBmbKhtsQLAGjujiBsEkGdoSjuWbkpqdBiwocJN1bql0k8ulk8RXtI84uwMr+0nKn+FVPnPfw+vnH7Gz2mC+Yzb29uwtF3vIklL301IMf78T8+xfl/+QC7WgMDcjyCIAiCIIieIDFFJOAwOVO1FmLK67SjzOeESyvRa+w0CpSH/7cV972+Gfe/sdnyGGI0OqCKMwAJoiwVZmeqpzlTPfVMxWUFXaEYgPR6phRF6VX4BqCGYXSFYvhsZ3uv3p8PbNHmi6VKR8wmu9tUEdWcQgwTBEEQBEEMJCSmiATEMj+7TUJloRuA6tqwOVNupx2SJAmJfkYHak1dOwBg8z7rgb5imR8AuB2ZO1OZpPnJssIDK5KJqa6QXnbXEYz2mA7459VbMWfJG3jly/p0lwxAFYzMVftyz+BNDWQln+Z5X/2BoijoDqtCt69lhbvbArj5P+vJ4SIIgiAIos+QmCIScNr0Mj/VgXIBUKPRg0I0OgAeQiE6NLKscJGwLYlrYXam3JozlUkABduHXVtvKvEjJv+1+sOWN+SsxI/RJrhTn+1s404MY9O+LgBIeL4nAmH9HL/cO4jFlCasewr+yAaBSBzsI4v1UUw99dEuLH13B5Z9tDMLKyMIgiAI4kCGxBSRgFNI1/M47SjxOQGo0ehimR+glwCKvVHbmv3o0lyE5u6wwfFhRMw9U9yZyqTMTzGsJVWZX1hwvGQFaLFI6zOLKVbq1+qP4NyH38eFf/0AiqIfg+0zkuGwYeawAMCXezozem8+wcRUTw5eNmDllwAymkVmRaf2fQwInwNBEARBEERvIDFFJOAQnCmfy45SryamTAEUADBEC6EQy/zMQ3F3NCeWU0Vj+tBecX+9KfPzulQxleqm3tyLZVXqZxZTLIRie3M3onEF+zrD6AzqN+AseTDSQ6+WGX9E38ee9qDBARtM8DK/DM+/N3SH9c+mr85UaADLEwmCIAiC2L8hMUUk4BKcKa/TjlKtzK8jTWfqi93G0rVtzYllcPqcKVW4sTS/TAIo2D7YWlLdHJuHATdbRJ+bxRQLOtjbrp/bXqGcsbfOlN/kiKzfOzjdKV1M9b8z1Sk4U33tmWKlqtEMPzeCIAiCIAgzJKaIBBw2oczPZUcpK/ML6HOmPJobxGLT6zsTnakijzrs1yrtLTHNL/MyP7YPn4uV+fWPMyX2g4m/M2cqUzHhDxvXMlj7pthn1VOKYjboFsRUtpypvu6HIAiCIAiCxBSRgDhnyuu08TK/jmAUAa1PhvU4MWdqn+ZMReMyd1pOnjYEQDIxZZ4zxaLR+6fMz1w+mImYMjhTwu/ZcqYGa6If+y70NN8rG3QZnKm+HS80gI5afxKXlUF/DgRBEAQx2CExRSQgRqN7nXYUa2JKVnQRwgQMS/Nr7AohFpexaV8XwjEZRW4Hjp1YDQDYkY4z1YsACnYTz8v8UgVQxDIXUy395EyxAAomIAdtmd8ABlD0R8/UQDhq/cnZD72HE+7+HwkqgiAIgsghJKaIBAxiymWHx2nngqVBK+djjysL3bBJqtBq9UewtUkVTpNqizC2qhCAmu4npuABFnOm+hJAkU6an7nMz2Lwa6cmpqqK1LlaLdo2YriGlTMVzrjMTxVTM4aXAgB2tPgH5Q3xQIqSrqz2TA1+Z0qWFXy2sx07WgLcQSUIgiAIYuAhMUUk4BDK/FgvE+ubYkETzFWx2SQUutXeqM5QjAuSUp8Loyp8kCT1RrjFdMNnjkZ3Z+hMKYqiR6OznqkU5V+JzlQoYRvmTI2pLACQrMxPCKBgzlSmZX6aozOy3AeHTYKi6GEXg4mBFCXZFFNMsA/mND+xtJJ6vwiCIAgid5CYIhJwmcr8AKBEK/VjooSJLAAo8qivdYai/Ka3yOOAx2nHUC063dw3lTyAIr0bc/EGkq0xkmpobwZlfmM1MdXijyAcixuEjuhSsbWmOq4VzJkqdDtQrblg+zoHn5jiPVMD7Ez1VTwM5LDh/kK85oP5PAiCIAhisENiikjAYQigMDpT5ucBPbWvKxTjA3qLNYE1tkoVJtubTGKKz5li0ehamV+a0ehiaRl3ptLomWIlfKmi0dmaW/0R7OswipyGjhAfGhvudZqfIKa0nrN9nYlOWb6jl/kNbM9UX52p3n5u+YTohg6EmCUIgiAIwhoSU0QCYs8Uix0v9boM24jOFAuo6BKcqWJNYLGSue0t1s6Uq5fR6GKZkzedaHRtv8PLVKesIxhN6KPSy/zUXq/2QBQ7WwP8fZKkulAt/oiWpKbexGaa5tetRaMXuB2oKVbFXeMgFFPBQe5MDWYRIn7/+youCYIgCILoPSSmiATEaHQ2TyrBmXIJYkpwpjo1Z4qV/o2uSOJMyaZodId1NLosK/jZPz/D/a9vNjwvulAsCTCSjjNV6ObnZ3anOgLq2lmvFwBsqFdjy0eW+1BVqAqf+o6gQYilOq4VujNlRw13pgZXmZ+iKHrP1ABEo3cLcfLxPjhKiqIgpH0XUvXY5TuiEBzMDhtBEARBDHZITBEJmKPRAaAkZZlfojPFSv/GsDI/c89UzLpnKmxyprY2deOFz/fiof9tNTzPXCibBLg0IWZ2puKygrW72hGJyYZer1Kf6rK1B3QxJcsKurQb9lKfE+XaNuv2qLHltSVe1Jaqrtbe9pChtyvjOVMR9Tg+l94z1WgRiNEb/rx6C65Ytqbf3YpIXAY7hKL0vzvSKQZQ9OFQ0bjC1xqNDV5HR/yuUwAFQRAEQeQOElNEAg5bopgyl/mxKHNAF06dQaFnSiv9GyuU+cly4l/TmUuULICCld75I3HD+yNCgAUrFTTfVC5fsxtn/Old3P/GZu4kuR3CEOKA3ofTFYqBpbeXeJ0oL1DPd01dGwBgaKkHQ7UBxWZnqrc9UwWGnqnsOFMP/28bXvyiHluburOyv2SwUjlGf7sj3SGxZ6r3xxJ78gbCUesvxOvd1yHGBEEQBEH0HhJTRAJimZ83WZmf2DOVwpkaVuqF0y4hEpOxVxh4G02IRmdzpow36WKvDHN0AL3Mz2m38cAMc6re2l3tAIAdLQE+E8rttPFzaRPEFBNtHqcNbocdh44uBwDs0aLQa0u8GKo5U/UdfXSmtJ6pQrdDKPPLjjPFrl8gkl7vWW8JRgdWTGWrZyokXJfBPLTXWOY3eM+DIAiCIAY7JKaIBMQyPz5nymsUU56kaX5MTKnbO+w2jCz3AQB2NAf4eyLJotFNoRCdgiMRiMSxpbEb3/3bh3h/W4u2fwkOu3WZ3w4t9CIQjvEyP7dDKPML6mV+TEyxCPhrTjwIZYKArC31oFZzpva2982Z6ubOlF0PoLCIas8URVH4eZqdo2yT6Ez17w29sWeqD2JKEMGDuddIXPtgFoUEQRAEMdghMUUkYBWNLvZMOWySQXDpc6bEAAoHf52l421v1kvPzGV+rGzQXOYn9sp0h2P47xf1eHtzM/7xQZ22Fhtc2j7MN5VMvHWHY5Zlfu0WzhQTUxWFbtxw2hT++tAsOlOBiB6NXlOkCrRWbaZVXxCduXRTEXuL2Znqz3j0WFw2OG19cabEdQ9mR8fgTFGZH0EQBEHkDEfPmxAHGoahvRbR6GKJH6ALp45ghDsIRjGlOlPbhBAKHo3uSB2N3hnUBY8/rIs11mPktEu8x8ssJlhZYSASNzlTWs9UMLmYAoAzZw3DF7s70OKPYEJ1IVq04b2dwaghKCPzob3qe31uB0p9TrjsNkTiMpq6whhe5stoXyJiEqJZ7GSbBGeqH0MQ2PVi9CXsQvx+De40P6FnahCLQoIgCIIY7JCYIhJwWKT5iT1THpdRTLGwiYbOEA9xYH1UgOhMiWLKHI1uHUBh6JkKx9GtPW7xh7W1StxJE52pna0BvhZ/OGbomXLYE9P8rMSUJEm46fSp/HGB28H3F4r1zpmKxGQuvgpdDkiShOpiN3a3BbGvs49iSrh2/V7mZ+6ZytCdywSx1BMA4kqWnKl+XHN/I37XB7MoJAiCIIjBDpX5EQkY5kxZiSmn8WvDXKj6djVEwWW3GXqq+OBeC2dK75lic6aS90z5wzHufLH7aadNTPPTbyrFY/kjxjK/ElOZ367WAJa+ux0AUKVFlVvBxFR3OGZwpjLpvfELvT8FbvUasRCKvg7uFa/dQDtT/XlDL/ZLAdlzpvrTTetvxO/cYC5XJAiCIIjBDjlTRAJiP5RPc6G8TjsvRzOX+bGhvayXRSzxA4Cx2qyp3W1BRGIyXA4bvxl0mKLRw+aeKbHMLxJLcCnEAArxprKuRRBTYb3Mz+Ww8ZLF9kAULd1hnPGnd9Hij6Cm2I3vf2Ns0uvCxE8gEjc4U7Ki9vWIjl4yWCKh22Hj27MQir4m+ollfgPdMxXpx5lNojsJ9C1wwVDmt58EUPT3jC+CIAiCIJJDYopIwGm3weO0IRKTuTCSJAklPieausIG1wnQAyj0x8avVXWRGz6XHYFIHD/752f45qRqPjDVZUrzi8RlxGUFdpsqsrpMARRml8Jh06PRxRvM7UJyoOpMJfZMtQcjeGdLM1r8EQwv8+LZH8/FEC2xzwrmTMVkxSDy1GMr+PPqzXj5ywaU+Zw4bcZQXHDEyIR9iLHo+vXR4tH7mOgnlhv2d5mfWaz1rzNlKvPry5ypqFEEi9+1wYQxGn3wikKCIAiCGOyQmCISsNsk3PftWQhF4wahVOpNJqaMX6NiU4y6JEmYPqwEH25vxSvrG/DK+gYUaWLCPGcKUMvVfC5tELAYjS70TDGcdkkv8xNuMHcIZX6KovdEmcv86jtUN+iw0eUphRQAFLj082z1RwyvRWIyHly9lTs2a3a2WYopJgZ9bv0aVveDM9XfZX7mOVb9WWqW4ExlKc0PUIWI3WZPsnX+IorXvlwPgiAIgiD6BokpwpKTpg5JeI45OuYyP6/TDodNSlrmBwB/vvAQ/G9TE5a8/DWausLo0kSF01TmB6jugTYKqmdnym6DQ3MWxIjoHUKZHwC0aoEVbodNcKaiqNeG8vYkpABVZHqcNoSiMk/2Y4TjccONeigqIxqXDSWTgN4zJQozFo/eqCUUdoWieG9rC+YfVJUgXFMh9nH1e8/UAA7tNYspuQ8BFImO2uAUIqILOVjPgSCIA5h4FAi2A8G2JD+t+u9FQ4Ez/pTrFRNEUkhMEWlTovUamQMoJElCkceBNi3QocjtTHhvRaEb3zpkOJ77bA+ahHI2JjbsNglOu4RoXDHc8Jqj0c031k5Dz5Q+sJY5TjZJLedq82vOlNOOMk2pRWIyj2sfmoaYAtTyvFA0ghaTM2VeF6C6NyVeazEllvmx0At2Xf705lY89L+tuHXRVHx3zui01gUMbM9UyBxAMZDOVB+ONZDzsfoTUUAN1nMgCGI/IBbpWQwl/LQD4c70j1Exvt+WTxDZgMQUkTbJnClA7ZviYsrCmWKMLDdGf7uE8j6Pw45oPGYQAj05U067LaHMr65VFUglXidcDhuausL8fW6HDT6XnQu3r+rV/6DXlnhTnTqnwO1Ac3ckocxPFH2SpJYWBiNxQ9Q6APg1EVIgiClWFsnWuLtN7feqawkgE8ID2DM1kM5UYs9UX5wp4zoznRGWL4jXuz+FLEEQBwjRUJpCSPsJaP8b9fe871R4SgBvmfBTbnpcBhTVZOccCaKfIDFFpE2pdtPvdVmJKYfwe6IzxRhVYRRTYhmc22lHV1gPi4jGZcNNe6NFQIPDJgkBFOpN5e7WID9WVyhmcMLcDpsapuF1obk7jOZuVRTVlqbnTLHyvAQxpYk+l90Gt9OGrlAMgUiiW8XL/ISeKXbtWH8YE5BMnDIaOkJwOWwoL3DBioGMRk/smer/Mr9CtwPd4VifytoSyvwGqRCJGuZMDc5zIAgiyygKEA32LIYCrYkldrFgHw4sAd7S5GJI/PEJr3tKgEHYs0oQZkhMEWkzfXgJAGDSkOKE18QhvcXeVM5UgeGxcaaVKqzYDa+5vKuhIzGgwWG38X2wG/o2bRhveYErwcVwO/S5Wc1C39PQtJ0p9f3mMj/mTLmdqvOliqlEQdNt0TPFxFR3OAZZVtCliaqOoH6MYCSOE+7+H0p8Trzz629ars0wtDfav45LQrlcP97Qs9CREq8T3eFY1uZMAYNZTInO1OB01wiCSIKiAJHuzMQQ+4n3IRVWsqcWQlwMlRqfc5cANhpbShy4kJgi0mbRzGGYO67ScrBtus6UucxPdKZY2AIrxTLHj1ul3TntEt8Hu6lkyX2lXmeCoHFrgq3UaxxCLA4lTgUrz0t0ptRjep12XgZp1bekO1NCmZ92vRRFjXG3cqZaAxF0hWPoCqtlkFbBFIaeqQGORu9PZ4q5fmUFTuxpD/Ypht1c/jhoy/yEz3owDx8miP0aRVF7gzIRQ+xHjva4+6TYHMkdIl8KoeQqIlFEEL2AxBSREVZCCjAKqJQ9U6nK/LT+qVDM2pkyu0EAmzOlBVBoN5Xtmggp8Tq5sDIfo9Snl8oNLfFCktKbNcREkNkd6Qyqa/W67PBqrpOVM8WeEwMo3A4bT0PsDotiSj/fsKmPzFpMDWCa3wBGowej6vVgwSZ90T/isGWgf+dj9SdRCqAgiIFDloFwh7FXKJ0UumA7oPThv8V2V6IoSiWGWJmdq0Bt3iUIYkAgMUVkBVFAFacQU4VuByoLXbxXyVjmpwoEJhzEGVPJcNglOFk0usmZKvG5UGASYC4upnTxl04sOqPAol9MXKvHYYdP2yZlmZ8gpsQ0xK5QjJf5tQvOlOg6dYailqJ2IOdMDWQqHhNuhR4mZLPnTO0PZX59KXskiAMKOd6zI2TZb9QOoA//zhze1GVyyQIYnF4SRQQxCCAxRWQFcVBvcYoyPwAYUe4TxJRY5sd6ptQbxS6hdC6ZOHAKzpSiqDeW7UHdmRJ7kwChZ0pYb7pJfoBRBIkwAedx6WKKOSoiVgEUgJ6G2B6I8sS/9kAEsqzAZpMMQskqhh0w9Uz1d5qftn+X3YZIXO7XMj/2fWBuXl/6s0T3Dhi8ZX6ieO1PV5Ag8pKUM4pShC+EOvp2XGdBZmLIW6Zu60z//2MIghh85FRMLVmyBMuXL8fXX38Nr9eLuXPn4rbbbsPEiRNTvu+ZZ57BDTfcgB07dmDChAm47bbbcMoppwzQqgkritPsmQKAUeU+fLazHYBJTGlC5/m1e7CnPYgKLbWutsTD50EBarAE61ly2CWDuxWNy4aeKbP4cVs4U0PTTPIDjOV5Iqy/y+Ow8Z6pdAMoAN3Zq+/QE5VkRRVOJT6nYUiruZeMEYmLQ4P13+Oygne3NGPWyNIeP5t0YeK22KtGxffnDT0r+2QCtC9OzP7jTIlpfoNTEBIEYmGTKEozkjvS1bfjuotTCKJkaXSlgMO6zJ0giAObnIqp//3vf7j88stx2GGHIRaL4frrr8eJJ56IDRs2oKCgwPI97733Hs4//3wsWbIEp512GpYtW4YzzjgDa9aswbRp0wb4DAiGMYAi9ddqZIX62dptEuw2XQixyPU3NzbhzY1NmDe+AoBahieKqZHlPi6mnHabQZBF4zI6tF6jEq8zwQFiYqpE6JnKxJnyuZKIqZDeM8WdKQsxZRVAAYhiyhiy0RaIoMTnNDgq6ThTopD7z+d7ceXTa/HdI0fh1jMS/418vqsdTrsNU4YmpjQmg4spj1MTU/3oTJlmc/UpGj02cOWJ/YkhzY/K/Ih8IOIHuhsBfzPgbwQCLT0HL2R9RlEa0dzeUsCenT8qEQRBADkWU6+88orh8aOPPorq6mp8+umnOProoy3fc99992HBggX45S9/CQC49dZbsXLlSjzwwAN46KGH+n3NhDXpBlAAqjMFGPulAOC8w0ZgT3sQTV1h7G4L4uPtbQASe5pGlPuwdlc734dDEGSxuKI7Uz6jM+XSZkwBQJngTKU7YwoACk3izCapDpLuTKUOoGB9UGWm9MBCLVyhvt0464OVLIpCKVkvmblnSlEUSJKET+pa1X13JM4Rqe8IYtGf3gUAbF9yStpBHMGIeiz2WffnDT0LjSh0WYd/ZMJ+k+ZH0ehEf6MoquDxN2kiqUn/ER+z36OZDRnXkUwuUZpzimhGEUEQeUJe9Ux1dKj1zOXl5Um3ef/993HVVVcZnjvppJOwYsUKy+3D4TDCYX3uQmdnZ98XSiRQbBBTqf/qxxL9nKYI1qMmVOGoCVV4/P0duPH59fxGt7rIw0ULoJbl2W0S4rICh90Gu02CJGnzCmWZC5BSn9MQGMFcKQAo9RrT/NLF7CgVuh3oDMX0aHRX6gAKlkhYUWgcvMvKJPe0JzpTgPGmvyupmIqbHsvwOO3Y2KCWxFj1na3csI//Ho0rcDnSFVOqO8Z65fq3Z8roTPVtzpS6TvZ9GaxlfuK6B+s5EDkgHlUdo+5G1T3yN1sLI/YjW7vgSXF4gIJqoLAK8FVYCyNzGh3NKCIIYpCTN2JKlmVceeWVmDdvXspyvYaGBtTU1Bieq6mpQUNDg+X2S5Yswc0335zVtRKJMIfC47TxxLxkTB1ajLGVBUnLyqYPKzE8ZkESXVqJXLHHiTKfWl7mtEmQJAlepx2BSBxdoRh3popNPVMsfALofZqfucyvyONUxZQWje5JMWcqJvRzlflcpv2o+91rdqY0MSUKJXYsM2FT7HcwEofbYcPXmpgKWQzyfW9LC/89Epd7/OwAQFEUQ5kf0H9iSlEUfh31NL/0xENcVgxlpID+mRS6HegKxQZtv1GEyvwIRiRgIYaSCKVga+b795QABVWqSCqoBAqrk//uKqT0OYIgDjjyRkxdfvnl+PLLL/HOO+9kdb/XXXedwcnq7OzEiBEjsnoMAhhV4YPXacdBQ4p63NbncuD1q+cnfX1ybTGfuwSoQqPArYupIo8DZT41Xp0l+Q0pVvuqtjR2Q9HuLUu8ToP4EZ2p2hLV3aosdKWMcjdjDqBgIoiX+TltvPcrEDGKHjaEV5KMc67U/WhlfqZSvDZ/YplfUmfKJJaC0TgCHXHeY2UucZNlBR9u18VUNCYDafRXR+Iydwl5mV8/uSPisdJN8/uqvhO/evYL1HcEseqq+YZrHRJEYFcohkg/uzqKomDJy18jLiu44bQpWdtvjAIo9l8UBQi1C2KoMcnvmkiKdGe2f8kG+CpVgVRYpQsly9+rKHSBIAiiB/JCTF1xxRV48cUX8dZbb2H48OEptx0yZAj27dtneG7fvn0YMmSI5fZutxtuN/2fQX9T6nPhrV8dmxD4kIxUvTkepx0H1RRhQ71aklnsdcIn7LfQ7UCZlvTn0PquajQxxUravE473A67cTiuUxdTFYVuLPvBESgrcKXdJwQkRpozZ4Y5BV5n8jI/VrJX6nUmOCbMdWkLGIWS7kylEY1uKvMLRuOoa9EbvM3hCxvqOw3HS9ddCkX07ZiY6q/eI9FN08v8kh9r1YZ9uOzJNXw9mxu7cdhotWxYdNR0Edi/QuStzc145K1tAIArjh3Pv7d9JUrR6IOLeEwtr2OuUXeT9e/+ZvX3eOKA8pTY3ZpLVJkojLho0n73lVOvEUEQRBbJqZhSFAU//elP8dxzz2H16tUYM2ZMj++ZM2cOXn/9dVx55ZX8uZUrV2LOnDn9uFIiHawGyfaWGcNLuJgq8jgMoqjI40S55jawvqtarVSPiSlWxieKMJfdWMJ2xNiKjNeVLIWP4XEmT/Nr0WZrWd1QJwvtYGInYhraa4VVmR8r8QP0VDzG/zY1pXx/MpggcdgkHrbRX84UG+Bst0ncWUzlTC19b7tB2LH0REAVHQPlqAHqf9/uW7WJPzaL2b4QlcWeqeSf2/2vb8Z/19Vj2Q+PRHmWhByhEQ1pAqhJE0TJfm9ShVSmQ1/dxbo7lNRB0gSUu5jK6wiCIHJETsXU5ZdfjmXLluH5559HUVER73sqKSmB16uGAlx00UUYNmwYlixZAgD4+c9/jvnz5+Ouu+7CqaeeiqeeegqffPIJHnnkkZydB5F9pg0rAT7eBUB1f8S5TIVuB86YNQx1rQHMn1gFQO97+rpBFWAlWjCC0Znq+19jexJTXmfyND/mTFVYiiljaAcL2Giz6plK6kwZb6pD0Ti+rhfElOl1s5hK15li5Ytepx1OzWHrr1IzJtw8DhtPf0zVM9VgipYXPwMxgINd72g/lsi9s6UZa7R5aoBREPeVaCy9nqnnPtuDbc1+rNzQgPMOG5m14++XKAoQ7jS5RkLUt+H3pl7MOpLUUAaDgyT8bnCQKmnQK0EQxCAhp2LqwQcfBAAcc8wxhueXLl2KxYsXAwB27twJm5D0M3fuXCxbtgy//e1vcf3112PChAlYsWIFzZjaz5gxXA+hKNZ6phhFHgfmjKvAgml6aScTU9u1eVRMTPmSpPn1lkKLAAoRj8sOHxvaawqgYLOxzOET6n6M+x1W6sXO1gAPrAgnG9rbslW9wXN6UBXageFSBGHFiRBcCIUC2Fivp1eKTllzdxif7DA2o6dbLsZEiddlh1O7ppFY/zg8rMzP47TDrv13IJWYauxSkzuHl3mxuy1ocKZEl4uFhESzKHDMsPI+RjZDOkTxmspdY8L7kx1tB6aYkuPqbKN0HaR4uOd9iticmiBiDpL5d0E0ecsBe15U1hMEQRBZJOdlfj2xevXqhOfOOeccnHPOOf2wIiJfmDikCD6XHaFoHJWFbsN8J3MIBKAGUAB6fLqlM5UFMeUz9UwllPk5bEKZn9FBak0Siw4gIQRjRLkqpng0erKeqQ/+DHz8VwDAQ4AxQOIp4GVFQtjtRBiqwFLuK4Hk8EIKS3jKGYfL7UVX3IGOqB3Vr/8bKC4CHF616dzhAZwe9X/Zj9MDd2sU82zbUGorQk13CBOkPSgJAeis1re3u7MSd8wCIzxOO58nlkxMBSN62MaYygLsbgsa3Cgrl6s/k/B2thrn7qRbRpkOkTQDKFhJ6Cd1bVk7ds6JhYXkOivXyDQ4VsnwursKkwijqsTHnhIqryMIgjjAoT+TEXmJ22HH3xcfho5gFGUFLvhMzpSZWtOsKN4zZUjz63uZn9OuRr9HYrLaM2QqHfS67EKaX/rOFBvayxhZ7sO7aNHT/JL1THnLgLIxQCyEjq5uuJQw3IjCJqk32zZJgRcReBEB4Afa1JvqCgAVNgBsV3YAmz9M6xqMB/CkC0AYwMfAmW4A2wDcbdrQLgoyty7SnN7UjwURV9Qex9n23ShBEYrrWnCUbQu8MR+wp9Qk8rxo6ozDBhkupwPVRaq49of1z4C5XF6XnadA9md4g9n1ymaZXyyNaPRwLM6Pub3Zj6aucFb7GrOGogDhruSzjgzx3k1AuCPzY3jLe3CQmItUBbh82T9HgiAIYr+FxBSRtxwpBESIDlOhhZiqKTHeJDJnyuWwwWW3IRKXDWl+faHQ7UBrLAKPUy9zY3gcupgyB1AwMWUVBGAWiCPK1Rs6qzlT3eEYZFmBzSYB3/yt+gPgpD+8jobOEIYUudHa1Y0TDirBR5v2YvoQN+r2tcKDKJ763kxEQgFc89SHcCtRLDl9Av7+5gZ0dHfj+0fUYlSJHYgG1b/+x7T/5Y9DQCyEts4uNLS0ocQpo8QZQygYQIEtCg8iRhcgHlZ/enPzqzEWwJ1OACEArwFPuADEAfwlcduRALZ5gCgciH/twrVuB9zv+YB1RYDDg+GyA/90hgHZg6JdhfiGM4YJX1UC/poUTlwPws8pCEC7URCbY9ez2jMV77nMz5z6+GldKxZMq83aGlIiy+pMowQxZP5dc5BioZ73KWJzpBBGpt99lVReRxAEQfQb9P8wxKCABVC47DZLh6mywG2YTSXOFvK57YgE5KyU+QFqH1arXxtQbEoI9Lrs8DnVtQaj1gEU6YipkZqY8kdUd0F0phQF6I7EeCw7gwmu0gIXGrqcWN8CNKEUnqoh2NnkQzSuwF89Gy9+sRdvxiM4dFQZyo6ci1Ufvo2vOjpxwuTDMWpCVY/n/+pHO3Ht8nU4bmw1Tpo2BL969gscO7EKSy85HIhH1RvjaIiLL+Njs0iz2kZ/3NDaga93NaLSo2B8uR3b6lvhtUUxpsRufI+su3VOxOCUY/BIACKdQIsabFMAYI4dgAygE5hmB7BP+8kGkt0gyP4dkxFwORCBCyE4cdDKKqC4OFGwpePWmR4XRdtQhCjCcCKaJCXQ0FsHtW+qT2IqFknuGCU8bgaUDNMLnQU9D4Vlv3vLqLyOIAiCyAtITBGDAjbfKVmEuM0moabYgz3t6tBb5kwBqhBrD0SzUuYH6C6Z22HhTAlDe4PROBRF4XOsUkWjF7gckCTwgcNDS72wSWoPWHsgktBv0xWyElPqNsXaue9qU69FbYkXHocd0XgMwWicp/idMl29sXZp/UPpOifdwvBk1nvEy+XsTvXH3fPw5nT44LM9uHLbWnxjVCV+f+Y0nHLHahS47Fj/iwXGDeMxPP7OJtzz8uc4ZVI5JlY68OS7m7BwShmuOGoEEAtiXV0jHnl9PcaWOXBQhRNrttbj6LFFmD+mqEdRZ3gcC+vPiYEFShyI+tUfAKMAQPx61H8N1GflsuDfAKBWMkLukID/S3TVhihO/MsVRkhxIQwnvOsKgOjw1CIOSuJQWPZ7qD3zhXrLTEl1KQbEugqyc3EIgiAIYgAhMUUMClian1WJH2NISRIxpQmxbDlTbC1upw1uu1lM6XOmFEXt02HiKlU0us0modDlQJcmVEq8TpR4nWgLRNEWiCIcNQqdpq4wVm3YhxOm1GBoqdovxsRUqXbuLKihtsQDj8uOrnAMoWicr2N0pep+OXn/UHpiiiXEFXocGb83U/QAChsfdBy3Cq6xO7A3YEMbiuEsH45oqQ8blRAmOoYCo2cBAPaE6vEfuQSHFZUhVFuGv23aBqlmDOZ/c0rvFyjLqqCyKI389oOr4ZAjGFogwe/vxo/mDcWMGrdeMtmTaOvpsYYNinbcoGFpPgCHi1/PMIDPe3+qAFT3zSyMDK6REO/tqwAcNNuKIAiC2L8hMUUMCriYskjyY7B4dEAPoBDfm3Ux5bDD6TCWGnmddkMoRSASg9dlh6IoaEkRQAGoTg8TU0VuB8p8LrQFopozZSyZ+ts72/Gfz/di/d4O3H72wYjFZS6exHMH1Jh1j9YvFozGeSgDL51k8eZphjF0MzHldsJh63mQbl9gYsrttPNjJUvza+xSBUZNsQcFPARE7xtigSAep50Pe+7zum02wOZNmAmkKAo+iKk21ERfETZ2deHEYTMxY+awvh1PY/Ytr6ErEIAHEYwuteOFH8029bcF8fGWevx99VeYXOVCY0s77EoEVx87AsUOWRd9ZpEG6EEMVgNiPaVZSWkkCIIgiP0FElPEoGD6sBK4HTYcNro86Ta1xbqYMpf5AVkUU9qNutozZSwd9DjtsNkkeJw2hKIyApE4KqDeyLMyOqtodECbWaUNnS3yOLkoagtEE0rw3tmsluo1aXOVxDLAUpNYqy31coEXisT57CUmCpm7lH6ZX1RbowMuByvz66+hvVoCn9PO7+GTCaDGTvVaVBe5efmlmObX0Kle26oiNxz2/l23uEYWp5/NAIqIrCACJyJwoknxAGWjE7bZ1rITL8vFCJdV48O2FvgjcXx/1rEorqC0OoIgCILIFiSmiEHBmMoCfP67E1MKIoMz5dUFBS/zc2anZ0p0uljPEIOJFp/LgVA0wkMoWJKf22FLiFNnsH4whybG2HECkVhCz1RbQBU0TCyIr4tCEgCGlnjgYWIqpoupQpOYSldYsJS4Io+Du0X9FTEulvmxYykK9DRDAeZMVRe7eVmk6Ezt1nrIhpf5+r08Udwvu87ZvEZigl+y/Yqfk9NhAyJxRPrpfIn9i5buMCoK8zBGnyAIIg+heg1i0OBx2nmYgxWimBIFBZtBla0ZO+zm2OO08xI5cY2ALqpYaRkf2FvgSnoOrB+syOOAJEncUfNH4lwsFZnKHFkYBCsDdNol7pyxx5WFbr6uQCQOv7Ymc/ljusKiWxBj/e3whGJs0K6d90wB1u5UYxdzpjzcDRJnfe3ShugOL/PqQ3v7SQRGY/p+2ecYSZK6l/DeuIzv/PVD3P7K1ym3YSQb2svS/Io9zozdR+LA5V+f7MLs/1uFZz7ZleulEARBDApITBH7DbWamJIkY+rfL44/CA995xAsmjk0K8dhARNuR2I0OhMmXlPPTmsgeZIfo0hL52P/ywVBOMZvxCtNgtCv7Z85MW5hzhWgCky17FB9jok6QHSmek7ze+L9Hfj1s18gLiuC4+Hk5x/rLzGliSGvyw6HIKbMfVOhaBztmltXU+zmAkYUU3s0Z2pEmU931Pqp10t0gNjnka4rtLGhC+9sacY/PqizfF1RFIOYTCYIOwVnytXPThyx/7BhbycAYNO+rhyvhCAIYnBAYorYbxhbWQi3w4ZxVYWGErASnxMLptVmLRq9IIkz5XHa+HGZ4GJlaq3dyWdMMYoEZwqApTNVaeq38nNniokpGxdOgO7KebUAimbNvbFJ4KEU3LVIcqOtKAr+38tf4+lPduGr+k50hVTRojpT/V3mp67J4zQ6U+ZEP9Y75nLYUOJ18tJOJjZlWcHudlbm5+U9VdF+cmqYaHHZdcEdiclQFAWf1rWiwzQDSqRTu77+iBqtn7hv43PJnCmD6M3QfSQOXJjL3V+hMgRBEPsbveqZ2rVrFyRJwvDhwwEAH330EZYtW4YpU6bg0ksvzeoCCSJdygpceO0XR6dM/MsGx0+uwctf1uP0g4dyIQLAIGLMZX6pBvYyzGJKdKaYWDKXKrKSu4ggpsSerKGaW8eea9acqQK3g5cb6sLC+uapqSvMSwObusOGOVOMvt6kv7+1BTYJOGJsheF5VubndtiMzpQmKELROP72znZ+XasK3ZAkCT7mTIX1dUdiMuw2CbUlHji1fSUTIn2FXQ+nXTKkJX64vRXffuQDLDx4KO4/f5blezuD6vWNywrCMdnwvbJaczJnioneYq8+D4x6poieYC63TGKKIAgiLXrlTF1wwQV48803AQANDQ044YQT8NFHH+E3v/kNbrnllqwukCAyYVRFQb83Tk8cUoQXf3oUjptcYxBToojxuYxiqqdYdAB8CC8r8zM4U5rDVWk6t1BURiwu878muxw2Q5kfm0HFbsiZMyUKTu6cxK17erY3+/nvrd0RHo2upvn1LmI8FI1jS2M3ACAYiWPx0o9wyaMfJ5QaBoU4c2PPlLrdyg37cMerG3Hd8nUA1BI/QL/+kbiMaFzG7ja1X6q2xAOH3dbvjhoXUw6bLqZiMg/B2NniT/pe5kwBulg27DtmdqYUSweL7adI6Jnqr/Ml9h/YH24s57kRBEEQCfRKTH355Zc4/PDDAQD/+te/MG3aNLz33nt48skn8eijj2ZzfQSR1xjL/EQxpYoVJgaYiLEa2MuYMrQYADBV+1+f0HfFHAWzmAKMZYBuh3HOVa1JTLUIzpT5HJLdaO8QbvybunWXqtDt4G5RpuVyN6z4Esff/T98sK0FjV0hhGNqjLyYvgcAoZgejS5JEpieYj1TYg8YoIZPAPr1B1RBqyf5qdfD2c/BGexaOu1GMcXKPrssRBKjUygB9FuJKQs3zWr2liHNz96/ZY3E/gP7jiab50YQBEEY6VU9VDQahdut3tStWrUKp59+OgBg0qRJqK+vz97qCCLPcSUr8zM5U9s0d2dUZUHSfR07sRof/eY4VGmCiQkev0WZn9thQ1xWgwjU19lwW2PPFCvz485Ud9iwb6DnAIrtzQH++85W/fdCj4Ofn9UNfio+3N4KAPiqvtMQLx+IxFEqjEHSo9HV9TtsNkTiMnfCzI5MteZMubTY+mhcQSASE5L81J07eXBG/zpTLrtN6EmL8/Nh7p4VXcJrls6UhQCMyQrMLYFsP8VCAAWV+RE9wZ0pElMEQRBp0StnaurUqXjooYfw9ttvY+XKlViwYAEAYO/evaioqOjh3QSx/2AOoGAwVykYiUFRFGzWkrEmVBem3F91kYf3MrF9dASjYJphSm0xJAmYO66CR6n7wzEhzc9Y5qcHUGjOlBaEUejWt2GDh5PdaO8QyvyYKHHZbXA77GmLksauEC5/cg3e2dysihut7K6pK4zmbt1dEtP3APDyRnZtWakfu9FjDtCwUi/mja/AObNH8Pcyd8ofTnSmuKM2AD1TbsGZYjeqViKJIZb5iUOHGexai2O2rAQW208xBVAQGcAEPwVQEARBpEevxNRtt92Ghx9+GMcccwzOP/98HHzwwQCAF154gZf/EcSBgOiqeJMEUDR1h9EZisEmqcOH04X1TLEBvYDar/XOr7+JB78zm7/eLThX5jK/YbzMz8a3FfcNAE5H6lI9scyvrkUVQXzAMJvXlKRvh7Fywz78d1097l21CVsau7k4bO4Oc4EH6DdyDDb02MudKaOYYgLwG+Mr8eQPjsT04SX8vQVCmeRuIRZdPefsOlNdoajh/CMxocxP6Fdi5xeIxJP+5Z8FUADWZX7snMVSRvO+FMUYYZ9O/D1BALozRQEUBEEQ6dGrMr9jjjkGzc3N6OzsRFlZGX/+0ksvhc/nS/FOgti/EJ0pr1WZXzSOLfvUoIWR5b6EZLZUsDS/NqEvyO2wcYFU6Nadl7CQeldV5MbwMi9KfU4Uex2G9TCsAiisXAtZVgxiao8WL85cMTGAIxpX4HJYDyRmUeDr9nTwOTYAc6bC/LHZmWLR6G7tutkF8Qbo4sBpcVyx1JI5YbxnypY9p+bLPR0440/v4jtHjsJNp0817NeRpGcKUIWtOFya0VMABROAbocN7NKZ+92CUV2sGXqmKICC6AGKRicIgsiMXjlTwWAQ4XCYC6m6ujrce++92LhxI6qrq7O6QILIZ5JFo/M5U5E4tjSpYmp8DyV+ZnRnShVTLoeNlwAC4LOUDM6UU+3Ref3q+Vhx2Ty+vcfUUGMVQGFV5revK8QFDaA7IOaBv0DqmHHmkoRjMlas3cOfb+oOo8UgpkwBFOYyP8lc5sfK6RL/U8bOsSsUw142Y6pc/WOPI4sBFJ/vbkdMVrBywz7+nN4zpUejh2Oy4VomK/XrMYBCOGen3TrinblbdpsEn8uux99TmR/RA+w7KlOaH0EQRFr0SkwtWrQIjz/+OACgvb0dRxxxBO666y6cccYZePDBB7O6QILIZ5IHUGhzjiJxbNacqXGZiilNLLE/ELtNgsEQUBHVy/zY/zrEtbmSiykekGAxZ4rFoouiCRDK/GyCM5VkThWgzzwCgA+2tfLfm7rCfPYV0HOZn900H4qLFkfif8qYoN3R7Ec0rsBhkzCk2KOdT+8i3a1o18ow97QHuYtoFDy6WDU4U0lCKDqF5/2RxJ4p3fWS+PVv80dx2ZOf4tX1DQD0613kUeeJpXIfCUKEO1PkYhIEQaRFr8TUmjVrcNRRRwEAnn32WdTU1KCurg6PP/44/vjHP2Z1gQSRz9hsEu/jMQRQOIUyP22e0oTqooz2LfbEAKrrJKLPoYoZyvys8JieFwMonClutHdoSX7ThpUYni90O7X36iIrVZhDVxLh0NIdQVNX8jI/JhI9SXqmWNmay8qZ0q7P1w1q+MfQUi8XYzwaPQs9RGI8+7o9HQDUAb3qcfQyv6gQQAEA3eEorOgK9eRM6efMrsebGxvx0roG/O2d7QB0QVbkMTqIlOZ34LG3PZhRMh/vmSJniiAIIi16JaYCgQCKitQbw9deew3f+ta3YLPZcOSRR6Kuri6rCySIfIfdLFsN7Q1GYn0u82O4k5TqqT1TepqfFeaeKes5UxZiSuuXOnh4qWFoLrtJlyRdTKZyPZKJqZisYKsmNgGjmIrLCr/595h6pswBFFZlfj7tHDdpSYqsXwrQHbVoFpwpVoYJ6GIqJg7tTeJMJbsmPZX5xURnSrserCeN7V9M8gNg6NsiDhze39qCuf/vDfzhpa/Sfg+l+REEQWRGr8TU+PHjsWLFCuzatQuvvvoqTjzxRABAY2MjiouLs7pAgsh32I28KFjKtOG86/d2cuclUzFlFkDmUjbmLvnDMX6TbFXuBiAh+MIgplIkvbEyvzGVBSjz6QOHmZgChES/FGVB5pK2mmI3ynzqjX5LkjI/8Xfm+jERFDcHUFiJKe2cNzeqYool+QHgQRmxLDg1YkDIl5qYEnumxGj0kHCNrcSULCuGgb5WfVWigGSlnEyAsevRleBMUZnfgchW7Q85W4Q/WKRCURSaM0UQBJEhvRJTN954I6655hqMHj0ahx9+OObMmQNAdalmzZqV1QUSRL7DBIwoWA4dVYbZo8q401Jb4jEk6KW7X7F8zew6MUFkjka3wutMnuaX6kablbDVFLtRUaCLqXTfzxAT6gDgoJoiVGrDiUVEZ8ogphzmnilzAEVimh9LQ2QN9ZbOVBb6QsToeqsyP70nLTHNz0x3JAaxuoo5U2LsekzYN3MF2fUNczHFeqY0Z4rS/A5I2B8LzL2IyYjEZf79IzFFEASRHr0SU2effTZ27tyJTz75BK+++ip//rjjjsM999yTtcURxGCA3aiKYspht+H+82dx9yVTV4rhE3qbkokpfziNnqkUzpQekJB488Ru5gvcDpSLYspjJaZSBVCo+5lSqzrXk2uLUVXUg5gS3DabJhqSpflZOXLmMsnh5YKYymKaX7tQ5re7TQ2hiAqOmVhGGe4hgEIs8QOA7nAcb29uwsE3v4bntRREUUCy82DXlzlTLM2PlfmJgo44cGB/dAgn+dzvXbUJf3x9M38sbkdiiiAIIj16JaYAYMiQIZg1axb27t2L3bt3AwAOP/xwTJo0KWuLI4jBgO5MGf85DS314v7zD8HIch/Onj28V/sWBUFimZ8QQMHnMSUTU8kDKPR+mjj84Rj+9OYWbNPKg5i48bkcKC8Uy/z0+UjONIQJc0puOG0KLjtmHH509FhLMSX+BT2oHVsMzzA7UyyB0CqAQhSiADBcKPPLZpofc+/Ydfhyb4chzS9ZNHqXhTMlDuwFVDH71qYmdIZieE2LXhfL/Ni8LCbCEp0pKvM7kGHfFSsx5Q/HcO+qzbh75Sb+7y5sMQaBIAiCSE2vxJQsy7jllltQUlKCUaNGYdSoUSgtLcWtt94KOUWiF0Hsj7CbaHMpHQB8Y0Il3vrVsVg0c1iv9u1zic6UtbvUbQigSK/Mz8qZisYV/HddPe54dSPu0/5azZypQrfDUOZXJLzf0cMAXEVReEnbuKoC/GrBJFQUupOU+eligt3gib1jDh5AYYxGt5wzZXKmRliIqbisQO7DTWMsLvPkvENHlQNQS/1E98gQQBFL7Ux1mcoh/ZEYmrtVsbarNaAdU12vw27j16OTO1NxbT/MmdLElCN7ThwxeGDflXAsscwvKPzhImJRDhinND+CIIi0yKyJQ+M3v/kN/va3v+H//b//h3nz5gEA3nnnHdx0000IhUL4/e9/n9VFEkQ+Y5Xmly18gmgxl/CJARQsRCLtMj/R8RJcCxam0KLdwPs1ceNz2Y1lfgYxZnSLzPgjcT4rSywP7KnMj90Aimu382h08DUD4ENpRUQR5rRLqBaO5xB6rLrCMQQjcQwp8ViuPxXtQlneIaNK8f62FtS3h3gAiepM6QEfQiCiZTR6p0lgdYdjaNaGGu/UxJQYbmHXhGxXmj1TVOZ3YMF6pkTHiWEo6YsnlgOSM0UQBJEevRJTjz32GP7617/i9NNP58/NmDEDw4YNw2WXXUZiijigOKi6COv3dva6LyoVBaIzZZ4zJfRMMQcreZlfz9HokZjMh8R2h2OIywovSyswO1OWPVN6Clg4FudzstiNvd0mGQSnlTNlLPPTYtEFt02fMyXzNQN6IqHhHAXBOKzUy/uuAPDyOAC46O8fYf2eDrz1q2MxtNSLTGD9UiVeJ7+moWjcWOZntxvWyrAKoGDleuUFLrT6I/CHYzwNsj0QRWcoyuPcHTYbF7KsPFANEFD0Ycfa90Lv28rODfKanW14f2sLfjx/nCEyn8gvIimcKfH7yGbEiduRmCIIgkiPXpX5tba2WvZGTZo0Ca2trX1eFEEMJm4/ewY+vO44TKjJbChvOvgsHCSGmObHyvE8Scr87DbJ8P4Ct9G1AdQb8YC2n+5wjLtSbPvyAl38FBqi0fUb9bc2NWH+HW/iiD+8znuJxJhuSdJvvEVnqlLrx7JK8/O4Ep2pxDS/1D1TI8p9htfE9L8vdrcjJivYnGZ8tAhL8ivzOfm1D8VkYc6UZAygEG5WraLRWSrfkGLVJfOH47zMD1BL/Xi4hUNP82NlWoqiXhvz3DGnUGqYDW75zwbc8epGfLCtJSv7I/qH1M5UXNhO/fcUop4pgiCIjOmVmDr44IPxwAMPJDz/wAMPYMaMGX1eFEEMJhx2G6qLMy8RS4cCd/KeqULBmWIlYGbRIMJCKJx2ybAv0VlizpQ/HEMgrP7u0IRYuaFnKjGA4m/vbMdFf/8Iu9uC6ArF+Mwl88wjRpXgTLFwCGOaX/IACn1orx4TbkZ0psRYdHE/AHgUtDgvKl2YYCwrcHH3T3Wm9GAMJqZigtMHJHOm1OeGlmpiKhJDqz/MX9/VGkBMcxGcNon3q4lEYjK/eWZrynYABTvvxq5QVvZH9A+p0vyswibImSIIgsicXpX53X777Tj11FOxatUqPmPq/fffx65du/DSSy9ldYEEcSAjOlPJyvzag1EuCEZXFiTdl8dpR2coZijxA2AYKssCILpDMUO/lCRJqChMXeb31qYmAGpJWSQmY1ebKvB4/44gwACgskjf38hyH9buardO8zP0TGnCJN5zNLoY3iEm+QGAJElw2iVD2VtrL8QUK/Mr87m4WA1F44bEvWSDlFMFUNSWqOJPUQDxlnZna4Cv2SkEUIiEY3rQhe5MZTeAgjmhbf7Evi8iNUvf3Y4v93TijrNnGEpP+wP2PYzEZciyYjie6FJGLRwsElMEQRDp0Stnav78+di0aRPOPPNMtLe3o729Hd/61rewfv16PPHEE9leI0EcsIg9U+Yyv0JNaDEhVV3kTjkYmPXPmFPumBiSFd1F6o7E+M0+2ydzkhw2ySDIHKYbwmMOqgIA7GoNAkjuTFUUuHkgwwhtBpTVnCkx1t1hcqZSlvkZxFRiL5T5PeK8qHRp9bMyP5MzJcyZshooDCRxpjQxVVPshmTxNlVMqft22CVeYikiOlNubU08ZCSWnRtktvbeXLMDnfvf2IJ/r9mNTY1d/X6smCCYzO6UKJxiVs4UpfkRBEGkRa+cKQAYOnRoQtDE559/jr/97W945JFH+rwwgiBMaX4JzpSx7G9sVXJXCtD7qcyCS0zC69ACEBQFPEWOraGswIWbT58Kr9NucFvE30t9Thw+phyvbdgnOFNMTBmdKbtNQlWRG/s6wxhVrq49EImjqSuM65avQ5t2o+61SvPTbvSiPIDCoszPLZb5JZY/mkVgawbCoDscgwTRmXIKzpRsGY1uxrJnSivzK/E6UeByJAiuna1BlHj1QbxOC2cjEpMThjgz4RjOgjMVi8v8xpz1jBHpoSgKDxkJRhJDIbKN6LyGY3FDwqVVz5QouGJZCishCILY3+m1mCIIov8pSDFnymG3we2w8RugMZWp0wRZkINZhIk3+22CoNjXGU5Yw8VzRyfsVxQlk4cUc+Gyu9VY5lfsSfzPzY2nTcW6PR04bIw6oykUjePV9Q1Y9dU+fd3OxDQ/PrRXCHowIzpTI9JwpqxK1na3BfDIW9vwvXljeAllKBrHKfe9jbis4JBRZQC0nilHYs+U026DJKmCyhz+0B2OJZReMWeq2OtEgdvOxRQvnWwNYPKQIm3fkmWSXiQeTwig4CEYWYhG94f1m/A2cqYyIhyTU/YxZZtoCmdKTPNjfXhima1MzhRBEERakJgiiDwm1ZwpQHWZwjH1hnZsin4pAPBqzom5Z0osQ+sQnIZ9nWq4gM+V+j8TYqnZ5NpiXrK3qy11mR8AnDqjFqfOqEWjdqxAJMZnXDHEmz7uTLFekFjyMr8ijxPnHjocimI908r8HqueqX98sBOPv18Hu03C7xZOBQA8v3YPD/xgQrHM5+IldaGYsWcK0MSQ9lyBy64HfURiBseOiykPi1pXBe30YSX4tK4Nu9sC/KZYLSFMPO9QVE4Y4pzNAIouYT5WOzlTGSE6jaJw6S9Ed8mc6CeKq6iVM0U9UwRBEGlBYoog8hhDz5SFmCpwO9CiiYAey/yc1mV+YhiDOISWJbWZxZcZl0FMFfFEQTYnyTxA1nJt2nnKCtDQGTS8Nr5Gd9wSo9H11Dwrbj/74KTHNIc3WLksu7VSxUZt1pOiKHj0vTr+Ohuym6rMT/xfQC3hi8RlROMKukIxtPmjqO8IosDt4GV+xV6Hobdt2tBifLG7HdG4gl2akHMkCaCIxGWEWay8k4m57AVQkDPVe8TQkYF3pozizSq5TxRcMokpgiCItMhITH3rW99K+Xp7e3tf1kIQhAlDmp/FDCmxlG1Mj84UK/NL/GfvtNsQjccNCV6NrMzPnXhcEfGGfsrQYhR7nCj1OdEeiGJXW4A7U4UWzhQ/D6GUb7fmaF19wkEYXVmAYydV89fM0eipAih6Ih1nqqFDFZStmlv28Y42fFXfmbCdORqdOQIuU5kdoIraQrcDbYEobn1xA17+siFhf6ozpV+T6mIPhpf5sL3Zj23NfnWfycr8YjIP73AnRKP3/QZZdFfImcoM8doNiJgS/j2HoinK/OKJZX757Ext2teFMp/L0nEmCIIYaDK6AykpKUn5M2rUKFx00UX9tVaCOOAwzpmyLvMD1F6iVDOmgOTOFGDteu3rSq/Mr6lLn4M0vlp1kUZofVO7WoPoCicv82M47DbuLjExNaLch4UHDzWsV+yZkmWF3/Alix9PhTllry0QgWLqE6nXxFSLNuvpsfd3AACOmlBp2E5M8wtHZcsyP4bbaefC8pX1qpAaVuo19J4VeZyG864sdPHPd7smptQAisTzDsdkfqPcH0N7/YIgIGcqMwxiakDK/FI5U0KZn0UfV746U42dIZx839u45NGPcr0UgiAIABk6U0uXLu2vdRAEYYEoZJKV+QHqnKae3BmP0zqAArB2dlgARWEPztTmxm7+O3PPRpR7sW5PB3a1BtIq81PXp/YV7dHElDgkmMHmTMmygqis3/glix9PBRt4K0lqemE0rqA7rPcwybLC+8aYa/XJjlYAwM+Pm4DtzX4u/MoKnHx/YpkdF1N20ZmyAXACCEJRVLftlSuPQn1HCEte+goOuw01xW6Dg1hZ6MaE6kI+ywtIXubHrjeQKKayUeYnCoJAJI5wLG7pmu7PNHWF8cr6BiyaORTFPXyvRXJb5pe8ZypuEUCRr85UfUcIcVnh//YIgiByDfVMEUQek64z1VOJHwAsmjkUXzd04uRptQmvWfUc8Wj0DJwpBnemhDK/VM4UO05nKMbdEysxJTpTYplS78r81H3VFnvQGoggFJXRHohyMdXsD/MbylZ/BNG4zM91ZIUPc8ZW4JlPdwMASr0unogG6KEbes+UIKYcdoOjdPDwEhR5nCjyOLH0ksP582YxNX1YiWH9Lrtked5i5DoTOezzjWThBt4c194eiKKm+MARU12hKA77/SoAqrv0g6PGpv3egQ6gMEeji/QUQJGvc6bYGrORTEkQBJENejW0lyCIgUEMIXA7E29YMxFTR46twHOXzcM00005YO16sXupnnqm7vv2TLjsNjx44SH8ueHlQpmfdnNvFY0uIvZ/AUBFoZUzpfdMiTeKfemZGlrqRblPPZbYN8X6pQA1GGNrUzdkzUmqKHBjzrgKAECR2wGXw8aj0QE9mIKJGLfD6EyJ/WNzxxlLBhmGMr8id8Ln5rDbLHummDMlSbqY49HoWS7zAw6sUr+4rOCn//yMP65rCWT0/oHumRIFfmKan9WcKSEaPU+dKfYHgWyUrBIEQWQDcqYIIo8RBYaVe3TmIcOwtakbZ80e3qfjpCqT6ynNb9HMYThleq1B0LC5TrvbMinzM4op6zI/3ZliwsBusw5i6AlWIldb6kUwGsfejpBhcG+9IKYAYMNeNXiiqtANu03C/IOqUF7gwiEj1VlTNps+T4qdszNJAIV4reZqosyMKKQrClxwFdsMserOJGV+LBHQ47BDkoxpgtG4AkVR+PO9IUFMWczn2l95fu0erN6ol1pm+r0b8ACKmBBAYXamBHGlz5nK/2h0Jviy8V0mCILIBiSmCCKPEYWM25kopo4cW4FnfzK3z8dJ5ewU9FDmZ/V+FpawszXA/5Lcc5mfLqaK3A7LPhwHd6ZkYcZU726muDNV4kG7JqLaBGeqvt3Yk8HEVE2xmiBWUejGB9cdZwiOcGt9X+xGmb1mFlPsXF0OGx/8a4Y5gkVuBxeaU4eV4KPtrdr6JcsACjarSvy+OIXjR+MKj0rvDd1h4015eyCCz3a2odjrxLiq1IOjBzvvbW0xPA5GMivVM/ZMDUCZXwpnSnR2rJwpAAlDpfMBUYRG4vIB169HEET+QWV+BJHHuB02sHsZq56pbJEqDc9cfpcOI8p8KPI4EIjE+V+4e3KmvMJxyi1K/ABrZyrZjKmeYNdzaKkXpRZlfvWdRmfqqwZVTFUXe/hzLofNcLNpdtesAijcDr3M79BRZQnvYTAhXSnEP4t9U86kZX4xw/mZj9/XUr/usNGJWrurHWf++T0cd9f/+rTfwcBnO9sAAPPGq26i2e3pCWOaX44DKCycKfM2+ehORSx6vQiCIHIJiSmCyGMkSeIipKcgiL6QypmyilLvCZfDhkvmjeGPJck4gNgKryAqrEr8AN3pkYWeqd7EogPAhUeOwjcnVePkaUNQ7lOvsdj/05CkzI85U1Z4TO6hy5Smp25jx9xxlShyO3DeYSOS7qtME3hDBPEmiimHTbJ05ViJofgXe2cWxZTf5EytWLunT/sbLLQHItjapMbSzxmriql0nCl/OIZX1zcgFI2byvwGIho9eQCF6Eyxf0vmUAw5D0MoxPOgEAqCIPIBKvMjiDzn1wsmYUtjN0ZXpJ4j1RdSuTu+XogpAPj+vDFY+s52dIVjKHQ7euxtEB2wiiRiikWji2l+vQmfAIBjJ1bj2InqQOCyAuZM6a4L65kq9qgpg23agNqaIg+S4XEkcaYMc6ZsmH9QFb646cSU1+SYiVX46TfH47jJNfw5MYTCJklwWJx7p4UzZbdJsElqkEZfE/2YIGD9WyxCH8B+3cPy2a52AGrYS22J2hMYSuNaPvy/rfjjG1tww2lTjGV+WXamZFlBOCYbHF5Dmp85gEIQTnGLOVNA/jtTFEJBEEQ+QM4UQeQ5FxwxEjcunNKvN6nizb65cqwnRykZJT4nFs8bDQBpzeLxCs5bMmeKaYe4rCQMxu0L7HhtFml+U4caU/RqilOIqYQyP4ueKU1w9fR5epx2XH3iRMwcUcqfGyukNjZ3hw39WozOYNRyLWwNfb0BZQEUVkOi8/HmO1t8trMdADBrZCkXLKE0nCkmyrc3d/drAMUPH/8ER/xhFe//A4wuZEIAhaFcLjGAAtBFVj5h6JkiZ4ogiDyAxBRBEIZysYpCYxlbT2l+qfjBUWNx/OQa/OCoMT1uayzzsy6lE52pKBdTfReZrKSOlfkpisLF1LRhxYZtqzMp87OMRu99w7zNJuG7R45CZaEbx0+psRRTVj1TgDi4t283yExMDSv1JryWjej1fIX1S80aWcY/53R6pph71dQV7tcyv7W72tEZimFbs58/FxN7pswBFDGxZ8o6gCLvxdR+/H0jCGLwQGV+BEEY3J3qIrdhEG86aX7JKPE68deLD01r23TK/ByGOVP94ExpYqrVH0EkLkOSgClDjWIqM2fKqmeqb+u99YxpuPn0qbDZjGV+kqTOBrNK8wN0YddXwdOlCYLhZRZiKqYA1h/doEaWFazVnKlDRpaiQ3P/0umZYtuYxZTZBeorTGSIa4rKSsLr+mOLMr9B5kztz+KdIIjBAzlTBEEYeqbMYsHbyzK/TDGk+SUt80tM88tGymGZz9gzxUqzKgvdCdcjIzHF5kzZs+NMMViCoOjKsVJKdsNujoxmgq6vpVF+LqYSy/zC8f4PVcgFW5q60RWOweeyY2JNEf8Mg9E0nCltm6bucL9Go7P9iXPAjGl+PZf5DQ5nSgygyL/1EQRx4EFiiiAIQ0+PmFbnstt6nZaXKYYyvyTR6GxIrSwriGg3Utl2phRF4WKqtsSDCqHk0GmXUOZL3v+VVs9UH50pEbswZ6pUWxcLYEso83Owwb3ZSfOzdKb206jqvdrMsVEVBXDYbfy7mo67xARXYplf9lyVuJBuyY4XlxWIYXzm4xnK/OJJnKl8TPOLimV++6d4JwhicEFiiiAIgyCpEtLq2ODYgSC9ND/mTMlZDaAoK3DCJqk3oPUdIXy5pwOAmtxWIQi76iJPyuAIj1nA2BLT/MyJf31BdKZKvUaR1x89U7KswB9JHkCxv0ZVs9I5Fsbi4WIq/TK/UFTutzlTolsT0I5nFs3mtYYte6ZMYioPxbHYJxUhZ4ogiDyAxBRBEAZBUu5z8t6k/pxtZSatMj9J6Jli0ehZcM7cDjuf4fT+1ha8v60FAHDEmAqU+Vxg+inVjCnA6EzZbZJQjpfdMj+Gw+BMGa9ZQppfFsr8AtE4dzsOpAAKJlDYd9SbgZhKtk02y/xEYZZMTKXqmYrFZUNCJiP/nan98/tGEMTggsQUQRAG58TndvAEv4F0psQyv4qkaX6JPVOuLKT5AcC88ZUAgNe/3sfDBo4cWw67TeI9Van6pQBjCZ/oGrlNc6ayhV1I8yv1pXam2GfcF8HD+nFsknq8YaVeeJw2LtQyLV1bt7sDR9/+Jl78Ym+v1zQQBDRB5DOJKfF7mIxkfVXZDKAQUwUD2mcUM7lKPaX5iY/Z5xmX80+s0NBegiDyDRJTBEEYBEmBy4FCLqYGzpliLpjPZU8aesF6prKd5gcA39DE1MtfNiASl1FT7MYYba4Tc8p6FlP6usV1GYb2DlSZX5Jkwb78NV8f2KsOYf7Xj+fgP1d8g8fFZyrU3trchJ2tAbywNr/FFJsnxb6joiDuyZ1KJqb6zZnSjheVkztR6mNRTMmG133aH1Hy0fihob0EQeQbJKYIgjDc+Pvcdl1MDWCZX1mBKgaGlCQXLCxwQS1Jyl4ABQAcMqoMboeNl7EdObaC90exHq5UM6YAo5gSE/xcWYxGFxGj0UtMZX6JPVN9D6BgzlShR/1eDCv1YkJNkeB6ZVYWxvbHAj/yFXOZn9th46WfPSX6JYtPF8VMKBrHEx/UYXdboFfrE/cV5GV+StJtzI9jcYU7ZXabxL+vsbx0piganSCI/ILEFEEQBuekwOXgN8u+AYpFB4AptcW4+fSpuP2sGUm3cViV+WUpbdDjtOPwMeX88ZFjK/jvrJ/q4OGlKfchChhR5PVXz5RTKPMzpwwmD6DIgjNlcix724/FRApLy8tXAlH1vFl5nyRJPEgkFEl+zrKsJIgY9jmJz7+0rh43rPgS/+/lr3u1PtEdC0RYmV/yAArWI8WIxhXuTHkcNv7vLA+1lMFB62vMP0EQRDYgMUUQhNGZctn5zXLhAJb5SZKEi+eOxqGjy5NuYxeG9rIbqWw5U4DeNwUYxdT1p0zGe9d+0/C6FYYyP4cudIzR6NkTU6l6ppIFUPRlNg+bk2QWU70VasyZavFH0gpz6E/2tgdx56sb+UBekWDE2DMF6C5VKEW5ntVrFYWquxmXFS54mDO3pbG7V2sXhZk/jQCKhKAJWebOlNtp58Ep+ehMUZkfQRD5BokpgiAMiXg+lx1F2s2ybwADKNKBpfn1RwAFABw7sRqSBIyu8GF0hR79bbNJGGqRXmcmnZ4pc3x6XxDL/Eq9PZX5aSERfSnz01yPQre5H0v9DDK9uQ0IJXAD6U7VtfjRYCot/OPrm/HAm1vwyFtbE7Y3l/kB+ueYrIwv2Wti7H9IEwadIVXA7WwNQOlFgp7o1qRT5mcOo4jKSZypfEzzE8v8yJkiCCIPGLg/OxMEkbe4hZvyAreDp/gNZM9UOth5AEV250wxJg4pwtOXzkF1kTvlPKlkiP1QrmRiKptlfoKQLEko8zM5U6yvqQ83oN3awF6zY8n2nXmZnz53qb4jhLFVhb1eW7p0haI45b63UeJ14r3rjuPPf93QBQD4tK4t4T3cmRI+Ow9zplI4aiGL6yHOLQtH4yh0O9AZVK9DIBJHiz+CysLUvXlmjNHoydL8hPI4k+iNxWUuUtxOO+8HM+8jH0jlsBEEQeQCcqYIgjCUpPlcdswcUQYAmNFDj9BAw3um4govV8vGnCmRw8eUY7SW4pcpYry7Q4xG76eeKVbmZ5PA3UR+TGf2e6b8SXqmel3mJzg3ewbImdrRHIA/EsfejhAXQoqiYKtWYrdud4ehnwjQBYo4d4191qkCKKycqWKPMyFKnjlTgOpOpcOqDftw6P+twtubmwzlhOyYTGiwfzOhFM5UXFb4tXALzlQ+zpmKGAIo8m99BEEceJCYIgjC1DPlwAVHjMQXN52IU2fU5nBVidiF8qNsR6Nng2RlfkzwOe2Soc+pr7Bj+FyOBCcqcc5UFtP8kgRQZLpv0ZnKtMzv7+9sx4J730Jzdzij94mirT2gipjGrjC6tHPzR+LY3NhlWqdFmR8f3Jv8nJlAET/yAreDfzZcTAl9WrvSFFMrN+xDc3cYb21qMogjJlBZPxYTvpGYzEsII3GjyIvGFb4Pt9MOmzAcO9+gAAqCIPKN/LkLIQgiZ7Cbco/Txm/2iz3OVG/JCQ4bi2zWxZRZNOQS49DexJI/TxZnTAHA6IoCHDyiFGfMGpqQamgWV/qcqd7fIHclCaDgZX4Z7jsQ1m+M69tTx6NH4zLe/LqRuzgr1u7B1w1d+GRHa0bHNIipYAQAuCvFYEObGcGoRQCFs+cyP/a+2hK9367Q7eCuIRMGopiqa9HF1LtbmvHYezss981EZCASt4xGj2lCSBS+bDuzAFTnTOn/nsR5bvkGlflZ0xGM4sHVW3sdr08QRO/Jn7sQgiByBrsZ9uVZj5QZ6zS/7Dk9fUUUMGLPFEvaKy3IrkB1OWx4/vJ5+L8zpieKqX4s8zM7U85eRqP7RWeqI7Uz9Z/P9+KSRz/Gfas2A9BFQypnyArRAevQnKktTSYxtavd8NjamdICKNIo8yv2OlGiDVUu8uguIlt7Z0i/DmKZ3y+f+Ry/e2E9tpicMgBo0sRUMBK3jEZnQsNKTJnj2tU5U1oAhdPOg17yUUxFKIDCkufX7sFtr3yNh/6XGKBCEET/kt93TgRBDAguXi6WX+l9ZuxCz1R/BFD0FWOZny7yRlUU4I6zZ2BURe96sdIhXWeqLzegzBUq8mSnZ0p0pnrqmWroVJ2rek10MRHT09BcM3sNzpR6PsyZGltZgG3N/gQxpUej6+ftycCZ8jptqCx0oSMYNTpT0URniompuKzwc27oCGN8dZFh381d1s5UgJf5qULI47LDJgGywpwwZ4LojQnzsFx2m+GPFvkGOVPWsD8MsDATgiAGjvy5CyEIImeMqSyA0y5hSm1xrpeSEodwk5efPVPWZX4AcM6hIwxDgbONudwxsWdKfdwWiOKqp9fi9a/2ZXwM1mNU6jPGsPN+rAyEmqIoBmeqvj2Epi61B8gKFjhidqQynU8lijY2U4o5U2fNHg4A2LSviw8oBsQAisSeqVRijq3N67JzIV1d7OZCN6z1MXVY9Ey1+iNgWqY1EDHsV1EUNHerzwWicUMfUTgmG2ZYueySfrwoc6aMa1bT/Jgzlb9iKnHYMIkpBrsW+TgbjCD2d8iZIggCQ0u9+PD641Hsye//JHBnSlZ4kpcrr8SUOLR3YNflsEmQJIAFsHlMZX5sHter6xvQHY5he4sfx02uyegYTEyVmWLYXbwfK/0buXBMhnivHozGccaf3sWe9iCWXzYXh4wsM2zPQhOYeAllwZlif83f2ugHAMwZV4GhJR7s7Qhh3e4OzBmnDm7mZX5Oq56p5OccFN53w2lTsGDqEBw9oQp/fH0LAPUaBKNx3t8EqA5cKBo3BGu0+Y1iqjMY49c6GIklrCEQiQlpfjZ4nDYEBdGVUOYnlM26HIKYyrM0P/P3y3weBzKsXzHSh6HcBEH0jvy5CyEIIqeUF7gMQ2DzEYdVmp8jf3qmRDE10CJPkiTDMZOV+THHpdV0g54OLLDBPCDY2QsxJQ7sLdcG2TLXaFuTP2F7Jp6DmnDQRVX6x1RFin7eHcEoukJRXk43rqoQkzV3dluz6lbFhRI4n0XPVDplfh6n6kyde9gIOOw2w3tZWZbdJqHQ7YCiALvbggYx1WL6rJq69bAOtczPuIZgJM7L/JwOW0KPVkKZX1wRAijseetMJQwbpmh0DnMiya0jiIEnv++cCIIgBPSeKT19LK/K/Bximd/Aizyxb8pc5me+Tma3Ix30Mj+jM+XkA4HTv7llYRYepw0jyryG11r9iXHnTACEInFEhXKvTMr8zPHr7cEItmrCrarIjRKvEyPKfQD03iVx/1ZzptIVUyJimR/rQyv2OPixd7UGUjpTTV3646CpZwpQ49H5HxtskpAeaAygYOcQlfUyP7fDBruWmpl3Ysp0nhRAoRMlMUUQOSOndyFvvfUWFi5ciKFDh0KSJKxYsSLl9qtXr4YkSQk/DQ0NA7NggiByikO4yYvynpD8EVMOuz7wNBcun+hGuU038Oayw85QjP81Ox3Em3azmOrNnCnmTBW4HIbocABo6U4UemzfwWjcUNqXiZgyh1x0BGM8fGJcldrTxATN7tagYZ2SZCyd9GguldVgXr42i/JAAMKcqTjvlyrxOjGyXL0OO1sDaBYEk7lnqkkQWoFIPMGxCURiiGpCyGGXDMcT/7fAra4rFtfL/NxOG9jfAfJNTJkdNQqg0GFlfiSmCGLgyeldiN/vx8EHH4w//elPGb1v48aNqK+v5z/V1dX9tEKCIPIJTUsZ5kwNdG9STzAXIhciz53CmXJZOGViJHdPsBI/h1aOZtg3mzNl4RS88mUDrnp6bYLoYOETXpcd5xw6HJOGFOGkqWoPl7msTdx3MGqMAldL2mSc8ad3cfmTa1KeQ4IzFYhglzaXZ7QWEMFcMuZMiX1PkqRfQzYzLGU0uhBAIcKEbjgq8yS/Yq8TIwVXLJUzxZL8AFU4hSzL/HTn1hxAEeFli+rnKJYyuu357EyZhw2TcGDozlR+fWYEcSCQ027zk08+GSeffHLG76uurkZpaWn2F0QQRF5jcKZi+RdAAajuRXc4/8r8zNHpANAWiPB+pZ5o8+slfqKoAPRztbq5ve/1zfiqvhOnTK/F8VP0wAsWi17gcuC4yTU4bnINnv54J15dv8+yn4vtOxSNIxTRjxOKyajvCGHtrnas3dWOP8oKLwc1s0cbDDyk2IOGzhA6glE0dKjPMXdsZIVWaqeJrEA0MckP0AVSqp4t9lpimZ9edqeX+TkxUhN0dS0BFHv1/3s2Xw/RmQpGrZypuCHt0uM0O1PGHrBoXOb7cDvtYP+k8i2AwlzmRwEUOlTmRxC5I7/uQtJk5syZqK2txQknnIB333035bbhcBidnZ2GH4IgBifGNL/865kC9BvnXKyLCUuXw2YheBLX0x5Iv2+KOVNs+KzVvq3Krhq1cAezIGDOlM+tC43yAjcAa2eK/cU9FI0bnJhgJG6IWBcjzc3saVOdqSlD1ZCJjmAUe7mY8gAARpSpYqo9EEVnKGo5sBfILIDCXOZnFUBRIjhTas+Ufg3aAsmdqWhc4f1njEAkxq+XwyYZerQA3aFiDmNM1ue2uR02wx8t8omEnqkDXDjsbQ/yf8Mkpggid+TXXUgP1NbW4qGHHsK///1v/Pvf/8aIESNwzDHHYM2a5KUdS5YsQUlJCf8ZMWLEAK6YIIhs4hAcBz2AIn/S/IAciynN8TC7UoD1epjblA4dPBY90clKVuYXjctcGJkFQVDomWIwl6yl2yKAQihj6hLKE8OxOPzC8N/OYBSyrODTurYEocPK/Ng8tfZAFA3aEODaUlVMFbgdqNDWsas1YCjzE8kkgMJriqkXxU0HL/NzGMr8mrrEMr8oFMElajZdH/O1DSSk+bEhwVqZnxYz72NiSpgz5XLYYMvXND9TmZ9VWemBQlcoiuPu+h/Ofuh9AHokOpX5EcTAk99DZUxMnDgREydO5I/nzp2LrVu34p577sETTzxh+Z7rrrsOV111FX/c2dlJgoogBil2QTixQar550zp7tBAw26azWVlgHU5pPkmPBVtSZL8AP0zMP9VXAySMIco+C0G4TIRY1XmJ944i45aMBI39GN1hqJYu64dP/3nZ/jhUWPwm1On8NdYAAVzpjpDUS4YmDMFAMPLfWjxR7CrNcD7h7wu4/9dutMZ2pvE1RIDIVhvU7HHiWGlXkiSus9t2iBhQBWS/kicO0lNScSU0y4hGleMZX6GND+tzE8TVQXaumKyopf5OWyG4dj5BDlTOo1dYQSjcdS1qGmU5EwRRO7Ir7uQXnD44Ydjy5YtSV93u90oLi42/BAEMTgRBQEr57JyYXIJCybIZc9Uus4Uc0XSgc+YsnKmuJgy3nyL7kq7yQXjPVNCmEVFobrvgEkgqfvWbxJFsRWKGcv8ukIxbNWEyOZGXZCI6zmophCAOuCYfY+GCImCerldkIt2Xx+cqbSi0b1OuBw2DNXWYRYOrYIwFZP+AF3oMtcwGIkjKmtDe+0Wc6a0a1nAnSnjnCmblJ9iKtH5zK/1DSTsexeNK5BlBTGZxBRB5Ir8ugvpBWvXrkVtbW2ul0EQxADgcdpRVaT21bAbqfxzpvK1zE9K+D0TZ4rPmLLomeJlfqYbOXG4bDrOVKHbwYVZi2nWlHiTyNYCWDhTwSh/XRRdkZjMxU1VocdQtlfkcRgSClmi3642vcwvIYDC2XMARU89U2qan3odirXryoQcg33f2fWTZYWX+bGqVyYymJjyR2J6mZ/dlhiNntAzJevR6IIzFcszMcUEH3PUDuQyP1FsR2WZB/IcyAKTIHJFTu9Curu7sXbtWqxduxYAsH37dqxduxY7d+4EoJboXXTRRXz7e++9F88//zy2bNmCL7/8EldeeSXeeOMNXH755blYPkEQOWBsZYHhcb5Fo7OSrlyU+TEhIs6bYojXadIQ1aFvC2TgTAWYM5UigMJ0c2twpiz6egCjSJEkifdNmUv9IsJNoigCQ1E5wZliceNimSFzgCRJFU/ieQw1zbkSe5eSB1CkEY3eQ5lfSJgzVexxGI4NqKEUNcWqmGLx6B3BKBc55vlcZQVOflw9oEVKWCsTVezay4r+mtgzJeebmNLWWKhdqwN5zpToiEZist5TeAALTILIFTm9C/nkk08wa9YszJo1CwBw1VVXYdasWbjxxhsBAPX19VxYAUAkEsHVV1+N6dOnY/78+fj888+xatUqHHfccTlZP0EQA8/YKpOYyrMAivMPH4FvjK/EMRMHfv4d6+NxOxP/0y6WSM4YXgLAKHACkRi2N/uT7ps7UxZlfsmi0Rs7dTGVkOYXZs6UsReJh1CYxZRwkyiKwFA0sWeqnYkpwd1ioqXQ7YDNJhlSCYcI/VKAPrh3V2uAi4zkzlSKnqkkzpRhzpRQ5gfo0ewAUFno4m4Tu36sX6rE60xIVmTXTu2Z0p2pUdo+3/y6CYqiJJT5AbpT6Hbo0ej55kyxdTNHjZwplUhM1num5AP3muQ7b3y9D/et2mwIkyH2D3IaQHHMMcek/FI9+uijhse/+tWv8Ktf/aqfV0UQRD4ztrLQ8Djf5kx9c1INvjmppucN+wHdmbIQU8Jz04epYkpM87v8yTV4c2MTXr3yaEwcUpTw/vYUARSuJAEUYlACe39cVmCTdGeqwG0UGqxvSuwRMu9bHGIbihrT/LpCMS4SQ1EZgUgMPpeDu1VMgIhCpNYspsrYrKlgUtHHSvWC0TgURUmIomevqdsmD6AQ50wBupADgMpCNxdIzI1jsehVRe4EgceEbiAS5yWADruEM2cNw/97+WtsqO/Emp1tCQEUgN7D5nbq0ehynt308fJE7VodyP1BYUHER+MKaGhv/nPLfzZgR0sAJ0yp4SE4xP5Bft2FEARB9MAYc5lfnompXOJKkeZXU+SBy2HDxJoi1Jaq5WFtQh/Oh9tbAQCf1rVZ7psFUGQSjW6I9w5EEIzE8c27VuO7f/tID3YwiZQK7kwl75kSy/yC0TjfF6D1TAnBGqzUTy+nsxJTxnK52lIP7DYJkZiMuhZ1eG9CmZ/2WFGSl5sxxyxlAIUwZwowlvlVFrkTnKlG7ZpWFroS1lSmCV11zhRL87Oh1OfC6QcPBQA8/n6dPrRXdKY00eiy23gARb45U2zdRdq6D2QxJfbqqc6U+lnFZSXvgkMIlU5tpIP43yti/4DuQgiCGFSYy/xy0ZuUr7hTBFCU+Jx4/ar5eOrSI/lNNxMYu9r03qAtpgQ8BiutSzW0N1Wan6wAn9S1oq4lgHe2NHNRYHamkg3ujcasxZSsGFMJO0NRPhML0EUIu5Ep9qo34qLDZnamnHYbhmmCc0O9OujdnObnEfrSQhHrm3p2w5tq4K9e5qeua5QgpqosnKk3NzYCACZUFyU4U2WCMxWVWZmfKowumjMaAPDSuno+b0uc8cXL/Jw2OOz52TPFxHohF1NK3q1xoBBnbkXiccMfMg5kkZnPhIUERmL/gu5CCIIYVIwo9xmG94q/H+joaX6JzhSgXruyAr0Ph92gf1XfxbfZosWK72kPciGiKAoXKCkDKMw9U11Gd+mL3R38940N6jETnKkkZX6RuHXPFGAUXp3BmEFccTFlKvMTe7/YwF4RJtpZH5lZEDntEuzady8US+ybist6b1JCz5T2+bR0R8Aq6ZhjVupzcuelstCFMiGQo80fwcvrGgAA5xw6PGm/WTAS5+LToX0204eXYOaIUkTjCrZp5+Rx2ng5INMkYjR6/jlTxgAK4MDtERKdqbDQMwXk3+dGqIRiFF+/v0JiiiCIQYXTbuOlUC67zbJX5UAllTMlwgRRKCojFI3jK819AYCtjd1o7ArhxLv/h7Mfeg+yrCAYjXNhkG6Zn6Io3JliomOdIKas0vwAJE/zE50p02vitvWdIcPNJIsRT13mlyimxlUZe/PMwkWSJC6SzDOxAGMwRWIAhXq92PVxO2y8FFCSJN43VVHoRrlQ5rf8sz2IxGVMHVqM6cNKEgQe+1wD0Ri/BmJAy5XHTzCuw2HnYkt/Lv+H9oox9gdqCIXoTIk9UwAl+uUjsbjM/z3FDtA/AOzPkJgiCGLQwVyDfEvyyzWjKtTrMtrUV2am0O3gN8xtgQi+btDF1J72IFZu2Ad/JI5tTX58tquNO0FOu5QgfgDrAAp/JM4DGEZraXLr9nQkvDeZu9JsLvMTSmPMf3kXxdTu1oDla6ycziqAYoipZwpILCe1Om8xhMKM+JxZ3LpNc7mKTaWTZ8waitoSD+aMreBx5y3+CP75kZpu++3DR0KSJEPpoctu42V7gbAYja4fe/5BVfjG+Er9PYJwEp+z5amYYsKpSHSmDtCSKXPPVEy4DuR85B8hQxnmgfmd3Z/JaZofQRBEb2AhFPk2YyrXnHXIMMwYXpLgqpiRJAmlPheau8No80fxdUOX4fWnPtrFf395XQPOPGQYALU0zsoJdDoSo9GZ61LodmBoqRdbm/zYo/XqiJh7pipZmV+KAAozLUJqoLnXylzmV2wSU+aBvQxzaqTZBQL0YAmreHQ9fEIXJ+b3Maq1wbyMS48eh0uPHqfuW3MgtjVp5YZOOxbNVMMkRIHndtp41HlAmDMlOk+SJOG6Uybh1D++w9dmFlMGZyrf0vxieqCH3SappZQHqAtj6JkS5kwBB/b8rXwlZEhfpM9nf4PuRAiCGHSM1cQCJfkZkSQJB9UU8bK6VLCSsL3tQZ5YN6Fava6ig/TK+gY9Ft0ifAIwBlCwcRdNQoQ3c5usKEhwplRhIfZMybKSsg+EhUtY0cLFlDE1r6ZYLe0bJcx1EhmXljNlHIYbFUp5ks2YAhKdqh/PH5d0/eVCWaXLYcPvFk7hpYpiGp/bYeeCLxCJcafCafouTB1agpsWTsE5s4dj6tCShH9D6pwpTUz1w1/QIzEZWxq7ejVrh5X5uR22pLPNDhREZyoaN/ZMkfORf4hzwWL0+ex3kDNFEMSgY7x2019gcYNLpAdL9PtwewsAoKbYjUNHl2OzluYnSWrp2O62IN7d0qy9x1oUiYmKkbgMt8OOxq4QADWRLtn7gOQ9U/5IHKFoHB6nvU9/aWfOFO+Z0lLzDh1Vht8tnILZo8os31dV5Eah24FuPmcq8bvmFYbvRuMyTrj7fyj2OvH85fO4wLIWU/pzR02oxGkzapOuv6rIjW/NGoZgNI5fL5hkKOE0OFMOG39sTPNL/IPD4nlj+O+i8JYkY7BGus5UdziGL3a348gxFQkunJnf/3cDHnu/Do9/73AcfVBVWvtnRAQx5bLbEIrKB6wLIzpTwWgc4t8aYgfoNclnRGfqQP3O7s+QmCIIYtAxe2QZrjh2PGYML8n1UgYtLM3u/W2qmJo0pJiLVPZ4VLkPr6xvwNMfq2V/JRZJfoBxcHI0rsDtMDpTopiSJEC8Ry8wldgVexxw2iVE4wpa/BEMK/X2yn1w2CTEZIWXAJqH49psEi4RRIUZSZIwtqqAJxB6nYn/d8mEUiASR0NHCDs0h68zGNPL/CxEWInXCY9TFQO3LJqWMkRFkiTcfd5My9dEoeZx2nj/WUxW+NwoRw99haLYYoEudin9nqlQNI6jb38Trf4I/r740B4HVm/ap4r1OlNvWzrozpQ96WwzM5v3daHU50KVqZRysCM6U0zwM+hmPf8IR8mZ2p+hGhmCIAYdNpuEa06aiBOnDsn1UgYtzJlav1cNn5hUW2QQU0eMKcfJ09Xry0rlaoqtb0jFG3KWJGYQUwVCcl6xhw/mtUmJJW+SJOm9P9pNYm/KlkZq5XstSaLR00HsPbPqmWJBCJ2hqCGOvaEzlNKZ8rrsePbHc/HqlUcnDKHOBDG8w+2wcyEKAPs6VGfQ1UMprCi22GdhzyCA4ub/rOfuH+vrSkVXWL1Ovel1Ym6M22mzDD0x09wdxoL73sZFf/8o42PlO2LZmN8kpgaizG9jQxfe0xxromfE8QmU5rf/Qc4UQRDEAQhzixRFLe06dXotKgp1sXTEmHKcOHUIWrojaOgMwW6T8J0jR1nuy26T9EAA7eaW9V1VF7sNM52Gl/kQV1TXyedyWLoyPqcd7Yjy+PRkN97MwbJibGUhtjX5Lcr80hdTY5OU1DFKhOHHnSYxxcp6zGETjGnD+u6qmgMoJElCZaEb9R0hdHFnKrWYEsv83Npa0xVTr61vwD+FsJJwGgKpS+tv642YYu9x2W08fCbVfvZ1hhCXFezqhQuW74hlYwFTNP9A9JF979GPUd8RxIfXH7/fuX79QdiUvkjsX5CYIgiCOABhLtTEmiLcc95MTBlaDEVRMKLci/ZAFEeMrYDdJuF730heCifitOvpam98vQ9vb26GwybhpKlDUN8e4tsNL/NCkiR8WtdmKVAAvTRODHawotTn4g4YoPZbMfE0tqoA+Eq90QxG4jykIhNnamwPzlSpVxWJ7QGjM7WvM8QdIitnKluIa/JofVhVRaqYYvRY5mczlvkB6YuptzcbnYlAJHkQCKMvYoqX+QnOVKqSNia0g9E4FEXZr2bSicLVXObX32IqFpd5MmeLP0xiKg2MzhSV+e1vkJgiCII4ADl79nBMGar2SbFABEn6/+29eZhcZZn+f5/aq7u6et+ydxayAImEJYYdEgmIDiA6qIzKqPhFYQR0XHC+gOh4gcyoIw7ifNWfcWYcUVSWQVzYwhqWBBoIkJCE7El3pzvpvWs/vz/qvO95z6lTW3d19ZL7c11cpKtOVZ1663Ty3nU/z/1o+P01pyOaSOVM4HPCawQCDMUS+Nb/vgUA+MyZbVjQGLIMtZ1VVwGfscG390sJKmxiKtuGubbCaxFTTVV+KaZm1Qbhc7sQS6aw/+iwFAaiZ6oQ1FlTFQ6iSAizvpEYelUx1ReRLl82Z6oU2J0pAGgIWTe2RZX5ea1iKt+mT6x9VcCDgUgCQ9HMiHgVXdcxYPSuxZK5j3UipvRMqQmS2RBBDMmUjnhSh88zNcRUJJ7Ed/74Ns5f0oTzljRlPUYwXOYyP/WLA6eB1SSTqPJ5MSBk+sGeKUIIOQbRNA3Hz6i2JMsBQFM4gNl1znHhuRCb9vs27cfunmE0VfnxD2sWAYBFmM2qDaLNmOHkNN8JMN0csVHL5UwJ/B6XjHsX94nXfbd7SJ6jGLRbCAubQjhxZjXOXdzoWC5Xo5T52XumOvrT7pCYmzUe2NP8gHR6oko+Z0p9X+JaEGIqlSfN77AR7jHPGBadz5mKJlJyoz+aDb9wY3yewsr8VBE+lTb9v9u8H//1wh587fevZ3UH1c35kL3Mb5zLyMQQb8AahEGyozqJMQZQTDvoTBFCCBkzIl1NRKu//8RWKZbUNL/ZtRVYMbsaH1jeigtPcA4QCRrBCiN5eqZqFfFU6fdYXKfqoBd1lT509EewyxBT4aBzj1Y2vG4XHrrujKz3CzHlVOYnytnGEjCRj6ASQCEcMHvJlceVx5lyOQRQaMU5U3PrK/DGgb68zpRIVARGWeZnCAi/xwV/AQEUamraSDyJahTuSk4kj7/dCQDoGoji5d1H8N759RnH5A6gGF+B0zdizoBTy9dIdiJ0pqY1FFOEEELGjCi76jIcGVVABX1u1FX60Dscw/zGSlT4PPj3j6/M+lxBwz0aztMzZXkNr9sSLlET9KLecIV2GSlzxZT4CXKJL/F6djHV0R+BhvTjxlNMqaWHQgjZnbC8ZX6KmBKCWLhZqRxiStd1KaaEMzWUx5kaUIYrFxJWYUc4TWlnKn2OuUSZet2MxKfGpn84lsBzO3vkzw+/ftBRTKmb84yeqXHuyTk6ZF7r0QLWtbM/gr6ROI5rrhrP05rUqNf7sTpoejrDMj9CCCFjRgQudBkbbDUOHQB++slT8NNPnoLmcCDvc4nI74h0ppw3h2qZX6XfLaPK0/d5ZQT7u91pt6yYJL9CqAlmKfPri0o3bHydKVVMCWfKur7FzJkSgswlnansm76hWFIKlLlGDP1wHmdKFVOjc6aUOVMFBVCMT5mfruu45cEt+M+Nu0v2nIJnt3cjlkhJkfunNzocnQx1c56R5qfc9/89uwu3/e+b0AscwFwIR4dNZ6oQkfp3P3sRF9/1jKW/8VhDFb/liK4n5YViihBCyJjxGZt5EQChCh0AOHluLdYszT3QVRBQhuEC5qZY7RFyabCIp6DPWuZXE/TJxMJX9/YCGAcxZbxHezR692AUg9EENM2cdzUe+D0u2d8U8Do7U/nElCUa3fgMxUY+1zgcsTGu9LllaWF+Z0op8xvFt/Pm0F6XFIG5nSlrmV82fvDoO/jmQ4ULjm2dA/jPjXvwL3/ZVtDxxfD4210AgCtOnY3aCi96hmJ44d0jGcepm/NsZX66ruOOP2/FL57bjf1HR0p2jr1F9kztPzqCeFLH7p78c8imCpv3HMU3H3rTck3nQl0nzpmaflBMEUIIGTM+26Zd7Wcqlow0P2PDbBFPXrcldrzSZzpTmiG0zljYAMDs/SkmFr0QxPMNRhPoHoxl3D+rNpgR8FFKNE2TpX5+j3PPVL4yP6/D0F6XiEbPIS66B82hzCKV0b6pt2N1popzinRdt5T5iZLEXCVThThT8WQKdz2xHeuf3429OeZR3f/qfnzx168iEk+iw4ieH4gkChpsXCiplI7Ht6bF1LrjW3DhCa0AgMeMHiqVQqLR+0bi8ncnUsIyR9WZKuR5xec2nZypu5/cgfXP78YTxueVj6hyvcezOO1k6kIxRQghZMx4bZv22orRp9iZaX7pTaLYHKrOU8DrtsSOV/jMnqlwwAuXS8OJM6stAiwcKG2bsPp8ToNhRS/ReCJK/WSanz2AIm/PVGaZn6eAaHSxMW4I+aX4tafK2RkYQwCFKhgqfR4pEosJoHCibyQOoRl392QXUz98bDseeu0gNu7sQVe/KQrsQmYs7O4ZQvdgFAGvC6vm12FJS7rHqGsgYjlO1/WczpRIi1PFy2h61LKhjgHI50wlkikpOKeTmBKfu+pI50JdpzidqWkHxRQhhJAxYxdTNWNwpoJZ5kypzlLA60bQZ75mhc8jxY14bY/bhdMXmM37pXamPG6XFGtic6UKrPnj2C8lEEJGzIgK+T1SFAHWgAkn3A5zplyu/AEUYmPcWOWXqY32eUd2LM5UkWV+oifN53Eh6HMXVOYXKyCAQu11293tXIaWSuk4aAyePtg3YhE3hW6mC0HE6c+oTjua1UpPnkoipUP9aOwiVvRYieh6oMRiytYzpes6NmzrkoN8VdTPoHtw+ogpsZ72frXsx7NnajpDMUUIIWTM+Dzj4EwZ3+aKzUel3wMRrhfwuhBQSugq/W4saq6CpgGLldSwM41SP6D0PVNApkBb3GK+9niGTwhEPLpw6TRNs7hTdpFrx5ujZ6oQZ6qxyi8DQ4bjyZwCrH8MARSiT0est7jecs3sUYMbIlk2vWr/z64sYqp7MCpFwaHeiAxZAawCcayINW0Kpz+/cDC9rnYxZS+ts6+lcOvU0tPRBH5kw57m9+bBflz1i5dx473tGceqrzudnKlYkWLK0jPFNL9pB8UUIYSQMaNu2n1ulyUsolikM2WU+cWUYa1CaAV9bgSU1wh6PVjQGMKzXzsfP/r4SfL2Mxc1yj+X2pkCMh24RYqQm1dOZ0oRs0JMuTRrwIQTahmgzzZnqiBnKuRHpT99DrqeO+jBGkBR3LfzwgESCYqlCqBQnaVsAQmq43KoL4LOftOZKjSAoBDE8zYZiYzieu0fsQq2fC6Tc5nf+PVMHTTW582DfRkhHtFpK6bS61lo5L41Gr08ztRUGlQ91aGYIoQQMmZ8HnPTXlPhLWo4rh17AIX4pt3nVsSU153hTAHAzBpr6MO8+grMrAkCGB8xpT5n0OvGnDozvW9+Q6jkr2enyRBOdZWmG9UQSv85nysFZBna68ofjX5YCaAIet3SMcyV6DeWaHTRp2N3pnIGUCjnLxwEuyvQqwyg3ZOlZ0qU+AHAob6RcXOmRC9Ws+FMZSvzyxf6YDpT5nmW0pmyp/mJtR2KJTOCWCzO1LQs8yvs87dGo4+/M/XWwX6suO2v+O6ft477axGKKUIIISVATY0bS4kfkD0a3edxyfvSPVOKM5XFCdM0DV+9cDEuWNaMMxY0OB4zFmqC5nutDnrRYszR8ro1zKwNlvz17Hzj/Utx5+XLce5i04ETzlRBYsohzU+IqVxBdWqZn6ZpqBSlfjlmTVkDKIr71lwICuEEivTInGIqYXWmntzahRO++Rfc/+p+83kVYbDvyLBjCdZBxZnq6ItYAigGoqVzpoRIE86UKEvtj8QtLmE+ZyrhkJ43mij6bKgCdCSetAhou7unOmLTy5kqtszPPK4c0ehbDvYhlkxh856j4/5ahGKKEEJICVA37mMJnwAUZypmDaDwujUpmtJpfuZris28E5e8Zyb+3ydPQfUYz8sJ9Tmrg15Z2reoqSpviV0pmF1Xgb89dbZl/YUzlW/GFGBN8/MV40wpYgqAkuhXoDM1ygCKsL1nKmeZn9IzFU/ihXd7EImn8NIuc26TmkyXSOmO85jUMr+DfSMWUVBKZ0qW+YmeKSO9UteBQWVd8ztTaeGlOlPRAuZBFUIknrT0/0TiSYuAtod4qMKvezCas3R0KiGu30JL6cpd5ideL9+4AlIaKKYIIYSMGa+ndM6UGUBhnTPlVcr8ArYyv2zO1HijlvlVB71YMasaP7hiBX5wxXsm5HyAIp0phwAKd56hvamUbpkzBUCZNZXLmSpdAIXsmSqwzG8klpQBGOoG2F5Ct8uhb0p1piLxlOU1ndL8fr95P675r81F96wctjlTAa9buoV9RQzKjTmV+ZXImVL7pQAgkkhZ3JlMZ8oqIuzrPVUR4rTgNL8yl/mJ1yv0/MjYoJgihBAyZixlfpVjc4CCNmcqnswUU0GvyyKgcjlT40mNIqbCwXSv2GUnzbKk+pWbRtEzVYAzpgZQyKG9Wm5nqnckLpP+6iuLcabyz5nKNgRXlvkZZZX2AIqewSj++Pohy0bVXuYnXl8VI6pIAYA9Dol+B/sy3SqBkzP1k6d24s9vduC5Hd1ZH+eEcKZEzxTg3DeVL0winsgs84uWaGivmuQHGM6UpczP2ndm/5ynS9/UWJypBJ2paQfFFCGEkDGjRqPXlMqZkmIqvfnwe1xyFpLdmRpLeuBYUEsax1reWCpECEYhn4PXYc6UKA/M9gW62KTXVnjl517pL6RnKrcztb1zAO+57a/44WPbM+7rM/p0qo24cHsAxR1/2opr/+cV/GlLh3xMwuZMiddXE9jsvVhOg3tFAIXfk7ll6ncQU+I5c4kwO4PRhJwX1WT03QHWvilBvpK9REpHKqWjR41GL5EbovZLAWkxZemZsonRDDE1Dfqm1EHEw/HJGUBR7BwsMjYopgghhIwZdVNeO+aeqfSGOVeZX9DrRsAytHdylPlNBpbNCONHHzsJ3/vbFXmPtfRMuY0yP02IKedNn71fCgAqFWfqG/e/ga/c95olJlvXdYsgcNrcb9pzFAPRBJ7efjjjPlP0+IxztTpTIolv/1FTDKmb1pF4Ur5+xEFMrZhVAyBz1tRILIkjQ2kBsWJ2TcZ5OUWji9dRUwDz0WW4UhU+txyCDKjx6JnOlPhdEIhExVgyhT7FPQRK1zPVO+zgTCkCek/PsOVzt4dlTAcxpV67o5kzVR4xJRIWExlx9aT0UEwRQggZM9YAitI4U4mUjngypQRQmKV9fq8bPrdLbiArJqjMr9qW5jdZ+OCKGVjaGs57XK40v2wld6IXRwRdAECFIQAOHB3B/7y4F/dt3o+eIdPFiCZSlsb7eFLP2OQJYePUV9OXJRpd9gcNRTMea5kzpThTqpgSARTvMYSSvedHuEshvwfHNZtR96LXzF7mF02YAQ2HinCmRJJfs+JKAc5lfuL5xVBfQYXxexNPpDLK6UrdM1Vf6ZPnojpTg9GE5XOfjs6U+p4KL/NT0/zKUOZnXCP5Zr+R0kAxRQghZMx4SxmNrjhOw7Gk7AHxejRZwja7NmiJ5A4FJqhnqmLyOVPFYAmg8BYmpnqNDbX6OQtnaufhQXnbEWVT3e/g4Ng3+EIw2N0P9TaRnijWWvTwiJK2fouYsjpTwkVyKvNb2prucbNv9kX4xIyaAFqrzaj7ufXp69DuTKni6lAxzpSD2wcAYeO6Vgf3io15VcB6vQlBG0+m0G17H6WaMyU+h9aatOhL90xZN+tqqZ+9v2s69EypazkqZ6qEM7+yoTqCuUJhSGmgmCKEEDJm/JY0v7GJCp/bJTf0kXjSMrT32vMW4vefPx0fWjkLAHDj+47DJ947F/PqK7I+33gyGcv8ikENoBClc1JMZSkPEn1CqjMinMGdh82NtNqzI0SGGlRi3+ALYdM/Es/qWok1FqLj8GAUsUTK0dWyR6ObzpT41l6XARSzDZE+HEtaHmeKqSBm1Jiu0cKmkOV9CVQxV0zPVJcMnyjcmaqyfYEgBG0ipWeIlnyzqQrlqCGQW8JB41wcxJTSdzYdnalokc6UrusWURkvizNlvl6hg4XJ6KGYIoQQMmZKWeanaZos9UtvbtObDzG09+S5tXLD/5kz2/DtS0+Apo3/TCcnLM7UJAmgKAarM2WNRk+mdPx5yyHc8aetlvlA9plPAGSfz7tZnCkhOuoqzWsjm5iKJVOWb/KTKV0+XoiLJkNM9QxGLRHg2cr8BiIJuekXzpQacy7EFGAVRAeOmmJKCAgAWNCYFlN2x00NpOjsjxQ8V8kc2Gt1pnKl+YXtzpQhaGOJVIZoKZWYEmWRrdXCmUrJxDjhGludqektpmLJlOOgZ5V4UrcMwM53fClQz3GQiX7jDsUUIYSQMWMt8xu7qFDj0aNKAMVkI2j0bgFT1ZnK3TP1z398Gz95aic27TkqjxNiQ93MV/jTn5e6iTsypA63NZ0lIeAyyvyU8j5VPKjiRqxxfcgPlwakdGB716Dj41SHSd3Ei54pkUzncWmo8ntQZQhC9TkOGKV6M0fhTMWTukXo5cIpFh1wTvPL5kyJEJZ4MoXuQWvqXr449UIRJZ4thpiKJVNysy5KJdUQECGYRX/ddBBT9i8BhvP0JNnXvjxDe1VnKokHXj2AC//t6Yy0RVIaJt+/TIQQQqYcajR6KUSFObg3YZkzNdnQNA3zGyvhdWuYXTsxpYZjwauk+UkxZbh8Kd0s1Xunc0AeJzb2qjPlNOer26HMryrgMcMjEil8/fev44Z7X02X3ClCRI3gFm5Ipc8trwG3S0OdMePq7UP98lj1OdR5PjFbyZ96bLUxHyzs4AL1DJm9TGoJnnCmhmNJi9Ngd6oO9hXWN9XVbx3YK3A6p/w9U5lDlUvVM3XUELwtylqYpX/p29SyP7HuM2vTrt606JmyfQmQaxwAkDlkuZzR6EB61tT9rx7A1o4BPFPk7DNSGJPvXyZCCCFTDhGNHg54LH04o6VCOlMps2fKYc7PZOC/P7sKf77h7IzwgKmAWynzE+urxqWLkrgdDu6PKpqdoumtZX7px6hiqm8kjntf3ocH2g/i8EDUIhicXCp7+agoiduqiinlcdkS7OJJHYlkKiPUwnSBTLdJvIe6Ch8CXjeuO28hPnbabEtSolpGpQZFAGbPVT46B9Kiqymcv8xPpvll6ZmKJ80yv5k1aRFTiJjacqAPT7+TGUuv0mcr8wMg52MJcasGfIjenVmGmDoyFCuLmBhP7AOQ8/UkReJ2Z6oMYiquiqmk/EIi4tDj1TUQwWU/fg7/tXH3uJ/XdGVi4o8IIYRMK0SpW23l2PqlBAHZM5VQAigmpi8qHw0hvyUmfCphLfNLr7nLQbNu71KcKUMwqJt5dTaSwKlnqirgldfKUUX4HB60iSmHP4dtjmdjlR84BLx9yDy3gWgCqZQOl0vL2ZsSUUIrhGARA4HV1xbvQVzX/7husbwv4HUhEk9hIJKQQi/DmSpQTB3O4kzlmjMV8LrhcWkyalv0TMWTKXneM2uCaN/Xm7dnKpZI4cqfvYjBaALPfe18WcZnRwjQmgoffG6XRbDWhdJroG7ko8b9zVUBuF0aksYw4WzPPxXIcKbyhFDY1z6lQ16j44Va5jcUS8jrxykm/Zl3uvHq3l68tq8Xi1vCOK2tbtzOa7oyOb/mI4QQMqUQm03xTfhYkc5UPGkZ2ktKi7qmIhrd46CmtneazpRTmV+Fg5gSJXIbd/bgf18/BMDqTIn+GyAdwKAKkb6ROA71jeC+Tftk71W1ba6ScKbUOHZdN4Vbrt6UkVjSdLyM9yF6wFThIkrY6h2+JBBldup5q48FgEMFlPn1DscwYLhbal+Wek59iuMlnKmA121xa82eKV2WSRZa5rdpzxH0jcSRTOmWEBGVdClm+nlrKrzyehGINbI6U+JcXfL+qd43ZV/LfHOchDOlurfxLAOxS4Uq4IajCfm75nSuwjVO6cCNv2l3nPNGcsN/mQghhIyZ0+bV4V8/sgLfvvSEkjyf6JmKxJOIGZtiiqnSYynzM9bXyZnqGojKErp+hzK/yixlfs9u78bHfvoCXtvXCwA4ra1OEVPmpm139xDUNPS+kTi++6et+MrvXsc9G3YCAGqCVkEjhIJ9CKrYDOYqp4rEk/L9mM6UtaQuEk/KEjYnx1UEQKghFFJoGvcVMrh3jxEl3lTlzxg+LUoQ1bh4sTn3e1xZxFRKvjdRNpgvgOIppbxvv+KmJVM6Nu0+gpFYEiNxM1mzOuiVv6MAoGlmsqVa1iZcHL/HrcTZFz5/q9TYI/dHg91pKtSZUt3b8Q6hsPRMxZKydNUpyl29fg/0jmD9c7vH9dymI/yXiRBCyJhxuTR8+ORZsjF/rIg0P3Xuz2TtmZrKeB3S/JycKQDYcXgAqZQuXRRLmp9DAEXPYAzt+9IpgCfNqcFjXzobl7xnphRtqpjaaXND+kbicmbVO4YrZg82sceIq48FChBTtl4se3KeOD+3S8voTwIUZ8qSPJhemyUt6Z6qgwUM7t3dk36f8+orM+4T71mNixcb5YDXbfmCQXwG0URKfkaNRvlptv4xwdPvmMEEIg4eAB5+/SA+/JON+P6j2+R6eN0aKnxuWYoLABVetxIao4iphPm7K8XUBDlT/ZE4PvCjZ/GJn784JlGV4Uzl6ZkSPVbqYPHxjkdXBW1nf0QO4HYS1fbo9I7+iRO7UxX+y0QIIWTSoW7MJnOa31RHCCefxyVndWVr5djeOYiBaEI6SFVZeqZENP7R4ZgURGuWNGFhUzo6W4i2o0qZ384ua2SzKPNTqamw90w5992YYir7hjkST8lSuLDNmRLiSPZLVfgc55iFHZwp8dqLW9LvtRhnaq7D4OlKn1u6h0LkWZwp5Xei0oin7x2Oyc+okDK/zv6IJRHxgOJM7e5On9v2rkEzsCOYXo+AUuZX4ffIL0DU9Doh/PwelxR25RRTqZQuX+/mB7bgzYP9eGZ795jmbmVEo+dxpiKGgFETL/OJ27Givj+1b8/JmRKulfgdLlWM/rEE/2UihBAy6ahQ5kzJb7cppkqOmPnkV1w/TdMs5X+C7V2DUmj4jQHKAjFnCgCWzUi7MikdaDfK+9oaTMfS687smdphc6a6B6MZs5IcAygU1JRAIPe3/yPxpOxDqpY9U+nNpHCXhNirq3SO+hfO3IDaMxWxiqmugWje9LZcYkrTTFdMvC/VmfJ7Mp0pUfUY9Lql4M0lHuwJfqozJd5Pz2BMCezwyNcXVPrcCHjM0lxBzNiYT5QzdctDW3Dqdx7DJ37+Ih5sP2ie1xjEjF1s5C3zU/rGhBOcGO8yP+UzUN1Rp54p4Uw1OASIkMLgv0yEEEImHQFFTElnyjM50/ymMiLG3m8roXQrTowQXNu7Bh1j0QHrt+6zaiqkANhlDAmd12AKBZ90ppQ0P9sGe1vHAOzYnSl7md88Q4yIc4zldKaSUsyJAArRnyQe36M4U0449kwZj53fUAmPS4OuI+/g3j1Gmd9chzI/ILOXK5qnZ0p9nEhozOVMiX6p985Pp7gdVNw01aUzwyfS6yHEU/q1VWdKCaBwKvMr46ypV/b0AgCe2W6drzSWuVt2Yerk9qgIZ8rvMcsyx11MKeeouqMjDkJJfBkgEknH25k62DuCZ7bnjuCfalBMEUIImXSIMr/hOJ2p8URsxv0e60ZcdaaOn1kNANjROeCY5Aekv3UXD2mpDmRExav9QE5pfvbXFSJMxS7g7M7UfMP9ks5UKrPxXzAST2YEaYRt6Xwiya8uS9y/FFPqnClDWFVXeM0erJHcPTW7czhTgNLL5eBMWXumMsWUWOtcztS7RinmB5bPAAAc6o0gZdhbYi26B6NKmV/6fAI+VUyZzlQipcsvQGJqmd8EOFNdxvyu1fPrcd7iRnn7WMRU0dHoijMlvpgYzzK/RDJlCWVRv7RwmjM1aFyz4vOxDxkuNf/w61fxiZ+/hDcP9o3r65QT/stECCFk0iE2hpGYmSDGnqnSc/yMMC5e3oprzl1guV0VUyfPqQUAHOyLoMOI+rYHMmiaJt2p1uqARYC0hAOoVASNDKBwiGAW0eBiL7hiVrU8PsMN83tkiqDf40Kr8VjZM5VwHm4LWAMohCNld4CO5BVTDmV+YiZWwGuWDUYy36dgKJqQztXcusKcKdkz5bU6U5U20VgdNGd65RIPA1GzNNHt0hBLpqR7JIRgNJGSvTfCyQt4rD1TAZ/5szjHieyZiidTslT03z9+En7x96dJYT0mZ8omNobjhQ3t9StR9olxjEbPJdSco9FFmd/4OFOv7D2KNd/bgCe2diKV0rHlQFpEiX686QD/ZSKEEDLpUAMoYkzzGze8bhfu/vhKfOK9cy23q2JqTl1Qbui3HEgHFdiFDWD2TbXYxJRa4gfAMRrdfC3rsQsaQ7j67DasmFWNkwxRpyK+TW8I+TNER9xQZFVK6qB4W5G4GRdtj0YXgsjsmcrtTInnicSTUjyEg94MR8kJ0S9VW+GVos5O2F7mJ5wpj9vi1gZtzlQ4aM6CyiUehGCqrfCiJZwWpPuNvilVCIowEXE+9p4pn9sFUR0qNu2mM+UuuzPVZbyO163Jz7AQpy4fdrGSv8zPFJQi8GU8y/xy9Tzl7pkaH2fqL292YOfhIfxu83509Efk2ov5cdMB/stECCFk0hE0XI7BaELG+tKZKh+qmKqu8EqRs8UozbGX+QHAaW31qA56ceLMatQrZX5q+ARgbmidHJs5NnempTqAr6xbggevO9OxXK/JSPSrD/ksYkjXzVKzsDLsV2wY1aG9orzPjEZPQNd1S5qfE/Yhv+Ibfk0DqvyejLJBJ0S/1Jws/VLW1zFFG5B2prx5eqakM5VMydI9FV3X5WY6HPDKodsi0c8qptIhIaJ3zZLm5/NA0zT5JYjY0DtFow/FkhiK5nZzSkGnEfHdVBWQaYyFOHX5EI8VwrHwMj83PO7xL/PLJRSd0/yMnqkqI4CixM5U90D69+idzkFL+a49YGYqw3+ZCCGETDrEpqxfae6nM1U+VDEVDphi6u2D/fI2O3d99D146Z/WoD7kR73i5rTZnCkRduE06sfeN9Ra7Rx/LhAb9PpKn8WZSqZ0JcLdm3H8kaGYFOlCbInHJ1M6hmLJvM5UrZHyJzaFYlMa8nvgcmnyeXP1TO05knam5mXpl1LPyyzzy+JMeR0CKJTbnDbww7GkXIeqgBczaw0xJZwp5dzFRrjGwZkSQi5gmzUVTZhhGSG/RwqwfKEcpaDLEFPNYVPYS6cuOXrBIN6T+FwKD6Awo+yLdab2HRnOmAeV9fUc3Kds96VS+rg7Uz2GA7W7ewjvdJrBMuLLiukA/2UihBAy6RCbM7VESh0wS8YXNc2vOujFbENMibAFpzI/TdNkkEWdRUzZnKkcDmNz2G+5v6U6mPM8pZiylfmpDfhqz5RIAOzsN0vAhAhRN7t9I3H0GCKpNouYEul7e3qGoOu6pV9K/X/uMr/cSX6AVUzFkykpVEIBjyWF0WebO6U6U4CzYyEEoMeVnhtlOlPD6QHNijMlHi/KEYMO0fjitoitzE/MMStFqd+m3UfwnT++lVfEiP6+5rApyMV6lGLOlHAsh/MO7c10pvLF5asc6hvB+d/bgL//xUsFHZ/TmbKJqeF4Un7pMF49U+L3KJHS8cTWLnk7xRQhhBAyjohvuPtUMeXiP1nlwuJMBb0ZvUxq6ZwT9SFVTFmFQi6HsTpo7R3K50ydtagBAa8LZy1qsIgO1YVRSxJFWWDngAjS8MoSME3TLH1O0pnKUuY3u7YCbpeG4VgSXQNR6aKK5zDLBp3F1JYDfXh+Zw8AYG5dfmeqPxK3CLNwwGNZS6/bZfnCoTrosfzsVNo2oJyzpmkWZ2ooloBDZSBqgun18Ft6pjzGbenzEUJHjUYHUJIQin97bDt++swuPLmtK+dxncZrWMSUp3RlfuJzKXRor9ozVYyYeutgP+JJHbtsgQ0He0fw2037Mt5LVJntZWcknoSuWMIDipgW76fUzpTqQm40rnf77VMd/stECCFk0iGcKSGmPC4NLodBsmR8sPRMBb2YXWd1iJzK/FTqK9ObZpeWGSph733z2F5Ldb3yiak1S5vx5m0X4pL3zLQ6U0nVmTKfT/SFCGfK3vslRGLfSBxHh9LXXm2Wob0+jwuzDPHx7uEhxZnyWP7vVOb3m5f34gM/ehZ7eoZR5ffg9IX1Wd+jek4iAbEq4IHHJp68bmsPVXWF13ALzb4pO2IzLcI0ZhjO1MHeiKXE1no+Tj1TRpmfGNybsEejp28XztRYNtKiLK0nj7PR2e/gTOUQU4lkyjKTKRtCIIreMadQB8vxijMlPp9iyvxE/1rU9jp3/nkrvvq71/HoW52O5+f0JYCuW50rEYteFTBLMEvpTOm6Lp0pABbHmM4UIYQQMo6IZDLRz8F+qfLiydIzJXAq81Npa6yESwOWzQhnfHb2n4UgEc8rntvndmXtV1IRwk91cMRm2e3SZAlayO9BheGgiH4au5gSz3Gwd0SKj1znIFy33T1DGTO4wrbz+de/bMPmPUcAABu2pYeWnrWoAY9cfxZac5QzqsEaIgFRbOTVtfS5TedDfZxMsHPY9AuhJ8SUGkCRrTxRBlDYhvYC5u+tdKZsSZylKPMTYiBX+SQAdPULZ8rsmcpV5nfzg29i9e1PyM8oG5llfoU7U15X8WV+IlnRfs5CTB42XFaBEG/VQS+cvn9S+6aEYE6XjIoSzZTFvRoLA9FE1rANiilCCCFkHLEnkzHJr7wIF9DndiHgdWFGTdCyMXNK81OZWRPEH794Fn5x1WkZ99nFlJpkVx00gyRaqs0UtkIQ56Tr5kbN4zJ7osIBj/yz2IjaZ1AJF0sM0g14XVIoOCHE1K7uISlMMnqmInE89c5h/PuTO/DdP22TtwHA5StnyX60bKhiqm8kfd6i1M7nNn9PvG4NPkuZn1GOZ2ySnTa1UgAa5yrmfA1GE1ldGhFAoUaxV/pFAIXpbui6bhnaCwCNofTzHx6DMyWck3xiqqNIZ+q1fb0AgJd2Hc35vGIdCw2gECJWHbIcd6qfzIIIA7EnMgpxNWwTyWJ9Al6XLL9UUZ004fJV+b0Wp7FUaYPdOUTzkWEzBGaqw3+dCCGETDpaq4OWb5QppsqLCKAQvTRet0uWgAH5y/wAYGlrWDoRKvYACrVfSHWm8pX42Ql43XLTLhLEfG6XFFBVAa8lgQ7I7kztNpLrsvVLCYSYevew6kx5LP/vHzGFSfeQdRhuvt4zwFzrPgdnyusxxZPbpVnL/Iz34pfOVPaeKeFMVfg88s/vdA46n49jmZ/hTHlNZ0rdkJfSmRJCKFfkPOBc5pdLWArxtavb+X0LxDqKzyBfAIVY4wqfEkBRRM/W/l5T1KrulFiHSMwupszSSuHKqqjiT5R5qs4UULq+KfGlhfrl2MKmdCCNrgO9w9PDneK/ToQQQiYdbpeGv1kxQ/7sY5JfWRGlc+pmf45N9IwWv82ZEnHoQa8bPo9r1GJKPS/Rk+Nxa6jwm+Im6HM5Hi8Q71ek7GVL8hNYyvyypflF4rLkTBxjd4QKeU9DsaR8X1IoGcI0PTBXs3zpYC/ziyVTeH5nN7qUsrB+2TNlnocY3CtirBuUMJGQ3yNfQy3zqzQ2y34lzU/d+AsBLcRUVwnK/PpyOFPDsYQUMZZo9CzOVCSelG7mblvQQ8brJ4sr8xOfWWPIL9cukSpcrAhnCrD2M4l1sPdsyTh6mzMlylUtzpQIIAmkw0qE+5yvb2rLgT5cevdzeOHdnpzH9RjvfUlLFaqM38PjmkNSiE6XUj+KKUIIIZOSS0+aKf8cGUP6Fikeex8SkE6vExTiqGTDXua3rDWcfn4j5OLEmdUAgJVza4t+buGaiKZ3r9uFsxc14H3LmvGZM+dbBACQKWbE+xUzlfL1bM1T4tG3dQxYHqMmAwonps8YKCyFVwGitEopRdx3JL2xtvdMiSAKJzElBMRLu47g4z99EV/+7WvyGJnmp4opQ8RuN5wpNY1RvR5Ul0+U/ElnKp6yCBZxDqKscjBLuEUhSGcqx/wuIV4rfG7LsOdsZX7CxQKAd5XBsrleX3wG0UQqZ7macGcaqvzyc4oXGEARiSctYR2qYxQzBI9dzIlj/B43Kv3CcXTL60jtmRKff8jvsYw2cHIxVf74xiG07+vFH17Zn/O4w8bvYUPIj4XNaUeqraFS/o5Ml8G9o//bkBBCCBlHxCYbmD7fYE4VpDOlbLLnKINl1Q1qsdjF1JLWMP7ns6vQapQRXn7yLJy1qMGxRDAfYvMoXAuv24WaCh9++slTACAjTtsuCs0Qi/QmszZPmd+MmiB8HhdiiRQ27TkKt0vD+5Y1p587YD6XcIPiSR0j8aR8/kIcPo87PfB2MJqQQ35lz5QQUzZRJVw+9RjhNKmDU+1pfoBZFre9K33c3PpKvLz7aMb5+pUyP7HuovRPdaaEa5a+3zqHajQI1ySXM6X2S6l9d8Ihs5f5HeozxVT3YBT9kXhW1zBmG9oLpN0ep9+JeDIlSzMbQn54RM9UgT1JB3utfWuqYyTeQ4YzFTedKVFeVxP0KiWY5muLuXHCmQx4XRiJJ/M6U+K6yecwCmeqPuTH8TOq0b6vF6cvaMBLu47g3cND0+bvdTpThBBCJiWapmHt0qaJPo1jEkdnyijzC/k9clM4GtTQBCD9rfnpCxssDkhTuLjwCYEoNxMbWPug53zOlP3n98yuyfl6bpeGeYrIvPD4FtlbJoRaMqVLEQSkY8eFk1FImR9gfg57jfJD2TPldjn+3yJ6DDElhtgeHogikbS6O6qYEmV+wuGoD/mko1SjzACzDO21OVOReDIjfAJQxFQWpzmWSMlQBCfUUItcPVNmv5RVkGdLNuzosybi7c7hTgmRmO4nTN+WrW9KOKRul4aaoFem+RUajX7AJqZUZ0q4R/YAjKiy7pWyxNXrKGTVnqn0Y8xEv1wIR0s4gNkwSxx9uH7tIrx+6wU4Y2GDHJ1wZGh6zJqimCKEEDJpufPDK3DmwgZ857ITJvpUjimceqYWGY3j9g1qsajOlEvL7KEaCyIIQcxjsou+oC0l0u4MCSHkc7vwtQuX4JOr5+Z9zXlKGuFVZ8wzX8vrlhHze3pMMbXvaPrPXrdmCXHIhSgHFDHZ9n4on9vqTKnvSxwjBEZKN8urBiKZ5YbNtl61cMCLBmPYbrYyP9GbE3AQUz6LmDKdKyc+8fMXcfadT8roejuJlC4HCedK8zNj0a3vRYqpHM4UkC7z3NrRj+2KiydQRaIauOGEEBN1lT64XJqS5leYM7X/6CicKTWAQjhTFYoz5dAzJcR0vs/H/rh8qYxCTNYb149wwOpCLPMjhBBCykJdpQ///dlVE30axxwizU/dPC9tDePfrngPFjSGxvTcqltU6fOMyoHKRshvHfZsT4EM5knzO2NhA+7++EosmxG2OGW5aGtMH3fCzDBOUfq8NE1DOOjFkSFrBPR+w6UKB7wFv3fhDImhpzUVIhrduWfKIqaM2zoUgdLRH0FLdUDpmcp0puRrB72oD/nwbveQxZlSxVSFjEY3N+xi4+9zcqbi6eh09f33jcTx4q70jKc/vHoA15yzIGMd1F6n/kgi4znU9wdkiqlsARQdthj4F949gq/9/nVU+Dx46RtrLKJciJi0WPFgOJbEUDS3mBJiVJb5JQp0po5md6bEe7D3TEWVuVa64UxVB72yT2vEoWdKhEPInqk8ParicT2DUSRTumXIt0qP0jOlUm/0TLHMjxBCCCHTEo+DwwGkQ0FOnFU9pudWN9dO0c1jQST39RmRy/YUSLsTZJ8z5XZpuHh5a8FCCgCuPG0u1ixpwrcvOSFjY29/fsB0GwoJnxDYP4fMAArr/8OWMr/M0i1R1pYrzU99D6IsS31ecU6VPrcUbKZYSuUs80vpmSEMWw/1yz/f/8oBx8Gx6iY/mdIxlMUREu/P/l6yBVAIZ0oMLb5v0z5E4ikcGYpllB1G4+b7ajL6+oTbaKdbigkhfo0yvwKdKXuZn8WZEtHoGT1TxvkpM9KqlZ6piNOcKaVnyuk57Yheq5RujiFwolv2TFl7D0UARa7HTiUopgghhBBiQWyORdBBKVE317kG4o4G2TOVrczPm7vMbzTMqa/Az686FSfNyUwfdBJMYuPtJLSyYT9PWeZnvD9PAWV+KqLkzynNr7na6iKEg14cZySxqa5kXaUP//Lh5fi3j54kRWTQ4kxlL/MTx6i8rYipbZ0DeEv5WWAXQdlCKA4aTpMYQizIJqaEk3X6gnoApgMImGskzyFpvq8lrVUAgK2HMssBgezOVKFDcbM5U8mULs8x05kynbMFTekvBY5rrjJdQ6c5U0U7U+a65+qbMt+/9e8RUfbXwzI/QgghhExHPnn6PPg8LqwZhwAQNYCiwldaZ0o03GcLoPDnKfMrNU4BEyLefEzOlPGzKPcTP+cKoFDpsIkpNYCiodIPj0uTm/VwwItrz1+I85Y0YfmsGsvzfOSU2ZafVWcjpmzqBelkv/TA1mg8CSjn+bYhSMT9f3jlAI6fUY1IPIk/vHIA5y1pzBBB/SNx6SapiBS81mrrffnS/E5fWI/7NlvjvlUxlUiaMeh+jwtLW8IADmBbZ6bwA4DuAauYEI5vsQEUfo8L0URKOlPqOtj7tYSrFPC68PHT5uDUeXVY2BjCLQ9tSR+vlvlFrZ+/v9CeKcWtyzaAOZZIydRKlvkRQggh5JjinOMacc/fnSy/QS4lqlNRWXJnKv18onwtb89UgWl6o8VpHpfpTBX+2nbhJX4+ra0O377keNz6N8sAmGIhrzPVl04UHIxmiimXS5Pla+nn8sDvceOkObVZe2MEQcvQ3syeKU3TZKKiPTHu7Y60ILnMmC/30GsHoes6Hnj1AL5x/xv4/l/fcRRTduLJlIzsbq1x7pmK2nqPhIOyqq0e9hYsVTioIszncWFxS25nSs6YMn6PxOeTKMCZSiRTUvSKslOxZhYxlSOAQtM0HNdcBZdLcwygkHOmAoU7U7quWwRmNjElSvg8Li3jWjfL/CimCCGEEEKKQt1c29P1xorowRLtNnYx5XVrEHpAncU0XjgJJuGaFTP4WBVHQa9blmy5XRo+sXoelrSkZ7KtXdaMWbVBnLu4UR7v92SucUd/xDI4t8p2nmqiXzGiT+2ZUudMWY8x3A+l/yeRTGGrMfT4mnMWwKWlN+ldA1F5e89QLGOT71Tm19kfga6nX7eh0jkaXRVFXQPm8S3hAE6aXQO/xyXdk8Go+RqqiPG5zTK/XT1Djol+GWV+LvH6+Z2pwWhCumAiZVII1GjSfK3MaHQzgEJFfjbK8YO2Ms9CeqZG4klLoIqYoWbHTPJLJxmqiB6qo8OxnAOPpwoUU4QQQggpGxZnqsQBFPbBqR7bJk7TzG/oixEzo0V1lOwlh8WIFFVMqYl6dt5/Yiue/dr5WKHMx3ISjB39EeneBbyujGPU4IZiyhHVND9Z5ufNsqlXNuy7uocQS6RQ4XNjYWNIzjTbeXgQ7xozn0ZimcNk+yOZ851EyV5LdSBjEy+EpSqKRFhFc7UfLpeGX1x1Gh7/8jnSdVJdGPE4l5buf2oM+VFf6YOum0OOVYRrU59R5pffmRKv61PmRQlnSnXWRuJJpBRBEnUI/gCsnw2QdvDEn4vpmRq0rXk2Z0qGT1RmuttiGLauA73DU9+dopgihBBCSNlQnYpSB1DYn8/rICSEGzbeJX7p1zDPR51HBRQnUlThV2xoRrYyP6ckP4GIFPe5XUXNAVOdjezOVGaZnwibWNySLkmbb5S17eoekgN01VALgZMzZfZLBTLucwqgEOKrNZx2f6orvJhVWyEFhlrmF7X1gWmaJkXXGwf6cM+GnXh2e7c8vnswS5lfAW7MUMyMLQ+I8kTRM2UTY+q6mGl+1i8qzDK/9P2qKAoVMWfKLmC7sogpEXLS5DCXzut2Yc2SJly8vBVJh9TGqQbFFCGEEELKhrVnqsQBFLbn8zr0+IiNcCmS/PKhCib7fK7RpvnlcqacUMWQ6I0aiiWlI+N0Hi2GEAkHi5sDJoSqJYDCtql3cqZE+MTS1nS5YltDeq22dQxgv9FjFnEQU049U0IczXAIphBiRh3a26E4WSpCYAxGEognU3j49YMyEEK9hkWJ5ff/+g6+++etuPnBdNBDKqXjiNE31Fgl0vzSa5ktze9/XtyLs+98Eu8eHsSQIeIq/R4lGCKzZwoAhmOq4HMu8xOfjSgLFCIx4HXJctiCnKloYWJKjACYVZv5OQDAz686FXd/fCWaqjJF71RjQsXU008/jQ9+8IOYMWMGNE3DAw88kPcxGzZswMqVK+H3+7Fw4UKsX79+3M+TEEIIIaVBdSqCpQ6gsJX52Xum0q8pyvzK4UyZr7GwySamRpnmV2xcvbqpnlkTlILqnc5BAM7OlCjzK9a9U8Ml8vZMKWJqqxE+IcTUfGMQ8lPvHIYwcdTSQUF/xEFMFelMte/vTR9vC6uoUpypv77Ziev+51Xc/MAWy/MAkH1TIkxBiLOjwzF57iJwwZsngOKPbxzE3iPDeH5nDwaNQcCVfo9cV6c0P8AaKpGtzM8+Z0q4eurnr342//XCHnz2l5syerIGbGuerczPFFMVjvdPJyZUTA0NDWHFihW4++67Czp+165duPjii3Heeeehvb0dN9xwAz772c/iL3/5yzifKSGEEEJKwbg6U7YeLPucKcDcMBbjDI0WtTwvQ0wVk+YXKI0zVVPhlUJpe2faDapyWIcTZlZD0yBL2ApFuh+5eqaE4FIEgdh4txmlkEJM7ekxh+E6OVOOZX6ibM/JmbKVy72xvw9/fP0QAOCDy2dYjhXO1EAkgb1H0uexvSstQNU1XWo4U4KReBIjsaQs8aup8CpDldPOlH1gsf39DEQS0pkK+d1yDUUJn30dIg5iKpDFERTCyx6OkX5fpjP1/57eicfe7sQTW7sszyPKA0XiY9dABF39Eby064jlOOEoZnOmphMTOmfqoosuwkUXXVTw8T/5yU/Q1taG733vewCApUuX4tlnn8UPfvADrFu3brxOkxBCCCElwu3S4HZpSKZ0VPjHt2fK584sUTMDKCbamSr8vavnWl2kmFLFa12lD163C9u7BqUwcFqHhU0hPP/18x3DA3IhhFIypcvSs6zOlOJ4dMpSu/TrzW+wrhWQLk/LjEZPv0YskcJXfvca3ju/HoeMgb0za3I7U7qu4zuPvAUgHcd+wsxqy7Ehf3pdBiIJ+D0xx+cBgEXNITSEfIgndQxFE0ikdPQMRR3Fikjzi2dxpkwxFZfldIU4U+rgXiGs8pX5ifI8NQZfdaaODqXPZfOeo7h4eas8RgRjtDVUomsgikg8hct+/DwO9I7gLzecLQX4seRMTamhvRs3bsTatWstt61btw433HBD1sdEo1FEo6YF2d/vPFiNEEIIIeXB53ZhJJUcB2cqf5mf+Ia+HD1T4jV8Hhdm2zaVxUaOi8GtxZb5+SzOlE8KzncMZyqbQ2cfeFvQefrM1xLCIGtEtyEMhqIJOTxWBF80h/2o9LkxZBEJqcw0P+M1ntvZjQfbD+LPWzqkeHM6f78ipjbu7MEL7x6Bz+PCly84LuNY2TMVjcN+GakCMeB145EvngVowN/86Dl09EdwZCimiCnz8xKBKImkDl3XM/rR+oZNZ0o4QCGlZ0o4U7GkdR3UUjx7SIbAXuYnyvMaq1RnKv06g5GEFHOb9x61PI/6WYX8HgxGE7KXbEfXIBa3VCGWMGdkHQvO1JQKoOjo6EBzc7PltubmZvT392NkZMTxMbfffjuqq6vlf7Nnz3Y8jhBCCCHlQWzwSz5nyldImV/50vwWNIZw8Ymt+Pw5C1AV8FgGwhbrjInjiy/zM9ekrsIny/zEptupZ2q0+Nwu+R7ziiljUy823ZU+tzwXTdPQ1mhNP4wlU1I01BprIHqmhAsSTaTkZn9GLjGlzLU6f3GTo3ui9kwdHbaWE9pDNZrCATRVBSzDaO1JfoAZiNIzFMWZ330SX/vd6/K+VEqX5z4QiStlfp4MARq1DTweVsv8hDNlK6+0D+11ElPidTqVPqg3D/RZyggHZAqkx+JqAWbp4KG+Eeh62ukS87qmM1NKTI2Gm266CX19ffK/ffv2TfQpEUIIIcc0QkxVljiAwuu2zkxyKvM7e1EDwgEPVs2vK+lrO+Fyabj7ypW48X3HweXS5AYdKF7MCZerpthodLfqTHnx3vn1cCsph6V06NQ5XkJM2aPZA7Zkuk4558laltfmUOonnCghAMTPoj9HUOFzO5ZR+tzmnCnhvNRWOr//KiXNzz4Lye8g0gFlGK3FmVLK/IzHvdM5iAO9I3ig/YAcWjsQTchh0wORBAZjSpqfx+5M2XqmHJ0pe5lf+me7mFIFkRBgHX2mQZFI6Xh9f5/8WZT5hQIeNNjElHhOtcSvmDTIqcqUElMtLS3o7Oy03NbZ2YlwOIxg0NlG9Pv9CIfDlv8IIYQQMnGIDX5FiYf2AtbBvU7O1CdWz0P7LRdg+ayakr92PkS/k9etSVFRKGctakBVwIPlykDeQlAdirpKH85c1IAN/3gublx7HNYubcLFJ7bmeHTxBGxiyl5uls2ZUgcFA5CzplT6bGJK/HzgqLU6qbU64LiJV3umBhXnxwlx+0A0gSND2XumVIQzdWQoZg4DVt6XfXBzNJHC7p70HC015l0NoKh0cqay9Ezpup61zE8GUMRyOFPGY+wJfZv2mOESovwwHPBimZG+OMcYsiwE5LEUPgFMsZ6p1atX45FHHrHc9uijj2L16tUTdEaEEEIIKZYVs6vROxzDoqbi0uIKocLnxpH0/tSxZwpIO0YTQXXQi30YQTjgLfob+1s/eDz+6f1LHQViLlRnqtbY7M+uq8D1axcV9TyFkt+Zsg7tlWLK5kyJRL+aCi9GYukkv14hpgy3ZyiWRCKZkk5IfaUPPUMxxxlT6rkkUrrsTxJBE3bUOVNJ25DdbIOMayvMMj8xPHhGjSqmMh+3rWMACxpDlmTC/kgcQ0Y0esjvznSmskSjq45VtjK/aCKFVErHYUP4NIYynSn7TOFX9ph9UwNRsW4eXHX6PFx60ky8dbAf37j/DQdn6tgQUxPqTA0ODqK9vR3t7e0A0tHn7e3t2Lt3L4B0id4nP/lJefw111yDd999F1/96lexdetW/PjHP8Zvf/tb3HjjjRNx+oQQQggZBf/+sZV46Z/WWr4VLxVq6aDdCZhoREndaJMEixVSgFXMiM3+eCI25FnFlMfqssgkP5szdVpbHUJ+D85f3CR763oNAaSWzvVHEnLzfvMHluGEmWFccapzf7x6LmIuVChLAEeV3+zL6rVFsGdzpkR/0JHBmBwerAZhOIkp0btld6ZM58wre7SyOVPCbVJvD2RxpsRxssxPWXf7Y0R4xuY9R6EbNYiizK8q4EGl34P3zK6Rx5nO1LGT5AdMsDO1adMmnHfeefLnL33pSwCAT33qU1i/fj0OHTokhRUAtLW14Y9//CNuvPFG/PCHP8SsWbPws5/9jLHohBBCyBTC5dIykvdKhTprKpszNVGIPqlyzLgS2AMoxhvhgIhNd2YAhXVobzZnqrU6iE3/dy38HhdOv+MJ9CIuBVqFzy3T/jr7I3ITf85xjbj0pJlZz0116Y4MpR9Tla3Mz/iMIvHMGPOsZX4h4UxFZZmfOjzY4yDutxkDi/ssYiouy+kqi3CmxP2alvlFgiqmeoaiUqw1OvRMCVYvaMBftnTg6HAc+46MYE59hdkzpaybeA7TmWKZX9k499xzpdJ1Yv369Y6PefXVV8fxrAghhBAyVam09ExNL2dqNFii0bOELZQS+7DYbGV+YuPv1FtkP1YINOHe+L1uzKwN4p3OQWzYdhhAOg0wX9Kh161B0wBdh+yDyuZM2QdAB7wuxJM6kik9a5mfcKa2dw0ilkxB06wi0evK7kypYmowmrD0dNl7prLNmVJnTNnLSN0uDT6PC7FESg4hDnrdlvEE9j6rpio/FjaF8Nahfrx1qB9z6ivkeakpkMIp7B6MQdf1Y86Zmlxf2RBCCCGEjAE1Hn2yOVNSTJUhll0gRIHP48rqwpSSoE1MLWi0pvJldaYcxJT5GFsfltuFk+fWAgAebD8AoLDkOE3TpDsly/yyrInf47YIwfpKv3RasgdQpEXFnp60WGkM+S3XoNdjnp9wjvYeGcZwLGERUynddHmc0/ysc6Yi8SQ27zmC37y8T567E+Kz2WeIqaaw37Jm9lCU2govlhohE28fSjtoajS6QDhTsWQK3YOxY2rGFDDFAigIIYQQQnIxmXumZhmpZzPLuMlsrQ7i8+cuwIyaYFliqtUN+ekL6uVm3LzfnHeUSJq9O/YyPxXZMyWdKRdOnluHX7+0Tzo7hW7cfcbwYzXiOxtVfg96EmnRVVvpRV2lH3t6hmXEup06m/PXagvC8CjO1HHNVUaJYgzvdA5axBQAdA2kBYnFmYo7z5nqHY7hEz9/STpUdkErCHrd6BuJW8Seil2E1VT4sLQ1fZsQU4NKz5Qg4HWjyu/BQDSB1/f3HlMzpgCKKUIIIYRMI9Qyv8nmTH3k5FloqPTh9IUNZX3dr124pGyvpYYg/J9zFmTcrwqD7sEYUnq6BK0hlD2MRAg0karn95jOlKBQger3uDCg/JzLrQsFPNLBqq3wYWFjCE+/c9hxhhVgOlOCGTaBqIr7eQ2VqKnwontHD7Z19MsBxAKRqBfye2S5qlhbkdoX8nswGE1gx+FBDMeS8Lo1nDSnFpe8Z4bj+YWDHnT0A6/t7wWAjAAYe89UbYVPDkh+u6MfyZSOIUOw2Yc9N1b5MRBN4Nkd3QDSjuSxMGMKoJgihBBCyDSiYhIHUAS8blxU4rlOkw1RQgakByTbUaPRRTlYY8hvGSRsx+60+DwuzKuvkFHoQBHOlO2ayOVMqSWAtRU+XH12G0IBDz5+2hzH42uCXrg0UwipSX6A9Xpsq6/ESDiA53b0YGvHAPpGEo7PWen3QGiSaCIFXddlz1R10JsWU12D6edsqMRv/0/2cUGr2urxTucgnt/ZA8A6sBfI7HdTy/z2HRnBIWWYr708siHkx7vdQ3hme1pM2R3J6czk+luGEEIIIWQMTOYyv2OBr6xbgrpKH/7z06c5OhMBo/8nkkia4RM5SvyAzE2+3+OGpmlYqbhThYYd2PudsvVM2e+rrfCitTqIL73vOEucuIrLpVni59UZU4A1EGVeQyUWt6TnrL3TOZBR5qeegxp4EU2kpJgSgRsicTDfGpy7uBFAOoADcHCmbGtTW+lDbaVP9rNt2n1UHmdfR/FcQtgtO4bEFJ0pQgghhEwbJnOZ37HAxctb8f4TW7KWeKlpfp0yfCL3vLEMZ8r4XE+ZW4tH3+oEAMzMMqjXjr0vSBXfdtS+oJoCY+XrFLfM7kyprlhbQ4UUNbu7h9HgMHPNpaVLHBMpcy2jiZQs97OnF+Zbg9UL6mWiH5Appnxul0w7BMy5ZEtbq9DRH8FLu48AsK6LQMyaEtCZIoQQQgiZgqhRzx6HKGoy/uTqlVF7pgpJ8gPMAAqB6O052eJMFR5AIQj5PXDlKC9Unam6AsMUaiuzO1OqkJtXX4l5DZUAgIN9I+g2gjhUgZQu8dPgcWkQpxmNJ6WYqrZF7OdbgwqfB6va6uTPTVXW89M0zeJOiXMRwuilXUJMZaZR2nve6EwRQgghhExBKpQNsM/DMr/JhhqN3tWfFhDZyubMxzg7UyfOqsay1jCqAp6CxY5dTOVCFQ35ZlgJ6i1iyipugj43vvy+4+ByaagP+aHrukzBO9ArZjMF0TtsxI8b56dpGgJeN4ZjaSElAiiqg9b3XEip47mLm2Rfk92ZAtJrHYmnEPS65boLMSVK+JzWTX2umTVBVBe4XtMBiilCCCGETBtCfjpTkxlzAG0KhwfTYsppU+/0GIHf+NnvceOR688q6vXVUrtc4RP2+2uLKPMDAE+WhMJ/WLNI/lnTNMxrqMQbB/rkbbNqKrDlQDqGXC1Z9XtcGI4lEYknETOG92aU+RXgzp27uBHffjj9Z6d1F85UrfLcK+fWwu3SZJqiUzmh+l6PpRI/gGKKEEIIIdOICh97piYzAaPULZnS0WGkw9nnHdnJ1jM1GlRnyqn3R8We5lcIwplqDgdyJhQK7GJKFUSqmEoLyri1Z6rIMj8AmN9Qic+e2YZESs9I8zNfx9ojNrMmiIeuOwPbOgYQiadw/pKmjMepwmzZDIopQgghhJApCdP8JjcBnylm9h81xFQeZypom39kn4dUDMWV+SliqrKwsjXhTNn7pbLRVm+W5nlcGpqVMI6QzZkCYDhTmT1ThQ7J1TQN//cDy7LeL50p2/s9fkY1jp9RnfVxaoDGstaqvOcxneBXNoQQQgiZNlRO4jlTxEyMA4BhYwBsroG9QGYAxVicKf84O1OnL2xAQ8iPi04obJ6YCKEA0uJI7dNSr2URXuEUjQ6k3aNSDMl1cqYKoSHkkyEZy1qzi67pCJ0pQgghhEwbLNHoHoqpyYamaQh43BiJJ+Vt9aHcG/fMnqnyOFPifp/bhQqboMvGcc1VePmf1hQsbNpsYko9p5Df6jwBaWfKTPNTSvEKnLOVD1GGWVtkgITf48Y33r8UQ9Ek5tSX5lymChRThBBCCJk2qJtebwE9K6T8BLwuKaZqKrx5HcQMMeUuTNg44beIqdyCQQRQ1FZ6i3J9ijlWFVPhoNfiloXyOFMhv0cGQxQaDZ8PIVQLdeJUPnvW/JKcw1SDX9kQQgghZNpQ6fOgyu+Bz+2yuFRk8qCKo3zhE0BmAMWYnKki0vyOn1GNOXUVBZfsjYaaCp8s18ss81N6phRnSkSj+zwuuTYlE1Oe0ZX5HcvwbxlCCCGETBtcLg2//MxpiMSTFFOTFFVM5euXAkrbM2VJ88tzfVQHvXjqK+eWpBcpF/PqK9E+3Itw0IuwIvCs0eiZzpTf40LQ58ZgNOEYVz4aLl7egl3dgzjnuMaSPN+xAP+WIYQQQsi0YuWc2ok+BZIDtdSuIU+SH2B1prxuDa4xlG9aeqbyOFNAcSV7o6WtoRLt+3pRHfRYnKmQJRpd7ZlKl0j6PC40hPw4PBDFwqZQSc7lspNm4bKTZpXkuY4VKKYIIYQQQkjZUJ2mhjzhE4DVyRIOzWhRH58vgKJcnHNcIx5sP4BT5tbZeqYynamReBLxZHp4rs/jwvf/dgXe6RzIGVtOxpfJcRURQgghhJBjgoAiaPLNmAJMVwawOkujoVhnqhxcetJMrDu+BUGfG7quy1CJSgdnajCSkLf5PC4sbQ1jaeuxNSR3ssEACkIIIYQQUjZUcVRQz5TFmRqjmHIX3jNVToRbp2madKScnKn+SFy5jdv4yQA/BUIIIYQQUjaKTvNTygKnozNlp7U6AABoDptrIwRo/4jiTHEo9aRgcl5FhBBCCCFkWmIRU4WU+XlK6EwVMbR3ovjBFe/BzsODWNRcJW+zO1M+t6ss4RgkP5PzKiKEEEIIIdOSYsv8XC4Nfo8L0URqzM6U3xKNnnto70Th1Acl1mzA6Jka6zqQ0sFPghBCCCGElA01Ua++gDQ/wHSzxp7mZ259K/1je65yIs67fyRu+ZlMPPwkCCGEEEJI2RA9UDUVXngL7PsRIRRj7RMSjk7Q64ZnCvUcCTEpy/wopiYN/CQIIYQQQkjZED1QhYRPCIQA83vHmuaXfp7JGj6RjXAwXZLYMxgDQDE1meAnQQghhBBCyobo/ymkX8p8jCjzG9vWtbYyLUqaCgi+mEwsaAwBABIpY2DvFHLVpjtTS5YTQgghhJApTYsR/T2/sbLgxwQNAeYbY8/UstYw/vUjK7C0tSr/wZOItoZKeFyaFFNjdehI6aCYIoQQQgghZeP9J7Yi5PfglLl1BT+mVM6Upmn48MmzxvQcE4HP48K8hkrs6BpM/0xnatLAT4IQQgghhJQNr9uFNUubUV1ReDS5DKA4hnuFFjWF5J+P5XWYbPCTIIQQQgghk5qArzTO1FRGHeI71nJHUjqO3SuSEEIIIYRMCehMAcc1m87UsSwqJxv8JAghhBBCyKSm1igJrA4WXho43TjO4kxxCz9ZYAAFIYQQQgiZ1HzmzPmoqfDho6fOmehTmTDm1ZuJfn4GUEwa+EkQQgghhJBJTUt1ANeetxB1lb6JPpUJQyT6iT+TyQE/CUIIIYQQQqYAom+KPVOTB34ShBBCCCGETAFWtdUDAObUFz7wmIwv7JkihBBCCCFkCvB3752L1QvqsbAxlP9gUhYopgghhBBCCJkCuF2aJdWPTDws8yOEEEIIIYSQUUAxRQghhBBCCCGjgGKKEEIIIYQQQkYBxRQhhBBCCCGEjAKKKUIIIYQQQggZBRRThBBCCCGEEDIKKKYIIYQQQgghZBRQTBFCCCGEEELIKKCYIoQQQgghhJBRQDFFCCGEEEIIIaOAYooQQgghhBBCRgHFFCGEEEIIIYSMAoopQgghhBBCCBkFFFOEEEIIIYQQMgo8E30C5UbXdQBAf3//BJ8JIYQQQgghZCIRmkBohGI55sTUwMAAAGD27NkTfCaEEEIIIYSQycDAwACqq6uLfpymj1aGTVFSqRQOHjyIqqoqaJo2YefR39+P2bNnY9++fQiHwxN2HscCXOvywvUuL1zv8sL1Lh9c6/LC9S4vXO/ykmu9dV3HwMAAZsyYAZer+A6oY86ZcrlcmDVr1kSfhiQcDvOXqExwrcsL17u8cL3LC9e7fHCtywvXu7xwvctLtvUejSMlYAAFIYQQQgghhIwCiilCCCGEEEIIGQUUUxOE3+/HrbfeCr/fP9GnMu3hWpcXrnd54XqXF653+eBalxeud3nhepeX8VzvYy6AghBCCCGEEEJKAZ0pQgghhBBCCBkFFFOEEEIIIYQQMgoopgghhBBCCCFkFFBMEUIIIYQQQsgooJiaAO6++27MmzcPgUAAq1atwksvvTTRpzQt+OY3vwlN0yz/LVmyRN4fiURw7bXXor6+HqFQCJdffjk6Ozsn8IynFk8//TQ++MEPYsaMGdA0DQ888IDlfl3Xccstt6C1tRXBYBBr167F9u3bLcccOXIEV155JcLhMGpqavCZz3wGg4ODZXwXU4N8a33VVVdlXOsXXnih5RiudWHcfvvtOPXUU1FVVYWmpiZceuml2LZtm+WYQv7u2Lt3Ly6++GJUVFSgqakJX/nKV5BIJMr5VqYEhaz3ueeem3F9X3PNNZZjuN6Fcc8992D58uVyUOnq1avxpz/9Sd7Pa7u05FtvXtvjxx133AFN03DDDTfI28p1fVNMlZnf/OY3+NKXvoRbb70Vr7zyClasWIF169ahq6trok9tWnD88cfj0KFD8r9nn31W3nfjjTfif//3f3HffffhqaeewsGDB/GhD31oAs92ajE0NIQVK1bg7rvvdrz/zjvvxF133YWf/OQnePHFF1FZWYl169YhEonIY6688kq8+eabePTRR/Hwww/j6aefxuc+97lyvYUpQ761BoALL7zQcq3/+te/ttzPtS6Mp556Ctdeey1eeOEFPProo4jH47jgggswNDQkj8n3d0cymcTFF1+MWCyG559/Hr/85S+xfv163HLLLRPxliY1haw3AFx99dWW6/vOO++U93G9C2fWrFm44447sHnzZmzatAnnn38+LrnkErz55psAeG2XmnzrDfDaHg9efvll/Md//AeWL19uub1s17dOysppp52mX3vttfLnZDKpz5gxQ7/99tsn8KymB7feequ+YsUKx/t6e3t1r9er33ffffK2t99+Wwegb9y4sUxnOH0AoN9///3y51Qqpbe0tOj/8i//Im/r7e3V/X6//utf/1rXdV1/6623dAD6yy+/LI/505/+pGuaph84cKBs5z7VsK+1ruv6pz71Kf2SSy7J+hiu9ejp6urSAehPPfWUruuF/d3xyCOP6C6XS+/o6JDH3HPPPXo4HNaj0Wh538AUw77euq7r55xzjn799ddnfQzXe2zU1tbqP/vZz3htlwmx3rrOa3s8GBgY0BctWqQ/+uijlvUt5/VNZ6qMxGIxbN68GWvXrpW3uVwurF27Fhs3bpzAM5s+bN++HTNmzMD8+fNx5ZVXYu/evQCAzZs3Ix6PW9Z+yZIlmDNnDte+BOzatQsdHR2W9a2ursaqVavk+m7cuBE1NTU45ZRT5DFr166Fy+XCiy++WPZznups2LABTU1NWLx4MT7/+c+jp6dH3se1Hj19fX0AgLq6OgCF/d2xceNGnHjiiWhubpbHrFu3Dv39/ZZvpEkm9vUW/OpXv0JDQwNOOOEE3HTTTRgeHpb3cb1HRzKZxL333ouhoSGsXr2a1/Y4Y19vAa/t0nLttdfi4osvtlzHQHn/7vaM8T2QIuju7kYymbR8aADQ3NyMrVu3TtBZTR9WrVqF9evXY/HixTh06BBuu+02nHXWWdiyZQs6Ojrg8/lQU1NjeUxzczM6Ojom5oSnEWINna5tcV9HRweampos93s8HtTV1fEzKJILL7wQH/rQh9DW1oadO3fiG9/4Bi666CJs3LgRbrebaz1KUqkUbrjhBpxxxhk44YQTAKCgvzs6Ojocr31xH3HGab0B4OMf/zjmzp2LGTNm4PXXX8fXvvY1bNu2DX/4wx8AcL2L5Y033sDq1asRiUQQCoVw//33Y9myZWhvb+e1PQ5kW2+A13apuffee/HKK6/g5ZdfzrivnH93U0yRacNFF10k/7x8+XKsWrUKc+fOxW9/+1sEg8EJPDNCSstHP/pR+ecTTzwRy5cvx4IFC7BhwwasWbNmAs9sanPttddiy5Ytll5LMn5kW2+1t+/EE09Ea2sr1qxZg507d2LBggXlPs0pz+LFi9He3o6+vj787ne/w6c+9Sk89dRTE31a05Zs671s2TJe2yVk3759uP766/Hoo48iEAhM6LmwzK+MNDQ0wO12ZySJdHZ2oqWlZYLOavpSU1OD4447Djt27EBLSwtisRh6e3stx3DtS4NYw1zXdktLS0bQSiKRwJEjR/gZjJH58+ejoaEBO3bsAMC1Hg3XXXcdHn74YTz55JOYNWuWvL2QvztaWlocr31xH8kk23o7sWrVKgCwXN9c78Lx+XxYuHAhTj75ZNx+++1YsWIFfvjDH/LaHieyrbcTvLZHz+bNm9HV1YWVK1fC4/HA4/Hgqaeewl133QWPx4Pm5uayXd8UU2XE5/Ph5JNPxuOPPy5vS6VSePzxxy31tKQ0DA4OYufOnWhtbcXJJ58Mr9drWftt27Zh7969XPsS0NbWhpaWFsv69vf348UXX5Tru3r1avT29mLz5s3ymCeeeAKpVEr+g0JGx/79+9HT04PW1lYAXOti0HUd1113He6//3488cQTaGtrs9xfyN8dq1evxhtvvGERsI8++ijC4bAs7yFp8q23E+3t7QBgub653qMnlUohGo3y2i4TYr2d4LU9etasWYM33ngD7e3t8r9TTjkFV155pfxz2a7vUiRpkMK59957db/fr69fv15/66239M997nN6TU2NJUmEjI4vf/nL+oYNG/Rdu3bpzz33nL527Vq9oaFB7+rq0nVd16+55hp9zpw5+hNPPKFv2rRJX716tb569eoJPuupw8DAgP7qq6/qr776qg5A//73v6+/+uqr+p49e3Rd1/U77rhDr6mp0R988EH99ddf1y+55BK9ra1NHxkZkc9x4YUX6ieddJL+4osv6s8++6y+aNEi/WMf+9hEvaVJS661HhgY0P/xH/9R37hxo75r1y79scce01euXKkvWrRIj0Qi8jm41oXx+c9/Xq+urtY3bNigHzp0SP43PDwsj8n3d0cikdBPOOEE/YILLtDb29v1P//5z3pjY6N+0003TcRbmtTkW+8dO3bo3/rWt/RNmzbpu3bt0h988EF9/vz5+tlnny2fg+tdOF//+tf1p556St+1a5f++uuv61//+td1TdP0v/71r7qu89ouNbnWm9f2+GNPSyzX9U0xNQH86Ec/0ufMmaP7fD79tNNO01944YWJPqVpwRVXXKG3trbqPp9Pnzlzpn7FFVfoO3bskPePjIzoX/jCF/Ta2lq9oqJCv+yyy/RDhw5N4BlPLZ588kkdQMZ/n/rUp3RdT8ej33zzzXpzc7Pu9/v1NWvW6Nu2bbM8R09Pj/6xj31MD4VCejgc1v/+7/9eHxgYmIB3M7nJtdbDw8P6BRdcoDc2Nuper1efO3eufvXVV2d8IcO1LgyndQag/+IXv5DHFPJ3x+7du/WLLrpIDwaDekNDg/7lL39Zj8fjZX43k598671371797LPP1uvq6nS/368vXLhQ/8pXvqL39fVZnofrXRif/vSn9blz5+o+n09vbGzU16xZI4WUrvPaLjW51pvX9vhjF1Plur41Xdf1or01QgghhBBCCDnGYc8UIYQQQgghhIwCiilCCCGEEEIIGQUUU4QQQgghhBAyCiimCCGEEEIIIWQUUEwRQgghhBBCyCigmCKEEEIIIYSQUUAxRQghhBBCCCGjgGKKEEIIIYQQQkYBxRQhhBBCCCGEjAKKKUIIjB6VdgAABK9JREFUIVOOw4cP4/Of/zzmzJkDv9+PlpYWrFu3Ds899xwAQNM0PPDAAxN7koQQQqY9nok+AUIIIaRYLr/8csRiMfzyl7/E/Pnz0dnZiccffxw9PT0TfWqEEEKOIehMEUIImVL09vbimWeewXe/+12cd955mDt3Lk477TTcdNNN+Ju/+RvMmzcPAHDZZZdB0zT5MwA8+OCDWLlyJQKBAObPn4/bbrsNiURC3q9pGu655x5cdNFFCAaDmD9/Pn73u9/J+2OxGK677jq0trYiEAhg7ty5uP3228v11gkhhEwyKKYIIYRMKUKhEEKhEB544AFEo9GM+19++WUAwC9+8QscOnRI/vzMM8/gk5/8JK6//nq89dZb+I//+A+sX78e3/nOdyyPv/nmm3H55Zfjtddew5VXXomPfvSjePvttwEAd911Fx566CH89re/xbZt2/CrX/3KItYIIYQcW2i6rusTfRKEEEJIMfz+97/H1VdfjZGREaxcuRLnnHMOPvrRj2L58uUA0g7T/fffj0svvVQ+Zu3atVizZg1uuukmedt///d/46tf/SoOHjwoH3fNNdfgnnvukce8973vxcqVK/HjH/8YX/ziF/Hmm2/iscceg6Zp5XmzhBBCJi10pgghhEw5Lr/8chw8eBAPPfQQLrzwQmzYsAErV67E+vXrsz7mtddew7e+9S3pbIVCIVx99dU4dOgQhoeH5XGrV6+2PG716tXSmbrqqqvQ3t6OxYsX44tf/CL++te/jsv7I4QQMjWgmCKEEDIlCQQCeN/73oebb74Zzz//PK666irceuutWY8fHBzEbbfdhvb2dvnfG2+8ge3btyMQCBT0mitXrsSuXbvw7W9/GyMjI/jbv/1bfPjDHy7VWyKEEDLFoJgihBAyLVi2bBmGhoYAAF6vF8lk0nL/ypUrsW3bNixcuDDjP5fL/OfwhRdesDzuhRdewNKlS+XP4XAYV1xxBX7605/iN7/5DX7/+9/jyJEj4/jOCCGETFYYjU4IIWRK0dPTg4985CP49Kc/jeXLl6OqqgqbNm3CnXfeiUsuuQQAMG/ePDz++OM444wz4Pf7UVtbi1tuuQUf+MAHMGfOHHz4wx+Gy+XCa6+9hi1btuCf//mf5fPfd999OOWUU3DmmWfiV7/6FV566SX8/Oc/BwB8//vfR2trK0466SS4XC7cd999aGlpQU1NzUQsBSGEkAmGYooQQsiUIhQKYdWqVfjBD36AnTt3Ih6PY/bs2bj66qvxjW98AwDwve99D1/60pfw05/+FDNnzsTu3buxbt06PPzww/jWt76F7373u/B6vViyZAk++9nPWp7/tttuw7333osvfOELaG1txa9//WssW7YMAFBVVYU777wT27dvh9vtxqmnnopHHnnE4mwRQgg5dmCaHyGEEGLglAJICCGEZINfpRFCCCGEEELIKKCYIoQQQgghhJBRwJ4pQgghxICV74QQQoqBzhQhhBBCCCGEjAKKKUIIIYQQQggZBRRThBBCCCGEEDIKKKYIIYQQQgghZBRQTBFCCCGEEELIKKCYIoQQQgghhJBRQDFFCCGEEEIIIaOAYooQQgghhBBCRsH/D74oBd4Vyzy0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# After trainer.train()\n",
    "log_history = trainer.state.log_history\n",
    "train_loss = [log['loss'] for log in log_history if 'loss' in log]\n",
    "eval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "steps = [log['step'] for log in log_history if 'loss' in log]\n",
    "eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, train_loss, label=\"Training Loss\")\n",
    "plt.plot(eval_steps, eval_loss, label=\"Evaluation Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:41:28.234123Z",
     "iopub.status.busy": "2025-06-16T16:41:28.233825Z",
     "iopub.status.idle": "2025-06-16T16:41:28.720970Z",
     "shell.execute_reply": "2025-06-16T16:41:28.720345Z",
     "shell.execute_reply.started": "2025-06-16T16:41:28.234104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LoRA adapters saved to ./llama3_sft_config_peft_fixed/final_model_adapters_combined_datasets\n"
     ]
    }
   ],
   "source": [
    "final_adapter_path = f\"{output_dir}/final_model_adapters_combined_datasets\"\n",
    "trainer.model.save_pretrained(final_adapter_path) # Saves only the LoRA adapters\n",
    "tokenizer.save_pretrained(final_adapter_path) # Save tokenizer with adapters\n",
    "print(f\"Final LoRA adapters saved to {final_adapter_path}\")\n",
    "\n",
    "# If you want to save the full merged model (optional, much larger):\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    "# merged_model_path = f\"{output_dir}/final_merged_model\"\n",
    "# # Load the PEFT model\n",
    "# merged_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     final_adapter_path, # path to the saved adapters\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16 # or torch.float16\n",
    "# )\n",
    "# # Merge LoRA and base model\n",
    "# merged_model = merged_model.merge_and_unload()\n",
    "# merged_model.save_pretrained(merged_model_path, safe_serialization=True)\n",
    "# tokenizer.save_pretrained(merged_model_path)\n",
    "# print(f\"Full merged model saved to {merged_model_path}\")\n",
    "# For inference, it's often easier to load the base model and then apply adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:41:30.618336Z",
     "iopub.status.busy": "2025-06-16T16:41:30.618069Z",
     "iopub.status.idle": "2025-06-16T16:43:11.203381Z",
     "shell.execute_reply": "2025-06-16T16:43:11.202755Z",
     "shell.execute_reply.started": "2025-06-16T16:41:30.618316Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 16.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from adapter path: ./llama3_sft_config_peft_fixed/final_model_adapters_combined_datasets\n",
      "Setting pad_token_id (128009) on model.config and model.generation_config\n",
      "Setting eos_token_id (128009) on model.generation_config\n",
      "Loading PEFT adapters from: ./llama3_sft_config_peft_fixed/final_model_adapters_combined_datasets\n",
      "Inference model loaded successfully.\n",
      "  Inference model's generation_config.eos_token_id: 128009\n",
      "  Inference model's generation_config.pad_token_id: 128009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Configuration for loading the base model\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "adapter_path = final_adapter_path # Or path to a specific checkpoint\n",
    "\n",
    "# Load the base model (quantized or full precision, depending on how you want to run inference)\n",
    "# For consistency with training and VRAM, using QLoRA for inference is common\n",
    "bnb_config_inference = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Loading base model: {base_model_id}\")\n",
    "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config_inference, # or remove for full precision if VRAM allows\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer from adapter path: {adapter_path}\")\n",
    "tokenizer_for_inference = AutoTokenizer.from_pretrained(adapter_path) # Load tokenizer saved with adapters\n",
    "\n",
    "# Ensure pad_token is set on the tokenizer (critical for generation)\n",
    "if tokenizer_for_inference.pad_token is None:\n",
    "    print(\"Tokenizer pad_token is None, setting to eos_token.\")\n",
    "    tokenizer_for_inference.pad_token = tokenizer_for_inference.eos_token\n",
    "\n",
    "# --- CRITICAL FIX: Update model's generation_config ---\n",
    "# Ensure the model's config and generation_config have pad_token_id and eos_token_id\n",
    "print(f\"Setting pad_token_id ({tokenizer_for_inference.pad_token_id}) on model.config and model.generation_config\")\n",
    "base_model_for_inference.config.pad_token_id = tokenizer_for_inference.pad_token_id\n",
    "base_model_for_inference.generation_config.pad_token_id = tokenizer_for_inference.pad_token_id\n",
    "\n",
    "if tokenizer_for_inference.eos_token_id is None:\n",
    "    print(\"ERROR: tokenizer_for_inference.eos_token_id is None! This should not happen.\")\n",
    "    # Handle error or set a default if absolutely necessary, though this indicates a problem with tokenizer loading.\n",
    "else:\n",
    "    print(f\"Setting eos_token_id ({tokenizer_for_inference.eos_token_id}) on model.generation_config\")\n",
    "    base_model_for_inference.generation_config.eos_token_id = tokenizer_for_inference.eos_token_id\n",
    "# --- END OF CRITICAL FIX ---\n",
    "\n",
    "print(f\"Loading PEFT adapters from: {adapter_path}\")\n",
    "inference_model = PeftModel.from_pretrained(base_model_for_inference, adapter_path)\n",
    "inference_model.eval() # Set to evaluation mode\n",
    "print(\"Inference model loaded successfully.\")\n",
    "print(f\"  Inference model's generation_config.eos_token_id: {inference_model.generation_config.eos_token_id}\")\n",
    "print(f\"  Inference model's generation_config.pad_token_id: {inference_model.generation_config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:50:52.985740Z",
     "iopub.status.busy": "2025-06-16T16:50:52.985036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Interactive Chat ---\n",
      "Type 'quit' to end the conversation.\n",
      "An empty input will also quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human 1:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human 2: Thinking...\r"
     ]
    }
   ],
   "source": [
    "def chat_with_human_model(conversation_history_str, max_new_tokens=150):\n",
    "    # Ensure the prompt ends appropriately for the model to generate Human 2's response\n",
    "    if not conversation_history_str.strip().endswith(\"Human 2:\"):\n",
    "         # If history is empty or ends with Human 2, start Human 1\n",
    "        if not conversation_history_str.strip() or conversation_history_str.strip().endswith(\"Human 1:\"):\n",
    "             # This case should ideally not happen if we provide full history\n",
    "             pass # Let it be, or add \"Human 1: \" if needed\n",
    "        # If history ends with Human 1's turn, add \"Human 2: \"\n",
    "        elif conversation_history_str.strip().endswith(\"Human 1:\"):\n",
    "             conversation_history_str += \"\\nHuman 2: \" # This indicates model should generate Human 2's line\n",
    "        # If history is complete, add new Human 1 prompt then Human 2:\n",
    "        # This part is handled by how you build `current_conversation_for_prompt`\n",
    "\n",
    "    # Add BOS token if not present (our training data has it)\n",
    "    if not conversation_history_str.startswith(tokenizer_for_inference.bos_token):\n",
    "        conversation_history_str = tokenizer_for_inference.bos_token + conversation_history_str\n",
    "\n",
    "    inputs = tokenizer_for_inference(conversation_history_str, return_tensors=\"pt\", padding=False, truncation=True).to(inference_model.device)\n",
    "    \n",
    "    # Remove token_type_ids if the model doesn't expect it (Llama generally doesn't)\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    # Define stopping conditions.\n",
    "    # The primary EOS token is already set on inference_model.generation_config.eos_token_id.\n",
    "    # Additional stopping sequences can be handled by passing their token IDs.\n",
    "    # If \"\\nHuman 1:\" is multi-token, this specific way of stopping might be inexact.\n",
    "    # A custom StoppingCriteria class would be more robust for multi-token sequences.\n",
    "    eos_token_ids_for_stopping = [tokenizer_for_inference.eos_token_id]\n",
    "    # Optional: Try to stop if it starts generating a new \"Human 1:\" turn.\n",
    "    # This requires \"\\nHuman 1:\" to be a single token or for generate to handle multi-token sequences here.\n",
    "    # Note: Llama 3 tokenizer might tokenize \"\\nHuman 1:\" into multiple tokens.\n",
    "    # For simplicity, we'll rely on the primary EOS and post-generation cleanup.\n",
    "    # If you observe issues, a custom StoppingCriteria is the way to go for complex sequences.\n",
    "    \n",
    "    # Forcing early stop if it generates the token for \"\\n\" followed by \"Human 1:\" can be tricky.\n",
    "    # The simplest and most robust is often to rely on the main EOS and clean up after.\n",
    "    # Let's use only the main EOS for generate, and clean up \"\\nHuman 1:\" post-generation.\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations for inference\n",
    "        outputs = inference_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            #eos_token_id=eos_token_ids_for_stopping, # Relies on generation_config.eos_token_id being set\n",
    "            pad_token_id=tokenizer_for_inference.pad_token_id # Already set on generation_config\n",
    "                                                               # but can be confirmed here.\n",
    "        )\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    response_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response_text = tokenizer_for_inference.decode(response_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-generation cleanup: Remove any partially generated \"Human 1:\" turn\n",
    "    if \"\\nHuman 1:\" in response_text:\n",
    "        response_text = response_text.split(\"\\nHuman 1:\")[0]\n",
    "    elif \"Human 1:\" in response_text: # If it somehow generates \"Human 1:\" without newline\n",
    "        response_text = response_text.split(\"Human 1:\")[0]\n",
    "        \n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "# --- Interactive Chat Loop ---\n",
    "print(\"\\n--- Starting Interactive Chat ---\")\n",
    "print(\"Type 'quit' to end the conversation.\")\n",
    "print(\"An empty input will also quit.\")\n",
    "\n",
    "# Initialize conversation history.\n",
    "# For Llama 3, it's good practice to ensure the full prompt adheres to its expected format,\n",
    "# even if we only explicitly add BOS here. The training data did not have system prompts.\n",
    "current_conversation_log = \"\" \n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Human 1: \")\n",
    "    if user_input.lower() == 'quit' or user_input == \"\":\n",
    "        break\n",
    "    \n",
    "    # Append user's turn to the conversation log\n",
    "    current_conversation_log += f\"Human 1: {user_input}\\n\"\n",
    "    \n",
    "    # Prepare the full prompt for the model to generate Human 2's response\n",
    "    # The chat_with_human_model function will prepend BOS if needed.\n",
    "    prompt_for_model = current_conversation_log + \"Human 2: \"\n",
    "\n",
    "    # Get model's response\n",
    "    print(\"Human 2: Thinking...\", end=\"\\r\") # Temporary \"Thinking\" message\n",
    "    bot_response = chat_with_human_model(prompt_for_model)\n",
    "    print(f\"Human 2: {bot_response}       \") # Print response, overwrite \"Thinking\"\n",
    "    \n",
    "    # Append bot's turn to the conversation log\n",
    "    current_conversation_log += f\"Human 2: {bot_response}\\n\"\n",
    "\n",
    "    # Optional: Truncate conversation_history_log if it gets too long to prevent exceeding max_seq_length significantly\n",
    "    # (A more sophisticated approach would use a sliding window or summarize older parts)\n",
    "    # For now, this simple loop lets it grow.\n",
    "\n",
    "print(\"\\n--- Chat Ended ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T16:44:27.754231Z",
     "iopub.status.busy": "2025-06-16T16:44:27.753577Z",
     "iopub.status.idle": "2025-06-16T16:47:58.248829Z",
     "shell.execute_reply": "2025-06-16T16:47:58.247575Z",
     "shell.execute_reply.started": "2025-06-16T16:44:27.754208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Non-Interactive Test ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/148595511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Running Non-Interactive Test ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Human 1: hey whats up\\nHuman 2: nm u\\nHuman 1: tell me a joke\\nHuman 2: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_with_human_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input:\\n{test_history}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output:\\n{response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/2173315937.py\u001b[0m in \u001b[0;36mchat_with_human_model\u001b[0;34m(conversation_history_str, max_new_tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Disable gradient calculations for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         outputs = inference_model.generate(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemv_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     return torch.ops.bitsandbytes.gemv_4bit.default(\n\u001b[0m\u001b[1;32m   1725\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/backends/cpu/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, B, shapeB, absmax, code, blocksize)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Right now we assume NF4, as this is the only one supported on CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     B_dq = torch.ops.bitsandbytes.dequantize_4bit.default(\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bitsandbytes/backends/cpu/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, absmax, blocksize, quant_type, shape, dtype)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Reshape to original shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Example Non-Interactive Test (Optional) ---\n",
    "print(\"\\n--- Running Non-Interactive Test ---\")\n",
    "test_history = \"Human 1: hey whats up\\nHuman 2: nm u\\nHuman 1: tell me a joke\\nHuman 2: \"\n",
    "response = chat_with_human_model(test_history)\n",
    "print(f\"Input:\\n{test_history}\")\n",
    "print(f\"Output:\\n{response}\")\n",
    "\n",
    "ai_test_history = \"Human 1: Are you an AI language model?\\nHuman 2:\"\n",
    "response_ai_test = chat_with_human_model(ai_test_history)\n",
    "print(f\"\\nInput:\\n{ai_test_history}\")\n",
    "print(f\"Output:\\n{response_ai_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T13:42:59.191565Z",
     "iopub.status.busy": "2025-05-23T13:42:59.191112Z",
     "iopub.status.idle": "2025-05-23T13:43:29.143968Z",
     "shell.execute_reply": "2025-05-23T13:43:29.142960Z",
     "shell.execute_reply.started": "2025-05-23T13:42:59.191544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"Human 1: \")\n",
    "    if user_input.lower() == 'quit' or user_input == \"\":\n",
    "        break\n",
    "    \n",
    "    # Append user's turn to the conversation log\n",
    "    current_conversation_log += f\"Human 1: {user_input}\\n\"\n",
    "    \n",
    "    # Prepare the full prompt for the model to generate Human 2's response\n",
    "    # The chat_with_human_model function will prepend BOS if needed.\n",
    "    prompt_for_model = current_conversation_log + \"Human 2: \"\n",
    "\n",
    "    # Get model's response\n",
    "    print(\"Human 2: Thinking...\", end=\"\\r\") # Temporary \"Thinking\" message\n",
    "    bot_response = chat_with_human_model(prompt_for_model)\n",
    "    print(f\"Human 2: {bot_response}       \") # Print response, overwrite \"Thinking\"\n",
    "    \n",
    "    # Append bot's turn to the conversation log\n",
    "    current_conversation_log += f\"Human 2: {bot_response}\\n\"\n",
    "\n",
    "    # Optional: Truncate conversation_history_log if it gets too long to prevent exceeding max_seq_length significantly\n",
    "    # (A more sophisticated approach would use a sliding window or summarize older parts)\n",
    "    # For now, this simple loop lets it grow.\n",
    "\n",
    "print(\"\\n--- Chat Ended ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7489203,
     "sourceId": 11912609,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7495359,
     "sourceId": 11922025,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7675073,
     "sourceId": 12185453,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
